#!/usr/bin/env python3
"""
è§†é¢‘åˆ†æä»»åŠ¡è°ƒè¯•å·¥å…· - å¼¹æ€§åˆ†æç‰ˆæœ¬
ä¸“æ³¨äºè§£å†³LLMè°ƒç”¨é˜»å¡é—®é¢˜ï¼Œè€Œéç®€å•çš„æ—¶é—´é™åˆ¶
"""
import os
import sys
import time
import logging
import asyncio
from pathlib import Path
from datetime import datetime, timezone
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent))

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("video_analysis_debug")

def check_ollama_service():
    """æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€å’Œæ¨¡å‹å¯ç”¨æ€§"""
    logger.info("=== æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€ ===")
    
    try:
        import requests
        
        # æ£€æŸ¥æœåŠ¡åŸºæœ¬çŠ¶æ€
        try:
            response = requests.get("http://host.docker.internal:11435/api/tags", timeout=10)
            if response.status_code == 200:
                data = response.json()
                models = data.get('models', [])
                logger.info(f"âœ… OllamaæœåŠ¡æ­£å¸¸ï¼Œå¯ç”¨æ¨¡å‹æ•°: {len(models)}")
                
                # æ£€æŸ¥å…·ä½“æ¨¡å‹
                qwen_models = [m for m in models if 'qwen' in m['name'].lower()]
                if qwen_models:
                    for model in qwen_models:
                        name = model.get('name', 'unknown')
                        size = model.get('size', 0) // (1024**3)  # GB
                        logger.info(f"  ğŸ“¹ {name} ({size}GB)")
                else:
                    logger.warning("âš ï¸ æœªå‘ç°Qwenè§†è§‰æ¨¡å‹")
                
                return len(models) > 0
            else:
                logger.error(f"âŒ OllamaæœåŠ¡å“åº”å¼‚å¸¸: {response.status_code}")
                return False
                
        except requests.exceptions.Timeout:
            logger.error("âŒ OllamaæœåŠ¡è¿æ¥è¶…æ—¶")
            return False
        except requests.exceptions.ConnectionError:
            logger.error("âŒ æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡")
            return False
            
    except Exception as e:
        logger.error(f"âŒ æ£€æŸ¥OllamaæœåŠ¡å¤±è´¥: {e}")
        return False

def test_llm_connection():
    """æµ‹è¯•LLMè¿æ¥ç¨³å®šæ€§"""
    logger.info("=== æµ‹è¯•LLMè¿æ¥ç¨³å®šæ€§ ===")
    
    try:
        from backend.services.video_vision_service import VideoVisionService
        
        # åˆ›å»ºæœåŠ¡å®ä¾‹
        vision_service = VideoVisionService()
        
        # æµ‹è¯•å¥åº·æ£€æŸ¥
        async def test_health_check():
            health_ok = await vision_service._check_ollama_health()
            if health_ok:
                logger.info("âœ… Ollamaå¥åº·æ£€æŸ¥é€šè¿‡")
            else:
                logger.error("âŒ Ollamaå¥åº·æ£€æŸ¥å¤±è´¥")
            return health_ok
        
        # è¿è¡Œæµ‹è¯•
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        health_result = loop.run_until_complete(test_health_check())
        
        return health_result
        
    except Exception as e:
        logger.error(f"âŒ LLMè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
        return False

def check_gpu_status():
    """æ£€æŸ¥GPUçŠ¶æ€ - åŒºåˆ†ä¸»æœºå’Œå®¹å™¨ç¯å¢ƒ"""
    logger.info("=== æ£€æŸ¥GPUçŠ¶æ€ ===")
    
    gpu_status = {
        "host_gpu": False,
        "container_gpu": False,
        "nvidia_smi": False
    }
    
    # 1. æ£€æŸ¥NVIDIAé©±åŠ¨å’Œç¡¬ä»¶
    try:
        import subprocess
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, timeout=10)
        if result.returncode == 0:
            gpu_status["nvidia_smi"] = True
            # æå–GPUä¿¡æ¯
            output_lines = result.stdout.split('\n')
            for line in output_lines:
                if 'NVIDIA GeForce' in line or 'RTX' in line:
                    gpu_info = line.strip()
                    logger.info(f"âœ… æ£€æµ‹åˆ°GPUç¡¬ä»¶: {gpu_info}")
                    break
        else:
            logger.error("âŒ nvidia-smi æ‰§è¡Œå¤±è´¥")
    except Exception as e:
        logger.error(f"âŒ nvidia-smi æ£€æŸ¥å¤±è´¥: {e}")
    
    # 2. æ£€æŸ¥ä¸»æœºPythonç¯å¢ƒçš„GPUæ”¯æŒ
    try:
        import torch
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory // 1024**3
            logger.info(f"âœ… ä¸»æœºGPUå¯ç”¨: {gpu_name} ({gpu_memory}GB)")
            gpu_status["host_gpu"] = True
        else:
            logger.warning("âš ï¸ ä¸»æœºGPUä¸å¯ç”¨ï¼ˆå¯èƒ½ç¼ºå°‘CUDAç‰ˆæœ¬PyTorchï¼‰")
    except ImportError:
        logger.warning("âš ï¸ ä¸»æœºç¯å¢ƒç¼ºå°‘PyTorch")
    except Exception as e:
        logger.error(f"âŒ ä¸»æœºGPUæ£€æŸ¥å¤±è´¥: {e}")
    
    # 3. æ£€æŸ¥å®¹å™¨å†…çš„GPUæ”¯æŒï¼ˆè¿™æ˜¯å…³é”®çš„ï¼‰
    try:
        import subprocess
        import os
        
        # å°è¯•æ‰¾åˆ°Dockerå¯æ‰§è¡Œæ–‡ä»¶
        docker_paths = [
            "docker",  # å…ˆå°è¯•PATHä¸­çš„docker
            "C:\\Program Files\\Docker\\Docker\\resources\\bin\\docker.exe",  # Docker Desktopè·¯å¾„
            os.path.expandvars("%ProgramFiles%\\Docker\\Docker\\resources\\bin\\docker.exe")  # åŠ¨æ€è·¯å¾„
        ]
        
        docker_executable = None
        for path in docker_paths:
            try:
                test_result = subprocess.run([path, '--version'], capture_output=True, text=True, timeout=5)
                if test_result.returncode == 0:
                    docker_executable = path
                    logger.debug(f"æ‰¾åˆ°Dockerå¯æ‰§è¡Œæ–‡ä»¶: {path}")
                    break
            except Exception:
                continue
        
        if not docker_executable:
            logger.error("âŒ æ— æ³•æ‰¾åˆ°Dockerå¯æ‰§è¡Œæ–‡ä»¶")
            logger.error("è¯·ç¡®ä¿Docker Desktopå·²å®‰è£…å¹¶è¿è¡Œ")
            return gpu_status
        
        cmd = [
            docker_executable, 'exec', 'all_data_solution_sys-celery-worker-1', 
            'python', '-c', 
            'import torch; print(f"CUDA:{torch.cuda.is_available()};COUNT:{torch.cuda.device_count() if torch.cuda.is_available() else 0};NAME:{torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\"}")'
        ]
        
        logger.debug(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=15)
        
        if result.returncode == 0:
            output = result.stdout.strip()
            logger.debug(f"å®¹å™¨è¾“å‡º: {output}")
            if "CUDA:True" in output:
                gpu_status["container_gpu"] = True
                # è§£æè¾“å‡º
                parts = output.split(';')
                cuda_available = "True" in parts[0]
                gpu_count = parts[1].split(':')[1] if len(parts) > 1 else "0"
                gpu_name = parts[2].split(':')[1] if len(parts) > 2 else "Unknown"
                logger.info(f"âœ… å®¹å™¨GPUå¯ç”¨: {gpu_name} (æ•°é‡: {gpu_count})")
            else:
                logger.error("âŒ å®¹å™¨å†…GPUä¸å¯ç”¨")
                logger.error(f"å®¹å™¨è¾“å‡º: {output}")
        else:
            logger.error(f"âŒ å®¹å™¨GPUæ£€æŸ¥å¤±è´¥: {result.stderr}")
    except Exception as e:
        logger.error(f"âŒ å®¹å™¨GPUæ£€æŸ¥å¼‚å¸¸: {e}")
    
    # ç”ŸæˆGPUçŠ¶æ€æŠ¥å‘Š
    if gpu_status["container_gpu"]:
        logger.info("ğŸ¯ GPUçŠ¶æ€: å®¹å™¨å†…GPUå¯ç”¨ï¼Œè§†é¢‘åˆ†æå°†ä½¿ç”¨GPUåŠ é€Ÿ")
    elif gpu_status["nvidia_smi"]:
        logger.warning("âš ï¸ GPUçŠ¶æ€: ç¡¬ä»¶æ­£å¸¸ä½†å®¹å™¨å†…ä¸å¯ç”¨ï¼Œéœ€è¦æ£€æŸ¥Docker GPUé…ç½®")
    else:
        logger.error("âŒ GPUçŠ¶æ€: GPUç¡¬ä»¶æˆ–é©±åŠ¨å¼‚å¸¸")
    
    return gpu_status

def check_video_analysis_health():
    """æ£€æŸ¥è§†é¢‘åˆ†ææœåŠ¡æ•´ä½“å¥åº·çŠ¶æ€"""
    logger.info("=== æ£€æŸ¥è§†é¢‘åˆ†ææœåŠ¡å¥åº·çŠ¶æ€ ===")
    
    health_results = {
        "ollama_service": False,
        "llm_connection": False,
        "gpu_available": False,
        "dependencies": False
    }
    
    # 1. æ£€æŸ¥OllamaæœåŠ¡
    health_results["ollama_service"] = check_ollama_service()
    
    # 2. æµ‹è¯•LLMè¿æ¥
    health_results["llm_connection"] = test_llm_connection()
    
    # 3. æ£€æŸ¥GPUçŠ¶æ€ï¼ˆä¿®æ”¹ä¸ºä½¿ç”¨æ–°çš„æ£€æŸ¥æ–¹æ³•ï¼‰
    gpu_status = check_gpu_status()
    health_results["gpu_available"] = gpu_status["container_gpu"]  # ä½¿ç”¨å®¹å™¨GPUçŠ¶æ€
    
    # 4. æ£€æŸ¥å…³é”®ä¾èµ–
    try:
        import aiohttp
        from langchain_ollama import ChatOllama
        from PIL import Image
        logger.info("âœ… å…³é”®ä¾èµ–åº“å¯ç”¨")
        health_results["dependencies"] = True
    except ImportError as e:
        logger.error(f"âŒ ç¼ºå°‘å…³é”®ä¾èµ–: {e}")
    
    # ç”Ÿæˆå¥åº·æŠ¥å‘Š
    healthy_count = sum(health_results.values())
    total_checks = len(health_results)
    health_score = healthy_count / total_checks * 100
    
    logger.info(f"\nğŸ“Š å¥åº·çŠ¶æ€æŠ¥å‘Š: {healthy_count}/{total_checks} æ£€æŸ¥é€šè¿‡ ({health_score:.1f}%)")
    
    if health_score >= 75:
        logger.info("ğŸŸ¢ ç³»ç»ŸçŠ¶æ€è‰¯å¥½")
    elif health_score >= 50:
        logger.warning("ğŸŸ¡ ç³»ç»ŸçŠ¶æ€ä¸€èˆ¬ï¼Œå»ºè®®æ£€æŸ¥")
    else:
        logger.error("ğŸ”´ ç³»ç»ŸçŠ¶æ€å¼‚å¸¸ï¼Œéœ€è¦ä¿®å¤")
    
    return health_results

def analyze_stuck_patterns():
    """åˆ†æä»»åŠ¡å¡ä½çš„æ¨¡å¼å’Œè§„å¾‹"""
    logger.info("=== åˆ†æä»»åŠ¡å¡ä½æ¨¡å¼ ===")
    
    try:
        from backend.core.database import get_sync_db
        from backend.models.video_analysis import VideoAnalysis, VideoAnalysisStatus
        from datetime import datetime, timedelta
        
        db = next(get_sync_db())
        
        # æŸ¥æ‰¾æœ€è¿‘24å°æ—¶çš„å¤±è´¥ä»»åŠ¡
        time_threshold = datetime.now() - timedelta(hours=24)
        
        failed_analyses = db.query(VideoAnalysis).filter(
            VideoAnalysis.status == VideoAnalysisStatus.FAILED,
            VideoAnalysis.updated_at > time_threshold
        ).all()
        
        stuck_analyses = db.query(VideoAnalysis).filter(
            VideoAnalysis.status == VideoAnalysisStatus.IN_PROGRESS,
            VideoAnalysis.updated_at < datetime.now() - timedelta(minutes=10)
        ).all()
        
        logger.info(f"æœ€è¿‘24å°æ—¶å¤±è´¥ä»»åŠ¡: {len(failed_analyses)}")
        logger.info(f"å½“å‰å¡ä½ä»»åŠ¡: {len(stuck_analyses)}")
        
        # åˆ†æå¤±è´¥æ¨¡å¼
        error_patterns = {}
        for analysis in failed_analyses:
            error_msg = analysis.error_message or "æœªçŸ¥é”™è¯¯"
            # æå–é”™è¯¯å…³é”®è¯
            if "è¶…æ—¶" in error_msg or "timeout" in error_msg.lower():
                error_patterns["timeout"] = error_patterns.get("timeout", 0) + 1
            elif "è¿æ¥" in error_msg or "connection" in error_msg.lower():
                error_patterns["connection"] = error_patterns.get("connection", 0) + 1
            elif "å†…å­˜" in error_msg or "memory" in error_msg.lower():
                error_patterns["memory"] = error_patterns.get("memory", 0) + 1
            else:
                error_patterns["other"] = error_patterns.get("other", 0) + 1
        
        if error_patterns:
            logger.info("é”™è¯¯æ¨¡å¼åˆ†æ:")
            for pattern, count in error_patterns.items():
                logger.info(f"  {pattern}: {count}æ¬¡")
        
        # åˆ†æå¡ä½ä»»åŠ¡çš„é˜¶æ®µ
        stuck_phases = {}
        for analysis in stuck_analyses:
            phase = analysis.current_phase or "unknown"
            stuck_phases[phase] = stuck_phases.get(phase, 0) + 1
        
        if stuck_phases:
            logger.info("å¡ä½é˜¶æ®µåˆ†æ:")
            for phase, count in stuck_phases.items():
                logger.info(f"  {phase}: {count}ä¸ªä»»åŠ¡")
        
        db.close()
        return {"failed_patterns": error_patterns, "stuck_phases": stuck_phases}
        
    except Exception as e:
        logger.error(f"åˆ†æå¡ä½æ¨¡å¼å¤±è´¥: {e}")
        return {}

def suggest_solutions(health_results: dict, patterns: dict):
    """åŸºäºå¥åº·çŠ¶æ€å’Œé”™è¯¯æ¨¡å¼æä¾›è§£å†³å»ºè®®"""
    logger.info("=== è§£å†³æ–¹æ¡ˆå»ºè®® ===")
    
    suggestions = []
    
    # åŸºäºå¥åº·æ£€æŸ¥ç»“æœ
    if not health_results.get("ollama_service"):
        suggestions.append("ğŸ”§ é‡å¯OllamaæœåŠ¡: docker restart ollama")
    
    if not health_results.get("llm_connection"):
        suggestions.append("ğŸ”§ æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œé˜²ç«å¢™è®¾ç½®")
    
    if not health_results.get("gpu_available"):
        suggestions.append("ğŸ”§ æ£€æŸ¥GPUé©±åŠ¨å’ŒCUDAç¯å¢ƒ")
    
    # åŸºäºé”™è¯¯æ¨¡å¼
    error_patterns = patterns.get("failed_patterns", {})
    if error_patterns.get("timeout", 0) > 0:
        suggestions.append("âš¡ è€ƒè™‘å‡å°‘å¸§é‡‡æ ·æ•°é‡æˆ–é™ä½åˆ†æå¤æ‚åº¦")
    
    if error_patterns.get("connection", 0) > 0:
        suggestions.append("ğŸŒ æ£€æŸ¥OllamaæœåŠ¡ç¨³å®šæ€§å’Œç½‘ç»œé…ç½®")
    
    if error_patterns.get("memory", 0) > 0:
        suggestions.append("ğŸ’¾ å¢åŠ ç³»ç»Ÿå†…å­˜æˆ–ä¼˜åŒ–æ¨¡å‹å‚æ•°")
    
    # åŸºäºå¡ä½é˜¶æ®µ
    stuck_phases = patterns.get("stuck_phases", {})
    if "è§†è§‰åˆ†æ" in stuck_phases:
        suggestions.append("ğŸ‘ï¸ æ£€æŸ¥è§†è§‰åˆ†ææ¨¡å‹åŠ è½½çŠ¶æ€")
    
    if not suggestions:
        suggestions.append("âœ… ç³»ç»ŸçŠ¶æ€è‰¯å¥½ï¼Œæ— éœ€ç‰¹æ®Šå¤„ç†")
    
    for i, suggestion in enumerate(suggestions, 1):
        logger.info(f"{i}. {suggestion}")
    
    return suggestions

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='è§†é¢‘åˆ†æå¼¹æ€§è°ƒè¯•å·¥å…·')
    parser.add_argument('--check-health', action='store_true', help='æ£€æŸ¥ç³»ç»Ÿå¥åº·çŠ¶æ€')
    parser.add_argument('--test-connection', action='store_true', help='æµ‹è¯•LLMè¿æ¥')
    parser.add_argument('--analyze-patterns', action='store_true', help='åˆ†æé”™è¯¯æ¨¡å¼')
    parser.add_argument('--full-check', action='store_true', help='å®Œæ•´å¥åº·æ£€æŸ¥å’Œå»ºè®®')
    
    args = parser.parse_args()
    
    if args.check_health:
        health_results = check_video_analysis_health()
    elif args.test_connection:
        test_llm_connection()
    elif args.analyze_patterns:
        patterns = analyze_stuck_patterns()
    elif args.full_check:
        health_results = check_video_analysis_health()
        patterns = analyze_stuck_patterns()
        suggest_solutions(health_results, patterns)
    else:
        # é»˜è®¤æ‰§è¡Œå®Œæ•´æ£€æŸ¥
        logger.info("æ‰§è¡Œå®Œæ•´å¥åº·æ£€æŸ¥å’Œåˆ†æ...")
        health_results = check_video_analysis_health()
        patterns = analyze_stuck_patterns()
        suggest_solutions(health_results, patterns)

if __name__ == "__main__":
    main() 