"""
LlamaIndex ä½¿ç”¨ç¤ºä¾‹
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨LlamaIndexæ„å»ºç®€å•çš„RAGåº”ç”¨
"""
import os
from llama_index.core import Document, VectorStoreIndex, Settings
from llama_index.core.embeddings import MockEmbedding
from llama_index.core.llms import MockLLM

def basic_example():
    """åŸºæœ¬ä½¿ç”¨ç¤ºä¾‹"""
    print("ğŸ“š LlamaIndex åŸºæœ¬ç¤ºä¾‹")
    print("-" * 40)
    
    # åˆ›å»ºæ–‡æ¡£
    documents = [
        Document(text="äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚"),
        Document(text="æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é›†ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ ï¼Œè€Œæ— éœ€æ˜ç¡®ç¼–ç¨‹ã€‚"),
        Document(text="æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„å¤æ‚æ¨¡å¼ã€‚"),
        Document(text="è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä¸“æ³¨äºä½¿è®¡ç®—æœºèƒ½å¤Ÿç†è§£ã€è§£é‡Šå’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚"),
    ]
    
    # ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å‹ï¼ˆä¸éœ€è¦APIå¯†é’¥ï¼‰
    Settings.embed_model = MockEmbedding(embed_dim=384)
    Settings.llm = MockLLM()
    
    # åˆ›å»ºç´¢å¼•
    print("ğŸ”§ åˆ›å»ºå‘é‡ç´¢å¼•...")
    index = VectorStoreIndex.from_documents(documents)
    print("âœ… ç´¢å¼•åˆ›å»ºæˆåŠŸï¼")
    
    # åˆ›å»ºæŸ¥è¯¢å¼•æ“
    query_engine = index.as_query_engine()
    
    # æµ‹è¯•æŸ¥è¯¢
    queries = [
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ",
        "NLPæ˜¯ä»€ä¹ˆï¼Ÿ"
    ]
    
    print("\nğŸ” æµ‹è¯•æŸ¥è¯¢:")
    for query in queries:
        print(f"\né—®é¢˜: {query}")
        # æ³¨æ„ï¼šä½¿ç”¨MockLLMæ—¶ï¼Œå“åº”æ˜¯æ¨¡æ‹Ÿçš„
        response = query_engine.query(query)
        print(f"å›ç­”: {response}")

def advanced_example():
    """é«˜çº§ç¤ºä¾‹ï¼šä½¿ç”¨å®é™…çš„åµŒå…¥æ¨¡å‹"""
    print("\n\nğŸš€ LlamaIndex é«˜çº§ç¤ºä¾‹")
    print("-" * 40)
    
    # å¦‚æœè¦ä½¿ç”¨OpenAI
    # os.environ['OPENAI_API_KEY'] = 'your-api-key'
    # from llama_index.llms.openai import OpenAI
    # from llama_index.embeddings.openai import OpenAIEmbedding
    # Settings.llm = OpenAI(model="gpt-3.5-turbo")
    # Settings.embed_model = OpenAIEmbedding()
    
    # ä½¿ç”¨æœ¬åœ°åµŒå…¥æ¨¡å‹
    try:
        from llama_index.embeddings.huggingface import HuggingFaceEmbedding
        
        print("ğŸ”§ åŠ è½½æœ¬åœ°åµŒå…¥æ¨¡å‹...")
        Settings.embed_model = HuggingFaceEmbedding(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        )
        
        # åˆ›å»ºæ›´å¤æ‚çš„æ–‡æ¡£
        documents = [
            Document(
                text="LlamaIndexæ˜¯ä¸€ä¸ªæ•°æ®æ¡†æ¶ï¼Œç”¨äºæ„å»ºLLMåº”ç”¨ç¨‹åºã€‚å®ƒæä¾›äº†æ•°æ®è¿æ¥å™¨ã€ç´¢å¼•ç»“æ„ã€æŸ¥è¯¢æ¥å£ç­‰åŠŸèƒ½ã€‚",
                metadata={"source": "documentation", "category": "framework"}
            ),
            Document(
                text="RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æ˜¯ä¸€ç§ç»“åˆæ£€ç´¢å’Œç”Ÿæˆçš„æŠ€æœ¯ï¼Œå¯ä»¥æé«˜LLMå›ç­”çš„å‡†ç¡®æ€§å’Œç›¸å…³æ€§ã€‚",
                metadata={"source": "tutorial", "category": "technique"}
            ),
        ]
        
        # åˆ›å»ºç´¢å¼•
        index = VectorStoreIndex.from_documents(documents)
        
        # é…ç½®æŸ¥è¯¢å‚æ•°
        query_engine = index.as_query_engine(
            similarity_top_k=2,  # è¿”å›æœ€ç›¸ä¼¼çš„2ä¸ªèŠ‚ç‚¹
            response_mode="compact"  # ç´§å‡‘çš„å“åº”æ¨¡å¼
        )
        
        print("âœ… é«˜çº§ç´¢å¼•åˆ›å»ºæˆåŠŸï¼")
        print("\nğŸ“ æ–‡æ¡£å…ƒæ•°æ®ç¤ºä¾‹:")
        for doc in documents:
            print(f"  - {doc.metadata}")
            
    except Exception as e:
        print(f"âš ï¸ é«˜çº§ç¤ºä¾‹éœ€è¦é¢å¤–é…ç½®: {e}")

def save_and_load_example():
    """æŒä¹…åŒ–ç¤ºä¾‹"""
    print("\n\nğŸ’¾ ç´¢å¼•æŒä¹…åŒ–ç¤ºä¾‹")
    print("-" * 40)
    
    # ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å‹
    Settings.embed_model = MockEmbedding(embed_dim=384)
    Settings.llm = MockLLM()
    
    # åˆ›å»ºå¹¶ä¿å­˜ç´¢å¼•
    documents = [Document(text="è¿™æ˜¯ä¸€ä¸ªæŒä¹…åŒ–æµ‹è¯•æ–‡æ¡£")]
    index = VectorStoreIndex.from_documents(documents)
    
    # ä¿å­˜ç´¢å¼•
    print("ğŸ’¾ ä¿å­˜ç´¢å¼•åˆ°ç£ç›˜...")
    index.storage_context.persist(persist_dir="./storage")
    print("âœ… ç´¢å¼•å·²ä¿å­˜ï¼")
    
    # åŠ è½½ç´¢å¼•
    print("\nğŸ“‚ ä»ç£ç›˜åŠ è½½ç´¢å¼•...")
    from llama_index.core import StorageContext, load_index_from_storage
    storage_context = StorageContext.from_defaults(persist_dir="./storage")
    loaded_index = load_index_from_storage(storage_context)
    print("âœ… ç´¢å¼•åŠ è½½æˆåŠŸï¼")

if __name__ == "__main__":
    # è¿è¡Œç¤ºä¾‹
    basic_example()
    advanced_example()
    save_and_load_example()
    
    print("\n\nğŸ‰ æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆï¼")
    print("\nğŸ’¡ æç¤º:")
    print("1. ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œå»ºè®®ä½¿ç”¨çœŸå®çš„LLMå’ŒåµŒå…¥æ¨¡å‹")
    print("2. å¯ä»¥é›†æˆå‘é‡æ•°æ®åº“ï¼ˆå¦‚Milvusï¼‰æ¥å­˜å‚¨å¤§è§„æ¨¡æ•°æ®")
    print("3. æŸ¥çœ‹å®˜æ–¹æ–‡æ¡£äº†è§£æ›´å¤šé«˜çº§åŠŸèƒ½ï¼šhttps://docs.llamaindex.ai/") 