import re
import logging
from typing import List, Dict, Any, Set, Tuple
import jieba
from .semantic_punctuation_service import semantic_punctuation_service

logger = logging.getLogger(__name__)

class TextOptimizationService:
    """
    çœŸæ­£çš„æ–‡æœ¬ä¼˜åŒ–æœåŠ¡ - ä¸“é—¨è§£å†³è¯­éŸ³è¯†åˆ«ä¸­çš„é‡å¤å¾ªç¯å’Œå†—ä½™
    æ ¸å¿ƒåŸåˆ™ï¼šæ£€æµ‹å¹¶ç§»é™¤é‡å¤ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´ï¼Œç»ä¸åˆ›é€ æ–°çš„é‡å¤
    """
    
    def __init__(self):
        # å¡«å……è¯ï¼ˆè¯­éŸ³è¯†åˆ«ä¸­çš„æ— æ„ä¹‰è¯æ±‡ï¼‰
        self.filler_words = {
            'å—¯', 'å•Š', 'å‘ƒ', 'å“¦', 'é¢', 'å””', 'å’³å’³', 'å—¯å—¯', 'é‚£ä¸ª', 'è¿™ä¸ª'
        }
        
        # é—®å¥æ ‡è¯†è¯
        self.question_indicators = ['ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å“ªé‡Œ', 'è°', 'å—', 'å‘¢', 'æ˜¯å§', 'å¯¹å§']
    
    def optimize_speech_text(self, text: str, language: str = 'zh') -> Dict[str, Any]:
        """
        çœŸæ­£ä¼˜åŒ–è¯­éŸ³è¯†åˆ«æ–‡æœ¬ - é‡ç‚¹è§£å†³é‡å¤å¾ªç¯é—®é¢˜
        """
        if not text or len(text.strip()) < 3:
            return self._create_result(False, text, "æ–‡æœ¬è¿‡çŸ­ï¼Œæ— éœ€ä¼˜åŒ–")
        
        original_text = text.strip()
        original_length = len(original_text)
        
        try:
            logger.info(f"ğŸ”§ å¼€å§‹æ–‡æœ¬ä¼˜åŒ–ï¼ŒåŸå§‹é•¿åº¦: {original_length}")
            
            optimized_text = original_text
            all_improvements = []
            
            # ç¬¬ä¸€æ­¥ï¼šæ£€æµ‹å¹¶ç§»é™¤é‡å¤å¾ªç¯ï¼ˆæœ€é‡è¦çš„æ­¥éª¤ï¼‰
            optimized_text, loop_improvements = self._remove_repetitive_loops(optimized_text)
            all_improvements.extend(loop_improvements)
            
            # ç¬¬äºŒæ­¥ï¼šç§»é™¤æ˜æ˜¾çš„å¡«å……è¯
            optimized_text, filler_improvements = self._clean_filler_words(optimized_text)
            all_improvements.extend(filler_improvements)
            
            # ç¬¬ä¸‰æ­¥ï¼šæ¸…ç†ç©ºæ ¼å’ŒåŸºæœ¬æ ‡ç‚¹
            optimized_text = self._normalize_whitespace(optimized_text)
            
            # ç¬¬å››æ­¥ï¼šè¯­ä¹‰åˆ†ææ·»åŠ æ™ºèƒ½æ ‡ç‚¹å’Œæ®µè½
            semantic_result = semantic_punctuation_service.add_intelligent_punctuation(optimized_text)
            if semantic_result['success']:
                optimized_text = semantic_result['processed_text']
                all_improvements.extend(semantic_result['improvements'])
            else:
                # å›é€€åˆ°åŸºç¡€æ ‡ç‚¹å¤„ç†
                optimized_text, punct_improvements = self._add_smart_punctuation(optimized_text)
                all_improvements.extend(punct_improvements)
            
            # æœ€ç»ˆå®‰å…¨æ£€æŸ¥
            if len(optimized_text) < original_length * 0.5:
                logger.warning("ä¼˜åŒ–ç»“æœè¿‡åº¦ç¼©çŸ­ï¼Œä¿ç•™åŸæ–‡")
                return self._create_result(True, original_text, "ä¿ç•™åŸæ–‡ä»¥ç¡®ä¿å®Œæ•´æ€§", 
                                         ["ä¿æŒåŸæ–‡"], original_length, original_length)
            
            optimized_length = len(optimized_text)
            
            logger.info(f"âœ… æ–‡æœ¬ä¼˜åŒ–å®Œæˆ")
            logger.info(f"ğŸ“ åŸå§‹é•¿åº¦: {original_length} -> ä¼˜åŒ–é•¿åº¦: {optimized_length}")
            logger.info(f"ğŸ“ åº”ç”¨æ”¹è¿›: {all_improvements}")
            
            return self._create_result(True, optimized_text, "ä¼˜åŒ–æˆåŠŸ", 
                                     all_improvements, original_length, optimized_length)
                
        except Exception as e:
            logger.error(f"æ–‡æœ¬ä¼˜åŒ–å¼‚å¸¸: {str(e)}")
            return self._create_result(True, original_text, f"ä¿ç•™åŸæ–‡: {str(e)}", 
                                     ["ä¿ç•™åŸæ–‡"], original_length, original_length)
    
    def _remove_repetitive_loops(self, text: str) -> Tuple[str, List[str]]:
        """
        æ£€æµ‹å¹¶ç§»é™¤é‡å¤å¾ªç¯ - è¿™æ˜¯è§£å†³ç”¨æˆ·é—®é¢˜çš„æ ¸å¿ƒæ–¹æ³•
        """
        improvements = []
        result_text = text
        
        # æ–¹æ³•1ï¼šæ£€æµ‹å®Œå…¨ç›¸åŒçš„å¥å­é‡å¤
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼Œï¼›]', result_text)
        cleaned_sentences = []
        sentence_count = {}
        
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) < 3:  # è·³è¿‡å¤ªçŸ­çš„ç‰‡æ®µ
                cleaned_sentences.append(sentence)
                continue
                
            # è®¡ç®—å¥å­å‡ºç°æ¬¡æ•°
            if sentence in sentence_count:
                sentence_count[sentence] += 1
                # å¦‚æœåŒä¸€å¥è¯å‡ºç°3æ¬¡ä»¥ä¸Šï¼Œåªä¿ç•™å‰2æ¬¡
                if sentence_count[sentence] <= 2:
                    cleaned_sentences.append(sentence)
                else:
                    improvements.append(f"ç§»é™¤é‡å¤å¥å­")
            else:
                sentence_count[sentence] = 1
                cleaned_sentences.append(sentence)
        
        if len(cleaned_sentences) < len(sentences):
            result_text = 'ã€‚'.join([s for s in cleaned_sentences if s.strip()])
            if not result_text.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ')):
                result_text += 'ã€‚'
        
        # æ–¹æ³•2ï¼šæ£€æµ‹çŸ­è¯­çº§åˆ«çš„é‡å¤
        result_text, phrase_improvements = self._remove_phrase_repetitions(result_text)
        improvements.extend(phrase_improvements)
        
        return result_text, improvements
    
    def _remove_phrase_repetitions(self, text: str) -> Tuple[str, List[str]]:
        """
        ç§»é™¤çŸ­è¯­çº§åˆ«çš„é‡å¤ï¼ˆå¦‚"æˆ‘æ²¡æœ‰çœ‹å“ªä¸ªç‚¹æ˜¯å§"é‡å¤å¤šæ¬¡ï¼‰
        """
        improvements = []
        
        # ä½¿ç”¨æ»‘åŠ¨çª—å£æ£€æµ‹é‡å¤çŸ­è¯­
        words = text.split()
        if len(words) < 8:
            return text, improvements
        
        # æ£€æŸ¥4-8ä¸ªè¯çš„çŸ­è¯­é‡å¤
        for phrase_len in range(4, 9):
            if len(words) < phrase_len * 3:  # è‡³å°‘éœ€è¦3æ¬¡é‡å¤æ‰å¤„ç†
                continue
                
            i = 0
            while i < len(words) - phrase_len * 2:
                # æå–å½“å‰çŸ­è¯­
                current_phrase = words[i:i+phrase_len]
                phrase_text = ' '.join(current_phrase)
                
                # æ£€æŸ¥åé¢æ˜¯å¦æœ‰è¿ç»­çš„é‡å¤
                repeat_count = 0
                j = i + phrase_len
                
                while j + phrase_len <= len(words):
                    next_phrase = words[j:j+phrase_len]
                    if current_phrase == next_phrase:
                        repeat_count += 1
                        j += phrase_len
                    else:
                        break
                
                # å¦‚æœå‘ç°è¿ç»­é‡å¤3æ¬¡ä»¥ä¸Šï¼Œåªä¿ç•™1æ¬¡
                if repeat_count >= 2:
                    # ç§»é™¤é‡å¤çš„éƒ¨åˆ†
                    words = words[:i+phrase_len] + words[j:]
                    improvements.append(f"ç§»é™¤é‡å¤çŸ­è¯­")
                    logger.info(f"ç§»é™¤é‡å¤çŸ­è¯­: '{phrase_text}' (é‡å¤{repeat_count}æ¬¡)")
                    break  # é‡æ–°å¼€å§‹æ£€æŸ¥
                else:
                    i += 1
        
        return ' '.join(words), improvements
    
    def _clean_filler_words(self, text: str) -> Tuple[str, List[str]]:
        """
        ç§»é™¤å¡«å……è¯ï¼Œä½†ä¿æŒå¥å­ç»“æ„
        """
        improvements = []
        
        # åˆ†è¯
        words = list(jieba.cut(text))
        cleaned_words = []
        removed_count = 0
        
        for i, word in enumerate(words):
            word_clean = word.strip()
            
            # åªç§»é™¤å•ç‹¬çš„å¡«å……è¯ï¼Œä¸å½±å“å¥å­ç»“æ„
            if (word_clean in self.filler_words and 
                len(word_clean) <= 2 and
                not self._is_important_context(words, i)):
                removed_count += 1
            else:
                cleaned_words.append(word)
        
        if removed_count > 0:
            improvements.append(f"ç§»é™¤{removed_count}ä¸ªå¡«å……è¯")
            logger.info(f"ç§»é™¤äº†{removed_count}ä¸ªå¡«å……è¯")
        
        return ''.join(cleaned_words), improvements
    
    def _is_important_context(self, words: List[str], index: int) -> bool:
        """
        åˆ¤æ–­å¡«å……è¯æ˜¯å¦åœ¨é‡è¦ä¸Šä¸‹æ–‡ä¸­ï¼ˆå¦‚æœæ˜¯ï¼Œåˆ™ä¿ç•™ï¼‰
        """
        if index == 0 or index == len(words) - 1:
            return False
        
        prev_word = words[index - 1].strip()
        next_word = words[index + 1].strip()
        
        # å¦‚æœå‰åéƒ½æ˜¯æ ‡ç‚¹ç¬¦å·ï¼Œå¯ä»¥å®‰å…¨ç§»é™¤
        if prev_word in 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š' and next_word in 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š':
            return False
        
        return True
    
    def _normalize_whitespace(self, text: str) -> str:
        """
        è§„èŒƒåŒ–ç©ºæ ¼å’Œæ ‡ç‚¹
        """
        # è§„èŒƒåŒ–ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text)
        # æ ‡ç‚¹ç¬¦å·å‰åçš„ç©ºæ ¼å¤„ç†
        text = re.sub(r'\s*([ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š])\s*', r'\1', text)
        # å»é™¤é¦–å°¾ç©ºæ ¼
        return text.strip()
    
    def _add_smart_punctuation(self, text: str) -> Tuple[str, List[str]]:
        """
        æ™ºèƒ½æ·»åŠ æ ‡ç‚¹ç¬¦å·
        """
        improvements = []
        result_text = text
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯é—®å¥
        is_question = any(indicator in text for indicator in self.question_indicators)
        
        if is_question and not text.endswith('ï¼Ÿ'):
            result_text = re.sub(r'[ã€‚ï¼ï¼›ï¼š]+$', '', result_text) + 'ï¼Ÿ'
            improvements.append("æ·»åŠ é—®å·")
        elif not re.search(r'[ã€‚ï¼ï¼Ÿï¼›ï¼š]$', text):
            result_text += 'ã€‚'
            improvements.append("æ·»åŠ å¥å·")
        
        return result_text, improvements
    
    def _create_result(self, success: bool, text: str, message: str, 
                      improvements: List[str] = None, 
                      original_length: int = 0, 
                      optimized_length: int = 0) -> Dict[str, Any]:
        """åˆ›å»ºç»“æœå¯¹è±¡"""
        return {
            'success': success,
            'optimized_text': text,
            'message': message,
            'improvements': improvements or [],
            'original_length': original_length,
            'optimized_length': optimized_length,
            'reduction_rate': round((original_length - optimized_length) / original_length * 100, 1) if original_length > 0 else 0
        } 