"""
Whisper Speech Recognition Service
ä¼˜åŒ–çš„è¯­éŸ³è¯†åˆ«æœåŠ¡ï¼Œé¿å…é‡å¤åŠ è½½æ¨¡å‹ï¼Œæ”¯æŒGPUåŠ é€Ÿ
"""
import logging
import time
import torch
import whisper
from pathlib import Path
from typing import Dict, Any, Optional
import os

logger = logging.getLogger(__name__)

class WhisperService:
    """
    å…¨å±€Whisperæ¨¡å‹ç®¡ç†å™¨ï¼Œé¿å…é‡å¤åŠ è½½ï¼Œæå‡æ€§èƒ½
    """
    _instance = None
    _model = None
    _device = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    @classmethod
    def get_instance(cls):
        """è·å–å•ä¾‹å®ä¾‹"""
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance
    
    @property
    def device(self):
        """è·å–å½“å‰è®¾å¤‡"""
        if self._device is None:
            if torch.cuda.is_available():
                self._device = "cuda"
                logger.info(f"ğŸš€ GPUåŠ é€Ÿå¯ç”¨: {torch.cuda.get_device_name()}")
                logger.info(f"ğŸ”§ CUDAç‰ˆæœ¬: {torch.version.cuda}")
                logger.info(f"ğŸ’¾ GPUæ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
            else:
                self._device = "cpu"
                logger.warning("âš ï¸ GPUä¸å¯ç”¨ï¼Œå›é€€åˆ°CPUæ¨¡å¼")
        return self._device
    
    @property
    def model(self):
        """è·å–æ¨¡å‹å®ä¾‹ï¼Œæ‡’åŠ è½½"""
        if self._model is None:
            logger.info("ğŸ¯ Loading Whisper model...")
            start_time = time.time()
            
            try:
                # ä¼˜å…ˆä½¿ç”¨Large V3æ¨¡å‹ï¼Œæä¾›æœ€ä½³è´¨é‡ï¼ˆå¯é€šè¿‡ç¯å¢ƒå˜é‡é…ç½®ï¼‰
                model_name = os.getenv("WHISPER_MODEL", "large-v3")  # é»˜è®¤ä½¿ç”¨large-v3
                
                if self.device == "cuda":
                    # GPUæ¨¡å¼ï¼šåŠ è½½æ¨¡å‹åˆ°GPUï¼Œè®©Whisperè‡ªå·±å¤„ç†ç²¾åº¦
                    logger.info(f"ğŸ”¥ GPUæ¨¡å¼å¯åŠ¨ä¸­ï¼Œä½¿ç”¨æ¨¡å‹: {model_name}")
                    self._model = whisper.load_model(model_name, device=self.device)
                    
                    # éªŒè¯GPUä½¿ç”¨æƒ…å†µï¼Œä½†ä¸æ‰‹åŠ¨è½¬æ¢ç²¾åº¦
                    gpu_memory = torch.cuda.memory_allocated() / 1024**3
                    logger.info(f"ğŸ“Š GPUæ˜¾å­˜ä½¿ç”¨: {gpu_memory:.2f}GB")
                    logger.info("âœ… æ¨¡å‹å·²åŠ è½½åˆ°GPUï¼Œå°†åœ¨è½¬å½•æ—¶ä½¿ç”¨FP16")
                    
                else:
                    # CPUæ¨¡å¼
                    logger.info(f"ğŸ’» CPUæ¨¡å¼å¯åŠ¨ä¸­ï¼Œä½¿ç”¨æ¨¡å‹: {model_name}")
                    self._model = whisper.load_model(model_name, device=self.device)
                
                load_time = time.time() - start_time
                logger.info(f"ğŸ‰ Whisperæ¨¡å‹({model_name})åŠ è½½æˆåŠŸï¼Œè€—æ—¶ {load_time:.2f}ç§’ï¼Œè®¾å¤‡: {self.device}")
                
            except Exception as e:
                logger.error(f"âŒ ä¸»æ¨¡å‹({model_name})åŠ è½½å¤±è´¥: {e}")
                # é™çº§ç­–ç•¥ï¼šlarge-v3 -> turbo -> base
                fallback_models = ["turbo", "base"] if model_name == "large-v3" else ["base"]
                
                for fallback_model in fallback_models:
                    try:
                        logger.info(f"ğŸ”„ é™çº§åˆ°{fallback_model}æ¨¡å‹...")
                        self._model = whisper.load_model(fallback_model, device=self.device)
                        load_time = time.time() - start_time
                        logger.info(f"âœ… {fallback_model}æ¨¡å‹åŠ è½½æˆåŠŸï¼Œè€—æ—¶ {load_time:.2f}ç§’")
                        break
                    except Exception as fallback_e:
                        logger.error(f"âŒ {fallback_model}æ¨¡å‹ä¹ŸåŠ è½½å¤±è´¥: {fallback_e}")
                        continue
                
                if self._model is None:
                    logger.error("ğŸ’¥ æ‰€æœ‰Whisperæ¨¡å‹éƒ½åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®‰è£…")
                    raise RuntimeError("æ— æ³•åŠ è½½ä»»ä½•Whisperæ¨¡å‹")
        
        return self._model
    
    def transcribe_audio(self, audio_path: Path, language: str = "zh") -> Dict[str, Any]:
        """
        ä¼˜åŒ–çš„éŸ³é¢‘è½¬å½•åŠŸèƒ½
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            language: è¯­è¨€ä»£ç ï¼Œé»˜è®¤ä¸­æ–‡
            
        Returns:
            è½¬å½•ç»“æœå­—å…¸
        """
        try:
            start_time = time.time()
            
            # æ”¯æŒå­—ç¬¦ä¸²è·¯å¾„å’ŒPathå¯¹è±¡
            audio_name = audio_path.name if hasattr(audio_path, 'name') else str(audio_path).split('/')[-1]
            logger.info(f"ğŸµ å¼€å§‹è½¬å½•éŸ³é¢‘: {audio_name}")
            logger.info(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")
            
            # è·å–å®é™…éŸ³é¢‘æ—¶é•¿
            import librosa
            actual_duration = librosa.get_duration(path=str(audio_path))
            logger.info(f"ğŸ“Š éŸ³é¢‘æ—¶é•¿: {actual_duration:.2f}ç§’")
            
            # è·å–æ¨¡å‹å¹¶éªŒè¯GPUçŠ¶æ€
            model_to_use = self.model
            gpu_enabled = self.device == "cuda"
            
            if gpu_enabled:
                logger.info("âš¡ GPUåŠ é€Ÿæ¨¡å¼å·²æ¿€æ´»")
                # è®¾ç½®GPUè®¾å¤‡
                torch.cuda.set_device(0)
            
            # è½¬å½•é…ç½®ä¼˜åŒ–ï¼ˆé˜²æ­¢é‡å¤å¾ªç¯ï¼‰
            transcribe_options = {
                "language": language,
                "task": "transcribe",
                "fp16": gpu_enabled,  # ä»…åœ¨GPUæ—¶ä½¿ç”¨FP16
                "verbose": False,
                # ä¼˜åŒ–å‚æ•°ï¼šé˜²æ­¢é‡å¤å¾ªç¯
                "beam_size": 1,  # é™ä½ä¸º1ï¼Œå‡å°‘é‡å¤å¯èƒ½æ€§
                "best_of": 1,  # é™ä½ä¸º1ï¼Œå‡å°‘é‡å¤å¯èƒ½æ€§
                "temperature": 0.2,  # å¢åŠ ä¸€ç‚¹éšæœºæ€§ï¼Œé˜²æ­¢å¡ä½
                "compression_ratio_threshold": 2.0,  # é™ä½é˜ˆå€¼ï¼Œå‡å°‘é‡å¤
                "logprob_threshold": -0.8,  # æé«˜é˜ˆå€¼ï¼Œå‡å°‘ä½è´¨é‡é‡å¤
                "no_speech_threshold": 0.8,  # æé«˜é˜ˆå€¼ï¼Œæ›´ä¸¥æ ¼çš„é™éŸ³æ£€æµ‹
                "condition_on_previous_text": False,  # ç¦ç”¨ä¸Šä¸‹æ–‡ä¾èµ–ï¼Œé˜²æ­¢é‡å¤å¾ªç¯
            }
            
            logger.info(f"ğŸ”§ è½¬å½•é…ç½®: {transcribe_options}")
            
            # æ‰§è¡Œè½¬å½•
            transcribe_start = time.time()
            result = model_to_use.transcribe(str(audio_path), **transcribe_options)
            transcribe_time = time.time() - transcribe_start
            
            # è§£æç»“æœ
            if not result or not result.get("segments"):
                logger.warning("âš ï¸ æœªæ£€æµ‹åˆ°è¯­éŸ³å†…å®¹")
                return {
                    "text": "",
                    "language": language,
                    "duration": actual_duration,  # å³ä½¿æ²¡æœ‰è¯­éŸ³å†…å®¹ï¼Œä¹Ÿè¿”å›å®é™…æ—¶é•¿
                    "confidence": 0.0,
                    "gpu_accelerated": gpu_enabled,
                    "processing_time": time.time() - start_time,
                    "transcription_time": transcribe_time,
                    "error": "No speech content detected"
                }
            
            # è®¡ç®—ç½®ä¿¡åº¦
            segments = result.get("segments", [])
            confidences = []
            
            for segment in segments:
                if "avg_logprob" in segment:
                    confidence = min(1.0, max(0.0, (segment["avg_logprob"] + 1.0)))
                    confidences.append(confidence)
                elif "no_speech_prob" in segment:
                    confidence = 1.0 - segment["no_speech_prob"]
                    confidences.append(confidence)
            
            avg_confidence = sum(confidences) / len(confidences) if confidences else 0.5
            
            total_time = time.time() - start_time
            
            result_data = {
                "text": result["text"].strip(),
                "language": result.get("language", language),
                "duration": actual_duration,  # ä½¿ç”¨å®é™…éŸ³é¢‘æ—¶é•¿
                "confidence": round(avg_confidence, 3),
                "gpu_accelerated": gpu_enabled,
                "processing_time": round(total_time, 2),
                "transcription_time": round(transcribe_time, 2),
                "segments_count": len(segments),
                "model_used": "large-v3" if hasattr(model_to_use, 'encoder') else "turbo"
            }
            
            logger.info(f"âœ… è½¬å½•å®Œæˆ: {total_time:.2f}ç§’ (è½¬å½•: {transcribe_time:.2f}ç§’)")
            logger.info(f"ğŸ“ è¯†åˆ«æ–‡æœ¬é•¿åº¦: {len(result_data['text'])} å­—ç¬¦")
            logger.info(f"ğŸ¯ å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.3f}")
            logger.info(f"âš¡ GPUåŠ é€Ÿ: {'æ˜¯' if gpu_enabled else 'å¦'}")
            
            return result_data
            
        except Exception as e:
            error_time = time.time() - start_time
            logger.error(f"âŒ è½¬å½•å¤±è´¥ ({error_time:.2f}ç§’): {e}", exc_info=True)
            
            return {
                "text": "",
                "language": language,
                "duration": 0,  # é”™è¯¯æƒ…å†µä¸‹æ— æ³•è·å–æ—¶é•¿
                "confidence": 0.0,
                "gpu_accelerated": self.device == "cuda",
                "processing_time": error_time,
                "error": str(e)
            }

# å…¨å±€å®ä¾‹
whisper_service = WhisperService.get_instance() 