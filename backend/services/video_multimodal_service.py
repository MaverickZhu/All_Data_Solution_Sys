"""
è§†é¢‘å¤šæ¨¡æ€è¯­ä¹‰èåˆæœåŠ¡
å°†è§†è§‰åˆ†æå’ŒéŸ³é¢‘åˆ†æç»“æœè¿›è¡Œæ™ºèƒ½èåˆï¼Œç”Ÿæˆç»¼åˆçš„è§†é¢‘ç†è§£æŠ¥å‘Š
"""
import logging
import asyncio
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import numpy as np
from datetime import datetime, timezone
import json
import re
from collections import defaultdict, Counter

logger = logging.getLogger("service")


class VideoMultimodalService:
    """
    è§†é¢‘å¤šæ¨¡æ€è¯­ä¹‰èåˆæœåŠ¡
    ä¸“é—¨å¤„ç†è§†è§‰å’ŒéŸ³é¢‘ä¿¡æ¯çš„æ™ºèƒ½èåˆ
    """
    
    def __init__(self):
        # å»¶è¿Ÿå¯¼å…¥LLMæœåŠ¡
        self._llm_service = None
        logger.info("ğŸ”— å¤šæ¨¡æ€è¯­ä¹‰èåˆæœåŠ¡åˆå§‹åŒ–å®Œæˆ")
    
    @property
    def llm_service(self):
        """å»¶è¿ŸåŠ è½½LLMæœåŠ¡"""
        if self._llm_service is None:
            from backend.services.llm_service import LLMService
            self._llm_service = LLMService()
        return self._llm_service
    
    async def fuse_multimodal_analysis(
        self, 
        visual_results: Dict[str, Any], 
        audio_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        èåˆå¤šæ¨¡æ€åˆ†æç»“æœ - ä¼˜åŒ–ç‰ˆæœ¬
        å°†åŸæ¥çš„16æ¬¡LLMè°ƒç”¨ä¼˜åŒ–ä¸º3æ¬¡é«˜æ•ˆè°ƒç”¨
        
        Args:
            visual_results: è§†è§‰åˆ†æç»“æœ
            audio_results: éŸ³é¢‘åˆ†æç»“æœ
            
        Returns:
            èåˆåçš„å¤šæ¨¡æ€åˆ†æç»“æœ
        """
        try:
            logger.info("å¼€å§‹ä¼˜åŒ–çš„å¤šæ¨¡æ€è¯­ä¹‰èåˆ...")
            
            # 1. æ—¶é—´è½´å¯¹é½ï¼ˆæ— éœ€LLMï¼Œçº¯ç®—æ³•å¤„ç†ï¼‰
            logger.info("ç¬¬1æ­¥ï¼šæ—¶é—´è½´å¯¹é½")
            timeline_alignment = await self._align_timelines(visual_results, audio_results)
            
            # 2. è§†è§‰å†…å®¹ç»Ÿä¸€åˆ†æï¼ˆ1æ¬¡LLMè°ƒç”¨ï¼‰
            logger.info("ç¬¬2æ­¥ï¼šè§†è§‰å†…å®¹ç»¼åˆåˆ†æ")
            visual_comprehensive = await self._analyze_visual_comprehensive(visual_results)
            
            # 3. éŸ³é¢‘å†…å®¹ç»Ÿä¸€åˆ†æï¼ˆ1æ¬¡LLMè°ƒç”¨ï¼‰
            logger.info("ç¬¬3æ­¥ï¼šéŸ³é¢‘å†…å®¹ç»¼åˆåˆ†æ") 
            audio_comprehensive = await self._analyze_audio_comprehensive(audio_results)
            
            # 4. æœ€ç»ˆå¤šæ¨¡æ€æ•´åˆï¼ˆ1æ¬¡LLMè°ƒç”¨ï¼‰
            logger.info("ç¬¬4æ­¥ï¼šå¤šæ¨¡æ€æœ€ç»ˆæ•´åˆ")
            final_integration = await self._integrate_multimodal_final(
                visual_comprehensive, audio_comprehensive, timeline_alignment
            )
            
            # 5. æ„å»ºèåˆç»“æœ
            fusion_result = {
                "timeline_alignment": timeline_alignment,
                "visual_comprehensive": visual_comprehensive,
                "audio_comprehensive": audio_comprehensive,
                "final_integration": final_integration,
                "fusion_metadata": {
                    "fusion_timestamp": datetime.now(timezone.utc).isoformat(),
                    "fusion_type": "optimized_multimodal_fusion",
                    "modalities_fused": ["visual", "audio"],
                    "llm_calls_count": 3,  # ä¼˜åŒ–åä»…3æ¬¡LLMè°ƒç”¨
                    "optimization_strategy": "consolidated_analysis"
                }
            }
            
            logger.info("ä¼˜åŒ–çš„å¤šæ¨¡æ€è¯­ä¹‰èåˆå®Œæˆï¼ˆä»…3æ¬¡LLMè°ƒç”¨ï¼‰")
            return fusion_result
            
        except Exception as e:
            logger.error(f"å¤šæ¨¡æ€è¯­ä¹‰èåˆå¤±è´¥: {e}")
            return {
                "error": str(e),
                "timeline_alignment": {},
                "visual_comprehensive": {},
                "audio_comprehensive": {},
                "final_integration": {}
            }

    async def _analyze_visual_comprehensive(self, visual_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        è§†è§‰å†…å®¹ç»¼åˆåˆ†æ - å•æ¬¡LLMè°ƒç”¨å¤„ç†æ‰€æœ‰è§†è§‰åˆ†æ
        æ›¿ä»£åŸæ¥çš„å¤šæ¬¡åˆ†æ•£è°ƒç”¨
        """
        try:
            # æå–æ‰€æœ‰è§†è§‰ä¿¡æ¯
            visual_analysis = visual_results.get("visual_analysis", {})
            scene_detection = visual_results.get("scene_detection", {})
            frame_extraction = visual_analysis.get("frame_extraction", {})
            
            # æ„å»ºç»¼åˆåˆ†ææç¤º
            comprehensive_prompt = f"""
            è¯·å¯¹ä»¥ä¸‹è§†é¢‘è§†è§‰å†…å®¹è¿›è¡Œç»¼åˆåˆ†æï¼Œè¿”å›JSONæ ¼å¼ç»“æœï¼š

            è§†è§‰ä¸»é¢˜: {visual_analysis.get("visual_themes", [])}
            æ£€æµ‹å¯¹è±¡: {visual_analysis.get("detected_objects", [])}
            åœºæ™¯ä¿¡æ¯: {scene_detection}
            å…³é”®å¸§ä¿¡æ¯: {frame_extraction}

            è¯·è¿”å›JSONæ ¼å¼çš„ç»¼åˆåˆ†æï¼š
            {{
                "visual_summary": "è§†è§‰å†…å®¹æ€»ä½“æè¿°",
                "main_themes": ["ä¸»è¦è§†è§‰ä¸»é¢˜1", "ä¸»è¦è§†è§‰ä¸»é¢˜2"],
                "scene_types": ["åœºæ™¯ç±»å‹1", "åœºæ™¯ç±»å‹2"],
                "visual_emotion": "ä»è§†è§‰æ¨æ–­çš„æ•´ä½“æƒ…æ„Ÿ",
                "key_objects": ["é‡è¦å¯¹è±¡1", "é‡è¦å¯¹è±¡2"],
                "visual_style": "è§†è§‰é£æ ¼æè¿°",
                "scene_progression": "åœºæ™¯å˜åŒ–æè¿°"
            }}
            """
            
            response = await self.llm_service.generate_response(comprehensive_prompt, timeout=30)
            
            try:
                return json.loads(response)
            except json.JSONDecodeError:
                return {
                    "visual_summary": "è§†è§‰åˆ†æå¤±è´¥",
                    "main_themes": visual_analysis.get("visual_themes", [])[:3],
                    "scene_types": ["unknown"],
                    "visual_emotion": "neutral",
                    "key_objects": visual_analysis.get("detected_objects", [])[:5],
                    "visual_style": "unknown",
                    "scene_progression": "æ— æ³•åˆ†æ"
                }
                
        except Exception as e:
            logger.error(f"è§†è§‰ç»¼åˆåˆ†æå¤±è´¥: {e}")
            return {"error": str(e)}

    async def _analyze_audio_comprehensive(self, audio_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        éŸ³é¢‘å†…å®¹ç»¼åˆåˆ†æ - å•æ¬¡LLMè°ƒç”¨å¤„ç†æ‰€æœ‰éŸ³é¢‘åˆ†æ
        æ›¿ä»£åŸæ¥çš„å¤šæ¬¡åˆ†æ•£è°ƒç”¨
        """
        try:
            # æå–æ‰€æœ‰éŸ³é¢‘ä¿¡æ¯
            enhanced_speech = audio_results.get("enhanced_speech", {})
            semantic_analysis = audio_results.get("semantic_analysis", {})
            timeline_analysis = audio_results.get("timeline_analysis", {})
            
            # æ„å»ºç»¼åˆåˆ†ææç¤º
            comprehensive_prompt = f"""
            è¯·å¯¹ä»¥ä¸‹è§†é¢‘éŸ³é¢‘å†…å®¹è¿›è¡Œç»¼åˆåˆ†æï¼Œè¿”å›JSONæ ¼å¼ç»“æœï¼š

            è¯­éŸ³è½¬å½•: {enhanced_speech.get("full_text", "")[:1000]}
            ä¸»è¦è¯é¢˜: {semantic_analysis.get("topic_analysis", {})}
            æƒ…æ„Ÿåˆ†æ: {semantic_analysis.get("emotion_analysis", {})}
            æ—¶é—´è½´åˆ†æ: {timeline_analysis}

            è¯·è¿”å›JSONæ ¼å¼çš„ç»¼åˆåˆ†æï¼š
            {{
                "audio_summary": "éŸ³é¢‘å†…å®¹æ€»ä½“æè¿°",
                "main_topics": ["ä¸»è¦è¯é¢˜1", "ä¸»è¦è¯é¢˜2"],
                "overall_emotion": "æ•´ä½“æƒ…æ„Ÿå€¾å‘",
                "speech_quality": "è¯­éŸ³è´¨é‡è¯„ä¼°",
                "key_messages": ["å…³é”®ä¿¡æ¯1", "å…³é”®ä¿¡æ¯2"],
                "audio_atmosphere": "éŸ³é¢‘æ°›å›´æè¿°",
                "content_structure": "å†…å®¹ç»“æ„åˆ†æ"
            }}
            """
            
            response = await self.llm_service.generate_response(comprehensive_prompt, timeout=30)
            
            try:
                return json.loads(response)
            except json.JSONDecodeError:
                topic_analysis = semantic_analysis.get("topic_analysis", {})
                emotion_analysis = semantic_analysis.get("emotion_analysis", {})
                return {
                    "audio_summary": "éŸ³é¢‘åˆ†æå¤±è´¥",
                    "main_topics": topic_analysis.get("main_topics", [])[:3],
                    "overall_emotion": emotion_analysis.get("overall_emotion", {}).get("dominant_emotion", "neutral"),
                    "speech_quality": "unknown",
                    "key_messages": [],
                    "audio_atmosphere": "unknown",
                    "content_structure": "æ— æ³•åˆ†æ"
                }
                
        except Exception as e:
            logger.error(f"éŸ³é¢‘ç»¼åˆåˆ†æå¤±è´¥: {e}")
            return {"error": str(e)}

    async def _integrate_multimodal_final(
        self, 
        visual_comprehensive: Dict[str, Any], 
        audio_comprehensive: Dict[str, Any],
        timeline_alignment: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        æœ€ç»ˆå¤šæ¨¡æ€æ•´åˆ - å•æ¬¡LLMè°ƒç”¨æ•´åˆæ‰€æœ‰ä¿¡æ¯
        è¿™æ˜¯æ•´ä¸ªåˆ†æè¿‡ç¨‹çš„æœ€ç»ˆæ•´åˆæ­¥éª¤
        """
        try:
            # æ„å»ºæœ€ç»ˆæ•´åˆæç¤º
            final_prompt = f"""
            è¯·åŸºäºè§†è§‰å’ŒéŸ³é¢‘çš„ç»¼åˆåˆ†æï¼Œè¿›è¡Œæœ€ç»ˆçš„å¤šæ¨¡æ€æ•´åˆï¼Œè¿”å›JSONæ ¼å¼ç»“æœï¼š

            è§†è§‰åˆ†æç»“æœ: {visual_comprehensive}
            éŸ³é¢‘åˆ†æç»“æœ: {audio_comprehensive}
            æ—¶é—´è½´å¯¹é½ä¿¡æ¯: {timeline_alignment.get("modality_coverage", {})}

            è¯·è¿”å›JSONæ ¼å¼çš„æœ€ç»ˆæ•´åˆåˆ†æï¼š
            {{
                "story_narrative": "å®Œæ•´çš„æ•…äº‹å™è¿°",
                "multimodal_coherence": 0.85,
                "key_moments": [
                    {{"timestamp": 0.0, "description": "å…³é”®æ—¶åˆ»æè¿°", "modalities": ["visual", "audio"]}}
                ],
                "overall_theme": "æ•´ä½“ä¸»é¢˜",
                "emotional_arc": "æƒ…æ„Ÿå˜åŒ–è½¨è¿¹",
                "content_summary": "å†…å®¹æ€»ç»“",
                "production_insights": "åˆ¶ä½œè§è§£",
                "audience_appeal": "å—ä¼—å¸å¼•åŠ›åˆ†æ"
            }}
            """
            
            response = await self.llm_service.generate_response(final_prompt, timeout=40)
            
            try:
                return json.loads(response)
            except json.JSONDecodeError:
                return {
                    "story_narrative": "å¤šæ¨¡æ€æ•´åˆå¤±è´¥",
                    "multimodal_coherence": 0.5,
                    "key_moments": [],
                    "overall_theme": visual_comprehensive.get("main_themes", ["unknown"])[0] if visual_comprehensive.get("main_themes") else "unknown",
                    "emotional_arc": f"è§†è§‰: {visual_comprehensive.get('visual_emotion', 'unknown')}, éŸ³é¢‘: {audio_comprehensive.get('overall_emotion', 'unknown')}",
                    "content_summary": f"è§†è§‰: {visual_comprehensive.get('visual_summary', 'unknown')[:100]}; éŸ³é¢‘: {audio_comprehensive.get('audio_summary', 'unknown')[:100]}",
                    "production_insights": "åˆ†æå¤±è´¥",
                    "audience_appeal": "æ— æ³•è¯„ä¼°"
                }
                
        except Exception as e:
            logger.error(f"æœ€ç»ˆå¤šæ¨¡æ€æ•´åˆå¤±è´¥: {e}")
            return {"error": str(e)}
    
    async def _align_timelines(
        self, 
        visual_results: Dict[str, Any], 
        audio_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        å¯¹é½è§†è§‰å’ŒéŸ³é¢‘çš„æ—¶é—´è½´
        """
        try:
            logger.info("æ‰§è¡Œæ—¶é—´è½´å¯¹é½...")
            
            # æå–è§†è§‰æ—¶é—´è½´ä¿¡æ¯
            visual_timeline = self._extract_visual_timeline(visual_results)
            
            # æå–éŸ³é¢‘æ—¶é—´è½´ä¿¡æ¯
            audio_timeline = self._extract_audio_timeline(audio_results)
            
            # åˆ›å»ºç»Ÿä¸€æ—¶é—´è½´
            unified_timeline = self._create_unified_timeline(visual_timeline, audio_timeline)
            
            # æ—¶é—´æ®µåŒ¹é…
            temporal_segments = self._match_temporal_segments(visual_timeline, audio_timeline)
            
            # åŒæ­¥äº‹ä»¶æ£€æµ‹
            sync_events = self._detect_sync_events(visual_timeline, audio_timeline)
            
            alignment_result = {
                "visual_timeline": visual_timeline,
                "audio_timeline": audio_timeline,
                "unified_timeline": unified_timeline,
                "temporal_segments": temporal_segments,
                "sync_events": sync_events,
                "alignment_quality": self._calculate_alignment_quality(temporal_segments, sync_events)
            }
            
            logger.info(f"æ—¶é—´è½´å¯¹é½å®Œæˆ: {len(temporal_segments)}ä¸ªæ—¶é—´æ®µ, {len(sync_events)}ä¸ªåŒæ­¥äº‹ä»¶")
            return alignment_result
            
        except Exception as e:
            logger.error(f"æ—¶é—´è½´å¯¹é½å¤±è´¥: {e}")
            return {
                "error": str(e),
                "visual_timeline": {},
                "audio_timeline": {},
                "unified_timeline": {},
                "temporal_segments": [],
                "sync_events": []
            }
    
    def _extract_visual_timeline(self, visual_results: Dict[str, Any]) -> Dict[str, Any]:
        """æå–è§†è§‰æ—¶é—´è½´ä¿¡æ¯"""
        try:
            frame_analyses = visual_results.get("visual_analysis", {}).get("frame_analyses", [])
            
            visual_events = []
            for frame in frame_analyses:
                visual_events.append({
                    "timestamp": frame.get("timestamp", 0),
                    "frame_number": frame.get("frame_number", 0),
                    "scene_type": frame.get("scene_type", "unknown"),
                    "visual_themes": frame.get("visual_themes", []),
                    "detected_objects": frame.get("detected_objects", []),
                    "confidence": frame.get("confidence", 0.0)
                })
            
            # åœºæ™¯å˜åŒ–æ£€æµ‹
            scene_changes = []
            prev_scene = None
            for event in visual_events:
                current_scene = event["scene_type"]
                if prev_scene and current_scene != prev_scene:
                    scene_changes.append({
                        "timestamp": event["timestamp"],
                        "from_scene": prev_scene,
                        "to_scene": current_scene,
                        "change_type": "scene_transition"
                    })
                prev_scene = current_scene
            
            return {
                "visual_events": visual_events,
                "scene_changes": scene_changes,
                "total_duration": max([e["timestamp"] for e in visual_events], default=0),
                "total_frames": len(visual_events)
            }
            
        except Exception as e:
            logger.error(f"è§†è§‰æ—¶é—´è½´æå–å¤±è´¥: {e}")
            return {"visual_events": [], "scene_changes": [], "total_duration": 0, "total_frames": 0}
    
    def _extract_audio_timeline(self, audio_results: Dict[str, Any]) -> Dict[str, Any]:
        """æå–éŸ³é¢‘æ—¶é—´è½´ä¿¡æ¯"""
        try:
            enhanced_speech = audio_results.get("enhanced_speech", {})
            timeline_analysis = audio_results.get("timeline_analysis", {})
            semantic_analysis = audio_results.get("semantic_analysis", {})
            
            # è¯­éŸ³ç‰‡æ®µ
            speech_segments = enhanced_speech.get("segments", [])
            
            # æƒ…æ„Ÿå˜åŒ–
            emotion_changes = semantic_analysis.get("emotion_analysis", {}).get("emotion_changes", [])
            
            # è¯­éŸ³æ´»åŠ¨
            speech_activity = timeline_analysis.get("speech_activity", {}).get("active_periods", [])
            
            # åœé¡¿åˆ†æ
            pauses = timeline_analysis.get("pause_analysis", {})
            
            audio_events = []
            
            # æ·»åŠ è¯­éŸ³ç‰‡æ®µäº‹ä»¶
            for segment in speech_segments:
                audio_events.append({
                    "timestamp": segment.get("start_time", 0),
                    "end_time": segment.get("end_time", 0),
                    "event_type": "speech_segment",
                    "content": segment.get("text", ""),
                    "confidence": segment.get("confidence", 0.0)
                })
            
            # æ·»åŠ æƒ…æ„Ÿå˜åŒ–äº‹ä»¶
            for emotion_change in emotion_changes:
                audio_events.append({
                    "timestamp": emotion_change.get("timestamp", 0),
                    "event_type": "emotion_change",
                    "from_emotion": emotion_change.get("from_emotion", "unknown"),
                    "to_emotion": emotion_change.get("to_emotion", "unknown")
                })
            
            # æ’åºäº‹ä»¶
            audio_events.sort(key=lambda x: x["timestamp"])
            
            return {
                "audio_events": audio_events,
                "speech_segments": speech_segments,
                "emotion_changes": emotion_changes,
                "speech_activity": speech_activity,
                "pause_info": pauses,
                "total_duration": enhanced_speech.get("total_duration", 0)
            }
            
        except Exception as e:
            logger.error(f"éŸ³é¢‘æ—¶é—´è½´æå–å¤±è´¥: {e}")
            return {
                "audio_events": [], "speech_segments": [], "emotion_changes": [],
                "speech_activity": [], "pause_info": {}, "total_duration": 0
            }
    
    def _create_unified_timeline(
        self, 
        visual_timeline: Dict[str, Any], 
        audio_timeline: Dict[str, Any]
    ) -> Dict[str, Any]:
        """åˆ›å»ºç»Ÿä¸€æ—¶é—´è½´"""
        try:
            # ç¡®å®šæ€»æ—¶é•¿
            visual_duration = visual_timeline.get("total_duration", 0)
            audio_duration = audio_timeline.get("total_duration", 0)
            total_duration = max(visual_duration, audio_duration)
            
            # åˆ›å»ºæ—¶é—´æ®µï¼ˆæ¯ç§’ä¸€ä¸ªæ®µï¼‰
            time_segments = []
            segment_duration = 1.0  # 1ç§’ä¸€ä¸ªæ®µ
            
            for i in range(int(total_duration) + 1):
                start_time = i * segment_duration
                end_time = min((i + 1) * segment_duration, total_duration)
                
                # æ‰¾åˆ°è¿™ä¸ªæ—¶é—´æ®µå†…çš„è§†è§‰å’ŒéŸ³é¢‘äº‹ä»¶
                visual_events_in_segment = self._find_events_in_timerange(
                    visual_timeline.get("visual_events", []), start_time, end_time
                )
                
                audio_events_in_segment = self._find_events_in_timerange(
                    audio_timeline.get("audio_events", []), start_time, end_time
                )
                
                time_segments.append({
                    "segment_id": i,
                    "start_time": start_time,
                    "end_time": end_time,
                    "duration": end_time - start_time,
                    "visual_events": visual_events_in_segment,
                    "audio_events": audio_events_in_segment,
                    "has_visual": len(visual_events_in_segment) > 0,
                    "has_audio": len(audio_events_in_segment) > 0,
                    "modality_overlap": len(visual_events_in_segment) > 0 and len(audio_events_in_segment) > 0
                })
            
            return {
                "total_duration": total_duration,
                "segment_duration": segment_duration,
                "total_segments": len(time_segments),
                "time_segments": time_segments,
                "modality_coverage": {
                    "visual_coverage": sum(1 for s in time_segments if s["has_visual"]) / len(time_segments) if time_segments else 0,
                    "audio_coverage": sum(1 for s in time_segments if s["has_audio"]) / len(time_segments) if time_segments else 0,
                    "overlap_coverage": sum(1 for s in time_segments if s["modality_overlap"]) / len(time_segments) if time_segments else 0
                }
            }
            
        except Exception as e:
            logger.error(f"ç»Ÿä¸€æ—¶é—´è½´åˆ›å»ºå¤±è´¥: {e}")
            return {"total_duration": 0, "time_segments": [], "modality_coverage": {}}
    
    def _find_events_in_timerange(
        self, 
        events: List[Dict], 
        start_time: float, 
        end_time: float
    ) -> List[Dict]:
        """æŸ¥æ‰¾æ—¶é—´èŒƒå›´å†…çš„äº‹ä»¶"""
        matching_events = []
        
        for event in events:
            event_time = event.get("timestamp", 0)
            event_end = event.get("end_time", event_time)
            
            # æ£€æŸ¥äº‹ä»¶æ˜¯å¦åœ¨æ—¶é—´èŒƒå›´å†…
            if (event_time >= start_time and event_time < end_time) or \
               (event_end > start_time and event_end <= end_time) or \
               (event_time <= start_time and event_end >= end_time):
                matching_events.append(event)
        
        return matching_events
    
    def _match_temporal_segments(
        self, 
        visual_timeline: Dict[str, Any], 
        audio_timeline: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """åŒ¹é…æ—¶é—´æ®µ"""
        try:
            matched_segments = []
            
            # åŸºäºåœºæ™¯å˜åŒ–å’Œè¯­éŸ³ç‰‡æ®µåˆ›å»ºåŒ¹é…æ®µ
            visual_events = visual_timeline.get("visual_events", [])
            audio_segments = audio_timeline.get("speech_segments", [])
            
            # ä¸ºæ¯ä¸ªè¯­éŸ³ç‰‡æ®µæ‰¾åˆ°å¯¹åº”çš„è§†è§‰å†…å®¹
            for audio_segment in audio_segments:
                start_time = audio_segment.get("start_time", 0)
                end_time = audio_segment.get("end_time", 0)
                
                # æ‰¾åˆ°æ—¶é—´èŒƒå›´å†…çš„è§†è§‰å¸§
                matching_visual = self._find_events_in_timerange(visual_events, start_time, end_time)
                
                if matching_visual:
                    # æå–è§†è§‰ç‰¹å¾
                    visual_themes = set()
                    detected_objects = set()
                    scene_types = set()
                    
                    for visual_event in matching_visual:
                        visual_themes.update(visual_event.get("visual_themes", []))
                        detected_objects.update(visual_event.get("detected_objects", []))
                        scene_types.add(visual_event.get("scene_type", "unknown"))
                    
                    matched_segments.append({
                        "start_time": start_time,
                        "end_time": end_time,
                        "duration": end_time - start_time,
                        "audio_content": audio_segment.get("text", ""),
                        "audio_confidence": audio_segment.get("confidence", 0.0),
                        "visual_themes": list(visual_themes),
                        "detected_objects": list(detected_objects),
                        "scene_types": list(scene_types),
                        "visual_frame_count": len(matching_visual),
                        "segment_type": "audio_driven"
                    })
            
            logger.info(f"åŒ¹é…äº† {len(matched_segments)} ä¸ªæ—¶é—´æ®µ")
            return matched_segments
            
        except Exception as e:
            logger.error(f"æ—¶é—´æ®µåŒ¹é…å¤±è´¥: {e}")
            return []
    
    def _detect_sync_events(
        self, 
        visual_timeline: Dict[str, Any], 
        audio_timeline: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """æ£€æµ‹åŒæ­¥äº‹ä»¶"""
        try:
            sync_events = []
            
            # æ£€æµ‹åœºæ™¯å˜åŒ–ä¸è¯­éŸ³åœé¡¿çš„åŒæ­¥
            scene_changes = visual_timeline.get("scene_changes", [])
            audio_events = audio_timeline.get("audio_events", [])
            
            # ä¸ºæ¯ä¸ªåœºæ™¯å˜åŒ–æŸ¥æ‰¾é™„è¿‘çš„éŸ³é¢‘äº‹ä»¶
            for scene_change in scene_changes:
                scene_time = scene_change.get("timestamp", 0)
                
                # åœ¨åœºæ™¯å˜åŒ–å‰å2ç§’å†…æŸ¥æ‰¾éŸ³é¢‘äº‹ä»¶
                nearby_audio = [
                    event for event in audio_events
                    if abs(event.get("timestamp", 0) - scene_time) <= 2.0
                ]
                
                if nearby_audio:
                    sync_events.append({
                        "timestamp": scene_time,
                        "sync_type": "scene_audio_sync",
                        "visual_event": scene_change,
                        "audio_events": nearby_audio,
                        "sync_confidence": self._calculate_sync_confidence(scene_change, nearby_audio)
                    })
            
            # æ£€æµ‹æƒ…æ„Ÿå˜åŒ–ä¸è§†è§‰å˜åŒ–çš„åŒæ­¥
            emotion_changes = audio_timeline.get("emotion_changes", [])
            visual_events = visual_timeline.get("visual_events", [])
            
            for emotion_change in emotion_changes:
                emotion_time = emotion_change.get("timestamp", 0)
                
                # æŸ¥æ‰¾é™„è¿‘çš„è§†è§‰äº‹ä»¶
                nearby_visual = [
                    event for event in visual_events
                    if abs(event.get("timestamp", 0) - emotion_time) <= 1.5
                ]
                
                if nearby_visual:
                    sync_events.append({
                        "timestamp": emotion_time,
                        "sync_type": "emotion_visual_sync",
                        "audio_event": emotion_change,
                        "visual_events": nearby_visual,
                        "sync_confidence": self._calculate_sync_confidence(emotion_change, nearby_visual)
                    })
            
            logger.info(f"æ£€æµ‹åˆ° {len(sync_events)} ä¸ªåŒæ­¥äº‹ä»¶")
            return sync_events
            
        except Exception as e:
            logger.error(f"åŒæ­¥äº‹ä»¶æ£€æµ‹å¤±è´¥: {e}")
            return []
    
    def _calculate_sync_confidence(self, primary_event: Dict, secondary_events: List[Dict]) -> float:
        """è®¡ç®—åŒæ­¥ç½®ä¿¡åº¦"""
        try:
            if not secondary_events:
                return 0.0
            
            # åŸºäºæ—¶é—´è·ç¦»è®¡ç®—ç½®ä¿¡åº¦
            primary_time = primary_event.get("timestamp", 0)
            min_time_diff = min([
                abs(event.get("timestamp", 0) - primary_time) 
                for event in secondary_events
            ])
            
            # æ—¶é—´å·®è¶Šå°ï¼Œç½®ä¿¡åº¦è¶Šé«˜
            if min_time_diff <= 0.5:
                confidence = 0.9
            elif min_time_diff <= 1.0:
                confidence = 0.7
            elif min_time_diff <= 2.0:
                confidence = 0.5
            else:
                confidence = 0.3
            
            return confidence
            
        except Exception as e:
            logger.error(f"åŒæ­¥ç½®ä¿¡åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _calculate_alignment_quality(
        self, 
        temporal_segments: List[Dict], 
        sync_events: List[Dict]
    ) -> Dict[str, Any]:
        """è®¡ç®—å¯¹é½è´¨é‡"""
        try:
            if not temporal_segments:
                return {"overall_quality": 0.0, "coverage": 0.0, "sync_ratio": 0.0}
            
            # è®¡ç®—è¦†ç›–ç‡
            segments_with_both = sum(1 for s in temporal_segments if s.get("visual_frame_count", 0) > 0)
            coverage = segments_with_both / len(temporal_segments)
            
            # è®¡ç®—åŒæ­¥æ¯”ä¾‹
            sync_ratio = len(sync_events) / len(temporal_segments) if temporal_segments else 0
            
            # è®¡ç®—å¹³å‡åŒæ­¥ç½®ä¿¡åº¦
            avg_sync_confidence = np.mean([
                event.get("sync_confidence", 0) for event in sync_events
            ]) if sync_events else 0
            
            # ç»¼åˆè´¨é‡è¯„åˆ†
            overall_quality = (coverage * 0.4 + sync_ratio * 0.3 + avg_sync_confidence * 0.3)
            
            return {
                "overall_quality": round(overall_quality, 3),
                "coverage": round(coverage, 3),
                "sync_ratio": round(sync_ratio, 3),
                "avg_sync_confidence": round(avg_sync_confidence, 3),
                "quality_level": "high" if overall_quality > 0.7 else "medium" if overall_quality > 0.4 else "low"
            }
            
        except Exception as e:
            logger.error(f"å¯¹é½è´¨é‡è®¡ç®—å¤±è´¥: {e}")
            return {"overall_quality": 0.0, "coverage": 0.0, "sync_ratio": 0.0}
    
    async def _correlate_semantics(
        self, 
        visual_results: Dict[str, Any], 
        audio_results: Dict[str, Any], 
        timeline_alignment: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        è·¨æ¨¡æ€è¯­ä¹‰å…³è”åˆ†æ
        """
        try:
            logger.info("æ‰§è¡Œè·¨æ¨¡æ€è¯­ä¹‰å…³è”...")
            
            # 1. ä¸»é¢˜ä¸€è‡´æ€§åˆ†æ
            theme_correlation = await self._analyze_theme_correlation(visual_results, audio_results)
            
            # 2. æƒ…æ„Ÿä¸€è‡´æ€§åˆ†æ
            emotion_correlation = await self._analyze_emotion_correlation(visual_results, audio_results)
            
            # 3. å†…å®¹äº’è¡¥æ€§åˆ†æ
            content_complementarity = await self._analyze_content_complementarity(visual_results, audio_results)
            
            # 4. è¯­ä¹‰å†²çªæ£€æµ‹
            semantic_conflicts = await self._detect_semantic_conflicts(visual_results, audio_results)
            
            # 5. åŸºäºæ—¶é—´è½´çš„è¯­ä¹‰å…³è”
            temporal_semantic_links = self._find_temporal_semantic_links(
                timeline_alignment.get("temporal_segments", [])
            )
            
            correlation_result = {
                "theme_correlation": theme_correlation,
                "emotion_correlation": emotion_correlation,
                "content_complementarity": content_complementarity,
                "semantic_conflicts": semantic_conflicts,
                "temporal_semantic_links": temporal_semantic_links,
                "overall_semantic_coherence": self._calculate_semantic_coherence(
                    theme_correlation, emotion_correlation, content_complementarity, semantic_conflicts
                )
            }
            
            logger.info("è·¨æ¨¡æ€è¯­ä¹‰å…³è”åˆ†æå®Œæˆ")
            return correlation_result
            
        except Exception as e:
            logger.error(f"è·¨æ¨¡æ€è¯­ä¹‰å…³è”å¤±è´¥: {e}")
            return {
                "error": str(e),
                "theme_correlation": {},
                "emotion_correlation": {},
                "content_complementarity": {},
                "semantic_conflicts": [],
                "temporal_semantic_links": []
            }
    
    async def _analyze_theme_correlation(
        self, 
        visual_results: Dict[str, Any], 
        audio_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """åˆ†æä¸»é¢˜å…³è”æ€§"""
        try:
            # æå–è§†è§‰ä¸»é¢˜
            visual_analysis = visual_results.get("visual_analysis", {})
            visual_themes = visual_analysis.get("visual_themes", [])
            
            # æå–éŸ³é¢‘ä¸»é¢˜
            semantic_analysis = audio_results.get("semantic_analysis", {})
            audio_topics = semantic_analysis.get("topic_analysis", {}).get("main_topics", [])
            
            # ä½¿ç”¨LLMåˆ†æä¸»é¢˜å…³è”æ€§
            correlation_prompt = f"""
            è¯·åˆ†æä»¥ä¸‹è§†è§‰ä¸»é¢˜å’ŒéŸ³é¢‘ä¸»é¢˜çš„å…³è”æ€§ï¼š

            è§†è§‰ä¸»é¢˜: {visual_themes}
            éŸ³é¢‘ä¸»é¢˜: {audio_topics}

            è¯·è¿”å›JSONæ ¼å¼çš„åˆ†æç»“æœï¼š
            {{
                "correlation_score": 0.85,
                "correlation_type": "highly_related/related/weakly_related/unrelated",
                "common_themes": ["å…±åŒä¸»é¢˜1", "å…±åŒä¸»é¢˜2"],
                "complementary_themes": ["äº’è¡¥ä¸»é¢˜1", "äº’è¡¥ä¸»é¢˜2"],
                "conflicting_themes": ["å†²çªä¸»é¢˜1"],
                "analysis_summary": "ä¸»é¢˜å…³è”æ€§åˆ†ææ€»ç»“"
            }}
            """
            
            # è®¾ç½®çŸ­è¶…æ—¶æ—¶é—´ï¼Œé¿å…å¡æ­»
            response = await self.llm_service.generate_response(correlation_prompt, timeout=15)
            
            try:
                theme_correlation = json.loads(response)
            except json.JSONDecodeError:
                theme_correlation = {
                    "correlation_score": 0.5,
                    "correlation_type": "unknown",
                    "common_themes": [],
                    "complementary_themes": [],
                    "conflicting_themes": [],
                    "analysis_summary": "ä¸»é¢˜å…³è”åˆ†æå¤±è´¥"
                }
            
            return theme_correlation
            
        except Exception as e:
            logger.error(f"ä¸»é¢˜å…³è”åˆ†æå¤±è´¥: {e}")
            return {
                "correlation_score": 0.0,
                "correlation_type": "error",
                "common_themes": [],
                "complementary_themes": [],
                "conflicting_themes": [],
                "analysis_summary": f"åˆ†æå¤±è´¥: {e}"
            }
    
    async def _analyze_emotion_correlation(
        self, 
        visual_results: Dict[str, Any], 
        audio_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """åˆ†ææƒ…æ„Ÿå…³è”æ€§"""
        try:
            # æå–è§†è§‰æƒ…æ„Ÿï¼ˆå¦‚æœæœ‰çš„è¯ï¼Œä»åœºæ™¯åˆ†æä¸­æ¨æ–­ï¼‰
            visual_analysis = visual_results.get("visual_analysis", {})
            scene_analysis = visual_results.get("scene_detection", {})
            
            # æå–éŸ³é¢‘æƒ…æ„Ÿ
            semantic_analysis = audio_results.get("semantic_analysis", {})
            emotion_analysis = semantic_analysis.get("emotion_analysis", {})
            overall_emotion = emotion_analysis.get("overall_emotion", {})
            
            # ä½¿ç”¨LLMåˆ†ææƒ…æ„Ÿä¸€è‡´æ€§
            emotion_prompt = f"""
            è¯·åˆ†æè§†è§‰åœºæ™¯å’ŒéŸ³é¢‘æƒ…æ„Ÿçš„ä¸€è‡´æ€§ï¼š

            è§†è§‰åœºæ™¯ä¿¡æ¯: {scene_analysis}
            éŸ³é¢‘æƒ…æ„Ÿåˆ†æ: {emotion_analysis}

            è¯·è¿”å›JSONæ ¼å¼çš„åˆ†æç»“æœï¼š
            {{
                "emotion_consistency": 0.8,
                "consistency_type": "highly_consistent/consistent/inconsistent/conflicting",
                "visual_emotion_inference": "ä»è§†è§‰æ¨æ–­çš„æƒ…æ„Ÿ",
                "audio_emotion": "éŸ³é¢‘æ£€æµ‹çš„æƒ…æ„Ÿ",
                "emotion_alignment": "æƒ…æ„Ÿå¯¹é½åˆ†æ",
                "emotional_journey": "æƒ…æ„Ÿå˜åŒ–è½¨è¿¹æè¿°"
            }}
            """
            
            response = await self.llm_service.generate_response(emotion_prompt, timeout=15)
            
            try:
                emotion_correlation = json.loads(response)
            except json.JSONDecodeError:
                emotion_correlation = {
                    "emotion_consistency": 0.5,
                    "consistency_type": "unknown",
                    "visual_emotion_inference": "unknown",
                    "audio_emotion": overall_emotion.get("dominant_emotion", "unknown"),
                    "emotion_alignment": "åˆ†æå¤±è´¥",
                    "emotional_journey": "æ— æ³•åˆ†æ"
                }
            
            return emotion_correlation
            
        except Exception as e:
            logger.error(f"æƒ…æ„Ÿå…³è”åˆ†æå¤±è´¥: {e}")
            return {
                "emotion_consistency": 0.0,
                "consistency_type": "error",
                "visual_emotion_inference": "unknown",
                "audio_emotion": "unknown",
                "emotion_alignment": f"åˆ†æå¤±è´¥: {e}",
                "emotional_journey": "æ— æ³•åˆ†æ"
            }
    
    async def _analyze_content_complementarity(
        self, 
        visual_results: Dict[str, Any], 
        audio_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """åˆ†æå†…å®¹äº’è¡¥æ€§"""
        try:
            # æå–è§†è§‰å†…å®¹ç‰¹å¾
            visual_analysis = visual_results.get("visual_analysis", {})
            
            # æå–éŸ³é¢‘å†…å®¹
            enhanced_speech = audio_results.get("enhanced_speech", {})
            full_text = enhanced_speech.get("full_text", "")
            
            # ä½¿ç”¨LLMåˆ†æå†…å®¹äº’è¡¥æ€§
            complementarity_prompt = f"""
            è¯·åˆ†æè§†è§‰å†…å®¹å’ŒéŸ³é¢‘å†…å®¹çš„äº’è¡¥æ€§ï¼š

            è§†è§‰åˆ†æç»“æœ: {visual_analysis}
            éŸ³é¢‘æ–‡æœ¬å†…å®¹: {full_text[:500]}...

            è¯·è¿”å›JSONæ ¼å¼çš„åˆ†æç»“æœï¼š
            {{
                "complementarity_score": 0.75,
                "complementarity_type": "highly_complementary/complementary/redundant/conflicting",
                "visual_unique_info": ["è§†è§‰ç‹¬æœ‰ä¿¡æ¯1", "è§†è§‰ç‹¬æœ‰ä¿¡æ¯2"],
                "audio_unique_info": ["éŸ³é¢‘ç‹¬æœ‰ä¿¡æ¯1", "éŸ³é¢‘ç‹¬æœ‰ä¿¡æ¯2"],
                "overlapping_info": ["é‡å ä¿¡æ¯1", "é‡å ä¿¡æ¯2"],
                "information_gaps": ["ä¿¡æ¯ç¼ºå£1", "ä¿¡æ¯ç¼ºå£2"],
                "complementarity_analysis": "äº’è¡¥æ€§åˆ†ææ€»ç»“"
            }}
            """
            
            response = await self.llm_service.generate_response(complementarity_prompt, timeout=15)
            
            try:
                complementarity = json.loads(response)
            except json.JSONDecodeError:
                complementarity = {
                    "complementarity_score": 0.5,
                    "complementarity_type": "unknown",
                    "visual_unique_info": [],
                    "audio_unique_info": [],
                    "overlapping_info": [],
                    "information_gaps": [],
                    "complementarity_analysis": "äº’è¡¥æ€§åˆ†æå¤±è´¥"
                }
            
            return complementarity
            
        except Exception as e:
            logger.error(f"å†…å®¹äº’è¡¥æ€§åˆ†æå¤±è´¥: {e}")
            return {
                "complementarity_score": 0.0,
                "complementarity_type": "error",
                "visual_unique_info": [],
                "audio_unique_info": [],
                "overlapping_info": [],
                "information_gaps": [],
                "complementarity_analysis": f"åˆ†æå¤±è´¥: {e}"
            }
    
    async def _detect_semantic_conflicts(
        self, 
        visual_results: Dict[str, Any], 
        audio_results: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """æ£€æµ‹è¯­ä¹‰å†²çª"""
        try:
            conflicts = []
            
            # æå–å…³é”®ä¿¡æ¯è¿›è¡Œå†²çªæ£€æµ‹
            visual_analysis = visual_results.get("visual_analysis", {})
            audio_analysis = audio_results.get("semantic_analysis", {})
            
            # ä½¿ç”¨LLMæ£€æµ‹æ½œåœ¨å†²çª
            conflict_prompt = f"""
            è¯·æ£€æµ‹è§†è§‰å’ŒéŸ³é¢‘å†…å®¹ä¹‹é—´çš„è¯­ä¹‰å†²çªï¼š

            è§†è§‰åˆ†æ: {visual_analysis}
            éŸ³é¢‘åˆ†æ: {audio_analysis}

            è¯·è¿”å›JSONæ ¼å¼çš„å†²çªæ£€æµ‹ç»“æœï¼š
            {{
                "conflicts": [
                    {{
                        "conflict_type": "content_mismatch/emotion_conflict/theme_inconsistency",
                        "severity": "high/medium/low",
                        "visual_aspect": "å†²çªçš„è§†è§‰æ–¹é¢",
                        "audio_aspect": "å†²çªçš„éŸ³é¢‘æ–¹é¢",
                        "description": "å†²çªæè¿°",
                        "confidence": 0.8
                    }}
                ],
                "overall_consistency": 0.7,
                "conflict_summary": "å†²çªæ£€æµ‹æ€»ç»“"
            }}
            """
            
            response = await self.llm_service.generate_response(conflict_prompt)
            
            try:
                conflict_result = json.loads(response)
                conflicts = conflict_result.get("conflicts", [])
            except json.JSONDecodeError:
                conflicts = []
            
            logger.info(f"æ£€æµ‹åˆ° {len(conflicts)} ä¸ªè¯­ä¹‰å†²çª")
            return conflicts
            
        except Exception as e:
            logger.error(f"è¯­ä¹‰å†²çªæ£€æµ‹å¤±è´¥: {e}")
            return []
    
    def _find_temporal_semantic_links(self, temporal_segments: List[Dict]) -> List[Dict]:
        """æŸ¥æ‰¾æ—¶é—´ç›¸å…³çš„è¯­ä¹‰å…³è”"""
        try:
            semantic_links = []
            
            for segment in temporal_segments:
                if segment.get("visual_frame_count", 0) > 0 and segment.get("audio_content"):
                    # åˆ†æè¿™ä¸ªæ—¶é—´æ®µå†…çš„è¯­ä¹‰å…³è”
                    visual_themes = segment.get("visual_themes", [])
                    audio_content = segment.get("audio_content", "")
                    
                    # ç®€å•çš„å…³é”®è¯åŒ¹é…ï¼ˆå¯ä»¥ç”¨æ›´å¤æ‚çš„NLPæ–¹æ³•ï¼‰
                    semantic_overlap = self._calculate_semantic_overlap(visual_themes, audio_content)
                    
                    if semantic_overlap > 0.3:  # é˜ˆå€¼å¯è°ƒ
                        semantic_links.append({
                            "start_time": segment.get("start_time", 0),
                            "end_time": segment.get("end_time", 0),
                            "visual_themes": visual_themes,
                            "audio_content": audio_content[:100] + "..." if len(audio_content) > 100 else audio_content,
                            "semantic_overlap": semantic_overlap,
                            "link_type": "temporal_semantic_correlation"
                        })
            
            logger.info(f"å‘ç° {len(semantic_links)} ä¸ªæ—¶é—´è¯­ä¹‰å…³è”")
            return semantic_links
            
        except Exception as e:
            logger.error(f"æ—¶é—´è¯­ä¹‰å…³è”æŸ¥æ‰¾å¤±è´¥: {e}")
            return []
    
    def _calculate_semantic_overlap(self, visual_themes: List[str], audio_content: str) -> float:
        """è®¡ç®—è¯­ä¹‰é‡å åº¦ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        try:
            if not visual_themes or not audio_content:
                return 0.0
            
            # ç®€å•çš„å…³é”®è¯åŒ¹é…
            audio_words = set(audio_content.lower().split())
            theme_words = set([theme.lower() for theme in visual_themes])
            
            # è®¡ç®—äº¤é›†
            common_words = audio_words.intersection(theme_words)
            
            if not theme_words:
                return 0.0
            
            overlap = len(common_words) / len(theme_words)
            return min(overlap, 1.0)
            
        except Exception as e:
            logger.error(f"è¯­ä¹‰é‡å åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _calculate_semantic_coherence(
        self, 
        theme_correlation: Dict, 
        emotion_correlation: Dict, 
        content_complementarity: Dict, 
        semantic_conflicts: List
    ) -> Dict[str, Any]:
        """è®¡ç®—æ•´ä½“è¯­ä¹‰ä¸€è‡´æ€§"""
        try:
            # æå–å„é¡¹å¾—åˆ†
            theme_score = theme_correlation.get("correlation_score", 0.5)
            emotion_score = emotion_correlation.get("emotion_consistency", 0.5)
            complementarity_score = content_complementarity.get("complementarity_score", 0.5)
            
            # å†²çªæƒ©ç½š
            conflict_penalty = len(semantic_conflicts) * 0.1
            
            # ç»¼åˆå¾—åˆ†
            overall_coherence = (theme_score * 0.3 + emotion_score * 0.3 + complementarity_score * 0.4) - conflict_penalty
            overall_coherence = max(0.0, min(1.0, overall_coherence))  # é™åˆ¶åœ¨0-1ä¹‹é—´
            
            return {
                "overall_coherence": round(overall_coherence, 3),
                "theme_contribution": round(theme_score * 0.3, 3),
                "emotion_contribution": round(emotion_score * 0.3, 3),
                "complementarity_contribution": round(complementarity_score * 0.4, 3),
                "conflict_penalty": round(conflict_penalty, 3),
                "coherence_level": "high" if overall_coherence > 0.7 else "medium" if overall_coherence > 0.4 else "low",
                "analysis_quality": "excellent" if overall_coherence > 0.8 else "good" if overall_coherence > 0.6 else "fair" if overall_coherence > 0.4 else "poor"
            }
            
        except Exception as e:
            logger.error(f"è¯­ä¹‰ä¸€è‡´æ€§è®¡ç®—å¤±è´¥: {e}")
            return {
                "overall_coherence": 0.0,
                "coherence_level": "unknown",
                "analysis_quality": "error"
            }
    
    async def _analyze_story_structure(
        self, 
        visual_results: Dict[str, Any], 
        audio_results: Dict[str, Any], 
        semantic_correlation: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        åˆ†ææ•…äº‹ç»“æ„
        """
        try:
            logger.info("æ‰§è¡Œæ•…äº‹ç»“æ„åˆ†æ...")
            
            # 1. æå–æ•…äº‹å…ƒç´ 
            story_elements = self._extract_story_elements(visual_results, audio_results)
            
            # 2. è¯†åˆ«æ•…äº‹æ®µè½
            story_segments = await self._identify_story_segments(visual_results, audio_results, semantic_correlation)
            
            # 3. åˆ†ææ•…äº‹å¼§çº¿
            story_arc = await self._analyze_story_arc(story_segments)
            
            # 4. è¯†åˆ«å…³é”®è½¬æŠ˜ç‚¹
            turning_points = self._identify_turning_points(story_segments)
            
            # 5. ç”Ÿæˆæ•…äº‹æ‘˜è¦
            story_summary = await self._generate_story_summary(story_elements, story_segments, story_arc)
            
            story_analysis = {
                "story_elements": story_elements,
                "story_segments": story_segments,
                "story_arc": story_arc,
                "turning_points": turning_points,
                "story_summary": story_summary,
                "narrative_structure": self._analyze_narrative_structure(story_segments, turning_points)
            }
            
            logger.info("æ•…äº‹ç»“æ„åˆ†æå®Œæˆ")
            return story_analysis
            
        except Exception as e:
            logger.error(f"æ•…äº‹ç»“æ„åˆ†æå¤±è´¥: {e}")
            return {
                "error": str(e),
                "story_elements": {},
                "story_segments": [],
                "story_arc": {},
                "turning_points": [],
                "story_summary": ""
            }
    
    def _extract_story_elements(
        self, 
        visual_results: Dict[str, Any], 
        audio_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """æå–æ•…äº‹å…ƒç´ """
        try:
            # ä»è§†è§‰åˆ†ææå–
            visual_analysis = visual_results.get("visual_analysis", {})
            detected_objects = visual_analysis.get("detected_objects", [])
            visual_themes = visual_analysis.get("visual_themes", [])
            
            # ä»éŸ³é¢‘åˆ†ææå–
            semantic_analysis = audio_results.get("semantic_analysis", {})
            topic_analysis = semantic_analysis.get("topic_analysis", {})
            content_analysis = semantic_analysis.get("content_analysis", {})
            
            story_elements = {
                "characters": self._extract_characters(detected_objects, topic_analysis),
                "setting": self._extract_setting(visual_themes, content_analysis),
                "themes": self._extract_themes(visual_themes, topic_analysis),
                "mood": self._extract_mood(visual_analysis, semantic_analysis),
                "content_type": content_analysis.get("content_type", "unknown")
            }
            
            return story_elements
            
        except Exception as e:
            logger.error(f"æ•…äº‹å…ƒç´ æå–å¤±è´¥: {e}")
            return {"characters": [], "setting": [], "themes": [], "mood": "unknown", "content_type": "unknown"}
    
    def _extract_characters(self, detected_objects: List[str], topic_analysis: Dict) -> List[str]:
        """æå–è§’è‰²ä¿¡æ¯"""
        characters = []
        
        # ä»æ£€æµ‹åˆ°çš„å¯¹è±¡ä¸­æå–äººç‰©
        person_objects = [obj for obj in detected_objects if any(keyword in obj.lower() for keyword in ["person", "äºº", "man", "woman", "child"])]
        characters.extend(person_objects)
        
        # ä»è¯é¢˜åˆ†æä¸­æå–æåˆ°çš„äººç‰©
        keywords = topic_analysis.get("keywords", [])
        person_keywords = [kw for kw in keywords if any(indicator in kw.lower() for indicator in ["å…ˆç”Ÿ", "å¥³å£«", "è€å¸ˆ", "åŒå­¦", "æœ‹å‹"])]
        characters.extend(person_keywords)
        
        return list(set(characters))  # å»é‡
    
    def _extract_setting(self, visual_themes: List[str], content_analysis: Dict) -> List[str]:
        """æå–åœºæ™¯è®¾ç½®"""
        setting = []
        
        # ä»è§†è§‰ä¸»é¢˜ä¸­æå–åœºæ™¯
        scene_themes = [theme for theme in visual_themes if any(keyword in theme.lower() for keyword in ["indoor", "outdoor", "office", "home", "street", "å®¤å†…", "å®¤å¤–", "åŠå…¬å®¤", "å®¶", "è¡—é“"])]
        setting.extend(scene_themes)
        
        # ä»å†…å®¹åˆ†æä¸­æ¨æ–­è®¾ç½®
        estimated_audience = content_analysis.get("estimated_audience", "")
        if estimated_audience:
            setting.append(f"é¢å‘{estimated_audience}")
        
        return list(set(setting))
    
    def _extract_themes(self, visual_themes: List[str], topic_analysis: Dict) -> List[str]:
        """æå–ä¸»é¢˜"""
        themes = []
        themes.extend(visual_themes)
        themes.extend(topic_analysis.get("main_topics", []))
        themes.extend(topic_analysis.get("topic_categories", []))
        
        return list(set(themes))
    
    def _extract_mood(self, visual_analysis: Dict, semantic_analysis: Dict) -> str:
        """æå–æƒ…ç»ªæ°›å›´"""
        # ä»éŸ³é¢‘æƒ…æ„Ÿåˆ†æä¸­è·å–ä¸»å¯¼æƒ…æ„Ÿ
        emotion_analysis = semantic_analysis.get("emotion_analysis", {})
        overall_emotion = emotion_analysis.get("overall_emotion", {})
        dominant_emotion = overall_emotion.get("dominant_emotion", "neutral")
        
        return dominant_emotion
    
    async def _identify_story_segments(
        self, 
        visual_results: Dict[str, Any], 
        audio_results: Dict[str, Any], 
        semantic_correlation: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """è¯†åˆ«æ•…äº‹æ®µè½"""
        try:
            # åŸºäºæ—¶é—´è¯­ä¹‰å…³è”åˆ›å»ºæ•…äº‹æ®µè½
            temporal_links = semantic_correlation.get("temporal_semantic_links", [])
            
            if not temporal_links:
                return []
            
            story_segments = []
            for i, link in enumerate(temporal_links):
                segment = {
                    "segment_id": i,
                    "start_time": link.get("start_time", 0),
                    "end_time": link.get("end_time", 0),
                    "duration": link.get("end_time", 0) - link.get("start_time", 0),
                    "visual_content": link.get("visual_themes", []),
                    "audio_content": link.get("audio_content", ""),
                    "semantic_overlap": link.get("semantic_overlap", 0),
                    "segment_type": "narrative_unit"
                }
                
                # ä½¿ç”¨LLMåˆ†æè¿™ä¸ªæ®µè½çš„å™äº‹åŠŸèƒ½
                segment_analysis = await self._analyze_segment_narrative_function(segment)
                segment.update(segment_analysis)
                
                story_segments.append(segment)
            
            return story_segments
            
        except Exception as e:
            logger.error(f"æ•…äº‹æ®µè½è¯†åˆ«å¤±è´¥: {e}")
            return []
    
    async def _analyze_segment_narrative_function(self, segment: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææ®µè½çš„å™äº‹åŠŸèƒ½"""
        try:
            analysis_prompt = f"""
            è¯·åˆ†æä»¥ä¸‹è§†é¢‘æ®µè½çš„å™äº‹åŠŸèƒ½ï¼š

            æ—¶é—´: {segment.get('start_time', 0):.1f}s - {segment.get('end_time', 0):.1f}s
            è§†è§‰å†…å®¹: {segment.get('visual_content', [])}
            éŸ³é¢‘å†…å®¹: {segment.get('audio_content', '')}

            è¯·è¿”å›JSONæ ¼å¼çš„åˆ†æç»“æœï¼š
            {{
                "narrative_function": "introduction/development/climax/resolution/transition",
                "content_summary": "æ®µè½å†…å®¹æ‘˜è¦",
                "emotional_tone": "æƒ…æ„ŸåŸºè°ƒ",
                "importance_level": "high/medium/low",
                "narrative_purpose": "å™äº‹ç›®çš„æè¿°"
            }}
            """
            
            response = await self.llm_service.generate_response(analysis_prompt)
            
            try:
                return json.loads(response)
            except json.JSONDecodeError:
                return {
                    "narrative_function": "unknown",
                    "content_summary": "åˆ†æå¤±è´¥",
                    "emotional_tone": "neutral",
                    "importance_level": "medium",
                    "narrative_purpose": "æ— æ³•ç¡®å®š"
                }
                
        except Exception as e:
            logger.error(f"æ®µè½å™äº‹åŠŸèƒ½åˆ†æå¤±è´¥: {e}")
            return {
                "narrative_function": "unknown",
                "content_summary": f"åˆ†æå¤±è´¥: {e}",
                "emotional_tone": "neutral",
                "importance_level": "low",
                "narrative_purpose": "åˆ†æå¤±è´¥"
            }
    
    async def _analyze_story_arc(self, story_segments: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†ææ•…äº‹å¼§çº¿"""
        try:
            if not story_segments:
                return {"arc_type": "unknown", "structure": [], "development": "æ— æ³•åˆ†æ"}
            
            # ä½¿ç”¨LLMåˆ†ææ•´ä½“æ•…äº‹å¼§çº¿
            arc_prompt = f"""
            è¯·åˆ†æä»¥ä¸‹æ•…äº‹æ®µè½çš„æ•´ä½“å¼§çº¿ç»“æ„ï¼š

            æ•…äº‹æ®µè½:
            {json.dumps([{
                'time': f"{s.get('start_time', 0):.1f}s-{s.get('end_time', 0):.1f}s",
                'function': s.get('narrative_function', 'unknown'),
                'summary': s.get('content_summary', ''),
                'tone': s.get('emotional_tone', 'neutral')
            } for s in story_segments], ensure_ascii=False, indent=2)}

            è¯·è¿”å›JSONæ ¼å¼çš„æ•…äº‹å¼§çº¿åˆ†æï¼š
            {{
                "arc_type": "linear/circular/episodic/experimental",
                "structure": ["introduction", "rising_action", "climax", "falling_action", "resolution"],
                "development": "æ•…äº‹å‘å±•æè¿°",
                "pacing": "fast/medium/slow",
                "narrative_coherence": 0.8,
                "story_completeness": "complete/incomplete/fragmented"
            }}
            """
            
            response = await self.llm_service.generate_response(arc_prompt)
            
            try:
                return json.loads(response)
            except json.JSONDecodeError:
                return {
                    "arc_type": "unknown",
                    "structure": [],
                    "development": "åˆ†æå¤±è´¥",
                    "pacing": "unknown",
                    "narrative_coherence": 0.5,
                    "story_completeness": "unknown"
                }
                
        except Exception as e:
            logger.error(f"æ•…äº‹å¼§çº¿åˆ†æå¤±è´¥: {e}")
            return {
                "arc_type": "error",
                "structure": [],
                "development": f"åˆ†æå¤±è´¥: {e}",
                "pacing": "unknown",
                "narrative_coherence": 0.0,
                "story_completeness": "error"
            }
    
    def _identify_turning_points(self, story_segments: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """è¯†åˆ«å…³é”®è½¬æŠ˜ç‚¹"""
        try:
            turning_points = []
            
            for i, segment in enumerate(story_segments):
                # æ£€æŸ¥æ˜¯å¦æ˜¯é‡è¦çš„å™äº‹èŠ‚ç‚¹
                narrative_function = segment.get("narrative_function", "unknown")
                importance_level = segment.get("importance_level", "medium")
                
                if narrative_function in ["climax", "transition"] or importance_level == "high":
                    turning_points.append({
                        "timestamp": segment.get("start_time", 0),
                        "segment_id": segment.get("segment_id", i),
                        "turning_point_type": narrative_function,
                        "description": segment.get("content_summary", ""),
                        "emotional_impact": segment.get("emotional_tone", "neutral"),
                        "importance": importance_level
                    })
            
            logger.info(f"è¯†åˆ«äº† {len(turning_points)} ä¸ªè½¬æŠ˜ç‚¹")
            return turning_points
            
        except Exception as e:
            logger.error(f"è½¬æŠ˜ç‚¹è¯†åˆ«å¤±è´¥: {e}")
            return []
    
    async def _generate_story_summary(
        self, 
        story_elements: Dict[str, Any], 
        story_segments: List[Dict[str, Any]], 
        story_arc: Dict[str, Any]
    ) -> str:
        """ç”Ÿæˆæ•…äº‹æ‘˜è¦"""
        try:
            summary_prompt = f"""
            è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯ç”Ÿæˆè§†é¢‘çš„æ•…äº‹æ‘˜è¦ï¼š

            æ•…äº‹å…ƒç´ : {story_elements}
            æ•…äº‹å¼§çº¿: {story_arc}
            æ®µè½æ•°é‡: {len(story_segments)}

            è¯·ç”Ÿæˆä¸€ä¸ªç®€æ´ä½†å…¨é¢çš„æ•…äº‹æ‘˜è¦ï¼ˆ200å­—ä»¥å†…ï¼‰ï¼š
            """
            
            summary = await self.llm_service.generate_response(summary_prompt)
            return summary.strip()
            
        except Exception as e:
            logger.error(f"æ•…äº‹æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
            return f"æ•…äº‹æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}"
    
    def _analyze_narrative_structure(
        self, 
        story_segments: List[Dict[str, Any]], 
        turning_points: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """åˆ†æå™äº‹ç»“æ„"""
        try:
            if not story_segments:
                return {"structure_type": "unknown", "complexity": "low", "organization": "unclear"}
            
            # åˆ†æç»“æ„å¤æ‚åº¦
            narrative_functions = [s.get("narrative_function", "unknown") for s in story_segments]
            unique_functions = set(narrative_functions)
            
            complexity = "high" if len(unique_functions) > 3 else "medium" if len(unique_functions) > 1 else "low"
            
            # åˆ†æç»„ç»‡æ–¹å¼
            if "introduction" in narrative_functions and "resolution" in narrative_functions:
                organization = "well_structured"
            elif len(turning_points) > 0:
                organization = "structured"
            else:
                organization = "loose"
            
            return {
                "structure_type": "narrative" if len(unique_functions) > 2 else "simple",
                "complexity": complexity,
                "organization": organization,
                "segment_count": len(story_segments),
                "turning_point_count": len(turning_points),
                "narrative_functions": list(unique_functions)
            }
            
        except Exception as e:
            logger.error(f"å™äº‹ç»“æ„åˆ†æå¤±è´¥: {e}")
            return {"structure_type": "error", "complexity": "unknown", "organization": "error"}
    
    async def _track_emotion_changes(
        self, 
        visual_results: Dict[str, Any], 
        audio_results: Dict[str, Any], 
        timeline_alignment: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        è¿½è¸ªæƒ…æ„Ÿå˜åŒ–
        """
        try:
            logger.info("æ‰§è¡Œæƒ…æ„Ÿå˜åŒ–è¿½è¸ª...")
            
            # 1. æå–æƒ…æ„Ÿæ—¶é—´çº¿
            emotion_timeline = self._extract_emotion_timeline(audio_results, timeline_alignment)
            
            # 2. æ£€æµ‹æƒ…æ„Ÿè½¬æŠ˜ç‚¹
            emotion_turning_points = self._detect_emotion_turning_points(emotion_timeline)
            
            # 3. åˆ†ææƒ…æ„Ÿå¼ºåº¦å˜åŒ–
            emotion_intensity = self._analyze_emotion_intensity(emotion_timeline)
            
            # 4. æƒ…æ„Ÿå¼§çº¿åˆ†æ
            emotion_arc = await self._analyze_emotion_arc(emotion_timeline, emotion_turning_points)
            
            # 5. æƒ…æ„Ÿä¸€è‡´æ€§æ£€æŸ¥
            emotion_consistency = self._check_emotion_consistency(visual_results, audio_results, emotion_timeline)
            
            emotion_tracking = {
                "emotion_timeline": emotion_timeline,
                "emotion_turning_points": emotion_turning_points,
                "emotion_intensity": emotion_intensity,
                "emotion_arc": emotion_arc,
                "emotion_consistency": emotion_consistency,
                "emotional_journey_summary": await self._summarize_emotional_journey(emotion_timeline, emotion_turning_points)
            }
            
            logger.info("æƒ…æ„Ÿå˜åŒ–è¿½è¸ªå®Œæˆ")
            return emotion_tracking
            
        except Exception as e:
            logger.error(f"æƒ…æ„Ÿå˜åŒ–è¿½è¸ªå¤±è´¥: {e}")
            return {
                "error": str(e),
                "emotion_timeline": [],
                "emotion_turning_points": [],
                "emotion_intensity": {},
                "emotion_arc": {},
                "emotion_consistency": {}
            }
    
    def _extract_emotion_timeline(
        self, 
        audio_results: Dict[str, Any], 
        timeline_alignment: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """æå–æƒ…æ„Ÿæ—¶é—´çº¿"""
        try:
            emotion_timeline = []
            
            # ä»éŸ³é¢‘åˆ†æä¸­æå–æƒ…æ„Ÿä¿¡æ¯
            semantic_analysis = audio_results.get("semantic_analysis", {})
            emotion_analysis = semantic_analysis.get("emotion_analysis", {})
            segment_emotions = emotion_analysis.get("segment_emotions", [])
            
            # è½¬æ¢ä¸ºæ—¶é—´çº¿æ ¼å¼
            for emotion_segment in segment_emotions:
                emotion_timeline.append({
                    "timestamp": emotion_segment.get("start_time", 0),
                    "end_time": emotion_segment.get("end_time", 0),
                    "emotion": emotion_segment.get("emotion", "neutral"),
                    "confidence": emotion_segment.get("confidence", 0.0),
                    "segment_id": emotion_segment.get("segment_id", 0),
                    "source": "audio_analysis"
                })
            
            # æŒ‰æ—¶é—´æ’åº
            emotion_timeline.sort(key=lambda x: x["timestamp"])
            
            logger.info(f"æå–äº† {len(emotion_timeline)} ä¸ªæƒ…æ„Ÿæ—¶é—´ç‚¹")
            return emotion_timeline
            
        except Exception as e:
            logger.error(f"æƒ…æ„Ÿæ—¶é—´çº¿æå–å¤±è´¥: {e}")
            return []
    
    def _detect_emotion_turning_points(self, emotion_timeline: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ£€æµ‹æƒ…æ„Ÿè½¬æŠ˜ç‚¹"""
        try:
            turning_points = []
            
            for i in range(1, len(emotion_timeline)):
                prev_emotion = emotion_timeline[i-1].get("emotion", "neutral")
                curr_emotion = emotion_timeline[i].get("emotion", "neutral")
                
                # æ£€æµ‹æƒ…æ„Ÿå˜åŒ–
                if prev_emotion != curr_emotion:
                    # è®¡ç®—æƒ…æ„Ÿå˜åŒ–å¼ºåº¦
                    intensity = self._calculate_emotion_change_intensity(prev_emotion, curr_emotion)
                    
                    turning_points.append({
                        "timestamp": emotion_timeline[i].get("timestamp", 0),
                        "from_emotion": prev_emotion,
                        "to_emotion": curr_emotion,
                        "change_intensity": intensity,
                        "change_type": self._classify_emotion_change(prev_emotion, curr_emotion),
                        "confidence": min(
                            emotion_timeline[i-1].get("confidence", 0),
                            emotion_timeline[i].get("confidence", 0)
                        )
                    })
            
            logger.info(f"æ£€æµ‹åˆ° {len(turning_points)} ä¸ªæƒ…æ„Ÿè½¬æŠ˜ç‚¹")
            return turning_points
            
        except Exception as e:
            logger.error(f"æƒ…æ„Ÿè½¬æŠ˜ç‚¹æ£€æµ‹å¤±è´¥: {e}")
            return []
    
    def _calculate_emotion_change_intensity(self, from_emotion: str, to_emotion: str) -> float:
        """è®¡ç®—æƒ…æ„Ÿå˜åŒ–å¼ºåº¦"""
        # ç®€åŒ–çš„æƒ…æ„Ÿå¼ºåº¦æ˜ å°„
        emotion_intensity_map = {
            "positive": 1.0,
            "excited": 1.2,
            "happy": 0.8,
            "neutral": 0.0,
            "negative": -1.0,
            "sad": -0.8,
            "angry": -1.2,
            "surprised": 0.5
        }
        
        from_intensity = emotion_intensity_map.get(from_emotion, 0.0)
        to_intensity = emotion_intensity_map.get(to_emotion, 0.0)
        
        return abs(to_intensity - from_intensity)
    
    def _classify_emotion_change(self, from_emotion: str, to_emotion: str) -> str:
        """åˆ†ç±»æƒ…æ„Ÿå˜åŒ–ç±»å‹"""
        positive_emotions = {"positive", "excited", "happy"}
        negative_emotions = {"negative", "sad", "angry"}
        neutral_emotions = {"neutral", "surprised"}
        
        if from_emotion in positive_emotions and to_emotion in negative_emotions:
            return "positive_to_negative"
        elif from_emotion in negative_emotions and to_emotion in positive_emotions:
            return "negative_to_positive"
        elif from_emotion in neutral_emotions:
            return "neutral_to_emotional"
        elif to_emotion in neutral_emotions:
            return "emotional_to_neutral"
        else:
            return "emotional_shift"
    
    def _analyze_emotion_intensity(self, emotion_timeline: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†ææƒ…æ„Ÿå¼ºåº¦"""
        try:
            if not emotion_timeline:
                return {"average_intensity": 0.0, "peak_intensity": 0.0, "intensity_variance": 0.0}
            
            confidences = [point.get("confidence", 0.0) for point in emotion_timeline]
            
            return {
                "average_intensity": np.mean(confidences),
                "peak_intensity": np.max(confidences),
                "min_intensity": np.min(confidences),
                "intensity_variance": np.var(confidences),
                "intensity_stability": 1.0 - np.std(confidences) if confidences else 0.0
            }
            
        except Exception as e:
            logger.error(f"æƒ…æ„Ÿå¼ºåº¦åˆ†æå¤±è´¥: {e}")
            return {"average_intensity": 0.0, "peak_intensity": 0.0, "intensity_variance": 0.0}
    
    async def _analyze_emotion_arc(
        self, 
        emotion_timeline: List[Dict[str, Any]], 
        emotion_turning_points: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """åˆ†ææƒ…æ„Ÿå¼§çº¿"""
        try:
            if not emotion_timeline:
                return {"arc_type": "unknown", "emotional_journey": "æ— æƒ…æ„Ÿæ•°æ®"}
            
            # ä½¿ç”¨LLMåˆ†ææƒ…æ„Ÿå¼§çº¿
            arc_prompt = f"""
            è¯·åˆ†æä»¥ä¸‹æƒ…æ„Ÿæ—¶é—´çº¿çš„å¼§çº¿ç‰¹å¾ï¼š

            æƒ…æ„Ÿæ—¶é—´çº¿: {emotion_timeline[:10]}  # é™åˆ¶é•¿åº¦
            æƒ…æ„Ÿè½¬æŠ˜ç‚¹: {emotion_turning_points}

            è¯·è¿”å›JSONæ ¼å¼çš„åˆ†æç»“æœï¼š
            {{
                "arc_type": "rising/falling/stable/fluctuating/u_shaped/inverted_u",
                "emotional_journey": "æƒ…æ„Ÿæ—…ç¨‹æè¿°",
                "dominant_emotion_phase": "ä¸»å¯¼æƒ…æ„Ÿé˜¶æ®µ",
                "emotional_complexity": "high/medium/low",
                "emotional_resolution": "resolved/unresolved/ambiguous"
            }}
            """
            
            response = await self.llm_service.generate_response(arc_prompt)
            
            try:
                return json.loads(response)
            except json.JSONDecodeError:
                return {
                    "arc_type": "unknown",
                    "emotional_journey": "åˆ†æå¤±è´¥",
                    "dominant_emotion_phase": "unknown",
                    "emotional_complexity": "medium",
                    "emotional_resolution": "unknown"
                }
                
        except Exception as e:
            logger.error(f"æƒ…æ„Ÿå¼§çº¿åˆ†æå¤±è´¥: {e}")
            return {
                "arc_type": "error",
                "emotional_journey": f"åˆ†æå¤±è´¥: {e}",
                "dominant_emotion_phase": "error",
                "emotional_complexity": "unknown",
                "emotional_resolution": "error"
            }
    
    def _check_emotion_consistency(
        self, 
        visual_results: Dict[str, Any], 
        audio_results: Dict[str, Any], 
        emotion_timeline: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """æ£€æŸ¥æƒ…æ„Ÿä¸€è‡´æ€§"""
        try:
            # æå–æ•´ä½“æƒ…æ„Ÿå€¾å‘
            semantic_analysis = audio_results.get("semantic_analysis", {})
            overall_emotion = semantic_analysis.get("emotion_analysis", {}).get("overall_emotion", {})
            dominant_emotion = overall_emotion.get("dominant_emotion", "neutral")
            
            # è®¡ç®—æ—¶é—´çº¿ä¸­å„æƒ…æ„Ÿçš„åˆ†å¸ƒ
            emotions = [point.get("emotion", "neutral") for point in emotion_timeline]
            emotion_counts = Counter(emotions)
            
            # æ£€æŸ¥ä¸€è‡´æ€§
            timeline_dominant = emotion_counts.most_common(1)[0][0] if emotion_counts else "neutral"
            consistency_score = 1.0 if timeline_dominant == dominant_emotion else 0.5
            
            return {
                "overall_emotion": dominant_emotion,
                "timeline_dominant_emotion": timeline_dominant,
                "consistency_score": consistency_score,
                "emotion_distribution": dict(emotion_counts),
                "consistency_level": "high" if consistency_score > 0.8 else "medium" if consistency_score > 0.5 else "low"
            }
            
        except Exception as e:
            logger.error(f"æƒ…æ„Ÿä¸€è‡´æ€§æ£€æŸ¥å¤±è´¥: {e}")
            return {
                "consistency_score": 0.0,
                "consistency_level": "unknown",
                "emotion_distribution": {}
            }
    
    async def _summarize_emotional_journey(
        self, 
        emotion_timeline: List[Dict[str, Any]], 
        emotion_turning_points: List[Dict[str, Any]]
    ) -> str:
        """æ€»ç»“æƒ…æ„Ÿæ—…ç¨‹"""
        try:
            if not emotion_timeline:
                return "æ— æƒ…æ„Ÿæ•°æ®å¯åˆ†æ"
            
            summary_prompt = f"""
            è¯·æ€»ç»“ä»¥ä¸‹æƒ…æ„Ÿæ—…ç¨‹ï¼š

            æƒ…æ„Ÿæ—¶é—´çº¿ç‚¹æ•°: {len(emotion_timeline)}
            ä¸»è¦æƒ…æ„Ÿè½¬æŠ˜: {len(emotion_turning_points)}
            æƒ…æ„Ÿå˜åŒ–: {[f"{tp.get('from_emotion', 'unknown')}â†’{tp.get('to_emotion', 'unknown')}" for tp in emotion_turning_points[:3]]}

            è¯·ç”¨50å­—ä»¥å†…æ€»ç»“è¿™ä¸ªæƒ…æ„Ÿæ—…ç¨‹ï¼š
            """
            
            summary = await self.llm_service.generate_response(summary_prompt)
            return summary.strip()
            
        except Exception as e:
            logger.error(f"æƒ…æ„Ÿæ—…ç¨‹æ€»ç»“å¤±è´¥: {e}")
            return f"æƒ…æ„Ÿæ—…ç¨‹æ€»ç»“å¤±è´¥: {e}"
    
    async def _generate_comprehensive_understanding(
        self, 
        visual_results: Dict[str, Any], 
        audio_results: Dict[str, Any], 
        semantic_correlation: Dict[str, Any], 
        story_analysis: Dict[str, Any], 
        emotion_tracking: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆç»¼åˆç†è§£æŠ¥å‘Š
        """
        try:
            logger.info("ç”Ÿæˆç»¼åˆç†è§£æŠ¥å‘Š...")
            
            # 1. è§†é¢‘æ•´ä½“ç†è§£
            overall_understanding = await self._generate_overall_understanding(
                visual_results, audio_results, semantic_correlation
            )
            
            # 2. å…³é”®æ´å¯Ÿæå–
            key_insights = await self._extract_key_insights(
                story_analysis, emotion_tracking, semantic_correlation
            )
            
            # 3. å†…å®¹ä»·å€¼è¯„ä¼°
            content_value = self._assess_content_value(
                visual_results, audio_results, story_analysis, emotion_tracking
            )
            
            # 4. å—ä¼—åˆ†æ
            audience_analysis = await self._analyze_target_audience(
                visual_results, audio_results, story_analysis
            )
            
            # 5. æ”¹è¿›å»ºè®®
            improvement_suggestions = await self._generate_improvement_suggestions(
                semantic_correlation, story_analysis, emotion_tracking
            )
            
            comprehensive_understanding = {
                "overall_understanding": overall_understanding,
                "key_insights": key_insights,
                "content_value": content_value,
                "audience_analysis": audience_analysis,
                "improvement_suggestions": improvement_suggestions,
                "analysis_confidence": self._calculate_analysis_confidence(
                    visual_results, audio_results, semantic_correlation
                )
            }
            
            logger.info("ç»¼åˆç†è§£æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
            return comprehensive_understanding
            
        except Exception as e:
            logger.error(f"ç»¼åˆç†è§£æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            return {
                "error": str(e),
                "overall_understanding": "",
                "key_insights": [],
                "content_value": {},
                "audience_analysis": {},
                "improvement_suggestions": []
            }
    
    async def _generate_overall_understanding(
        self, 
        visual_results: Dict[str, Any], 
        audio_results: Dict[str, Any], 
        semantic_correlation: Dict[str, Any]
    ) -> str:
        """ç”Ÿæˆæ•´ä½“ç†è§£"""
        try:
            understanding_prompt = f"""
            è¯·åŸºäºä»¥ä¸‹å¤šæ¨¡æ€åˆ†æç»“æœï¼Œç”Ÿæˆè§†é¢‘çš„æ•´ä½“ç†è§£æŠ¥å‘Šï¼š

            è§†è§‰åˆ†ææ‘˜è¦: {visual_results.get('visual_analysis', {}).get('analysis_summary', '')}
            éŸ³é¢‘åˆ†ææ‘˜è¦: {audio_results.get('semantic_analysis', {}).get('content_analysis', {})}
            è¯­ä¹‰ä¸€è‡´æ€§: {semantic_correlation.get('overall_semantic_coherence', {})}

            è¯·ç”Ÿæˆä¸€ä¸ª300å­—ä»¥å†…çš„ç»¼åˆç†è§£æŠ¥å‘Šï¼ŒåŒ…æ‹¬ï¼š
            1. è§†é¢‘çš„ä¸»è¦å†…å®¹å’Œç›®çš„
            2. è§†è§‰å’ŒéŸ³é¢‘çš„é…åˆæ•ˆæœ
            3. æ•´ä½“ä¼ è¾¾çš„ä¿¡æ¯å’Œä»·å€¼
            """
            
            understanding = await self.llm_service.generate_response(understanding_prompt)
            return understanding.strip()
            
        except Exception as e:
            logger.error(f"æ•´ä½“ç†è§£ç”Ÿæˆå¤±è´¥: {e}")
            return f"æ•´ä½“ç†è§£ç”Ÿæˆå¤±è´¥: {e}"
    
    async def _extract_key_insights(
        self, 
        story_analysis: Dict[str, Any], 
        emotion_tracking: Dict[str, Any], 
        semantic_correlation: Dict[str, Any]
    ) -> List[str]:
        """æå–å…³é”®æ´å¯Ÿ"""
        try:
            insights_prompt = f"""
            è¯·ä»ä»¥ä¸‹åˆ†æç»“æœä¸­æå–3-5ä¸ªå…³é”®æ´å¯Ÿï¼š

            æ•…äº‹åˆ†æ: {story_analysis.get('story_summary', '')}
            æƒ…æ„Ÿè¿½è¸ª: {emotion_tracking.get('emotional_journey_summary', '')}
            è¯­ä¹‰å…³è”: {semantic_correlation.get('overall_semantic_coherence', {})}

            è¯·è¿”å›JSONæ ¼å¼çš„æ´å¯Ÿåˆ—è¡¨ï¼š
            {{
                "insights": [
                    "æ´å¯Ÿ1ï¼šå…·ä½“çš„è§‚å¯Ÿå’Œå‘ç°",
                    "æ´å¯Ÿ2ï¼šé‡è¦çš„æ¨¡å¼æˆ–è¶‹åŠ¿",
                    "æ´å¯Ÿ3ï¼šæœ‰ä»·å€¼çš„ç»“è®º"
                ]
            }}
            """
            
            response = await self.llm_service.generate_response(insights_prompt)
            
            try:
                insights_data = json.loads(response)
                return insights_data.get("insights", [])
            except json.JSONDecodeError:
                return ["å…³é”®æ´å¯Ÿæå–å¤±è´¥"]
                
        except Exception as e:
            logger.error(f"å…³é”®æ´å¯Ÿæå–å¤±è´¥: {e}")
            return [f"å…³é”®æ´å¯Ÿæå–å¤±è´¥: {e}"]
    
    def _assess_content_value(
        self, 
        visual_results: Dict[str, Any], 
        audio_results: Dict[str, Any], 
        story_analysis: Dict[str, Any], 
        emotion_tracking: Dict[str, Any]
    ) -> Dict[str, Any]:
        """è¯„ä¼°å†…å®¹ä»·å€¼"""
        try:
            # åŸºäºå„é¡¹æŒ‡æ ‡è®¡ç®—å†…å®¹ä»·å€¼
            visual_quality = visual_results.get("visual_analysis", {}).get("analysis_quality", 0.5)
            audio_quality = audio_results.get("enhanced_speech", {}).get("confidence", 0.5)
            story_coherence = story_analysis.get("story_arc", {}).get("narrative_coherence", 0.5)
            emotion_stability = emotion_tracking.get("emotion_intensity", {}).get("intensity_stability", 0.5)
            
            # ç»¼åˆè¯„åˆ†
            overall_value = (visual_quality * 0.25 + audio_quality * 0.25 + 
                           story_coherence * 0.25 + emotion_stability * 0.25)
            
            return {
                "overall_value_score": round(overall_value, 3),
                "visual_quality_score": round(visual_quality, 3),
                "audio_quality_score": round(audio_quality, 3),
                "story_coherence_score": round(story_coherence, 3),
                "emotion_stability_score": round(emotion_stability, 3),
                "value_level": "high" if overall_value > 0.7 else "medium" if overall_value > 0.4 else "low",
                "content_strengths": self._identify_content_strengths(visual_quality, audio_quality, story_coherence, emotion_stability),
                "content_weaknesses": self._identify_content_weaknesses(visual_quality, audio_quality, story_coherence, emotion_stability)
            }
            
        except Exception as e:
            logger.error(f"å†…å®¹ä»·å€¼è¯„ä¼°å¤±è´¥: {e}")
            return {
                "overall_value_score": 0.0,
                "value_level": "unknown",
                "content_strengths": [],
                "content_weaknesses": []
            }
    
    def _identify_content_strengths(self, visual_q: float, audio_q: float, story_q: float, emotion_q: float) -> List[str]:
        """è¯†åˆ«å†…å®¹ä¼˜åŠ¿"""
        strengths = []
        
        if visual_q > 0.7:
            strengths.append("è§†è§‰å†…å®¹è´¨é‡é«˜")
        if audio_q > 0.7:
            strengths.append("éŸ³é¢‘è¯†åˆ«å‡†ç¡®åº¦é«˜")
        if story_q > 0.7:
            strengths.append("æ•…äº‹ç»“æ„æ¸…æ™°è¿è´¯")
        if emotion_q > 0.7:
            strengths.append("æƒ…æ„Ÿè¡¨è¾¾ç¨³å®šä¸€è‡´")
        
        return strengths
    
    def _identify_content_weaknesses(self, visual_q: float, audio_q: float, story_q: float, emotion_q: float) -> List[str]:
        """è¯†åˆ«å†…å®¹å¼±ç‚¹"""
        weaknesses = []
        
        if visual_q < 0.4:
            weaknesses.append("è§†è§‰å†…å®¹éœ€è¦æ”¹è¿›")
        if audio_q < 0.4:
            weaknesses.append("éŸ³é¢‘è´¨é‡æœ‰å¾…æå‡")
        if story_q < 0.4:
            weaknesses.append("æ•…äº‹ç»“æ„ä¸å¤Ÿæ¸…æ™°")
        if emotion_q < 0.4:
            weaknesses.append("æƒ…æ„Ÿè¡¨è¾¾ä¸å¤Ÿç¨³å®š")
        
        return weaknesses
    
    async def _analyze_target_audience(
        self, 
        visual_results: Dict[str, Any], 
        audio_results: Dict[str, Any], 
        story_analysis: Dict[str, Any]
    ) -> Dict[str, Any]:
        """åˆ†æç›®æ ‡å—ä¼—"""
        try:
            audience_prompt = f"""
            è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯åˆ†æè§†é¢‘çš„ç›®æ ‡å—ä¼—ï¼š

            å†…å®¹ç±»å‹: {audio_results.get('semantic_analysis', {}).get('content_analysis', {}).get('content_type', 'unknown')}
            æ•…äº‹å…ƒç´ : {story_analysis.get('story_elements', {})}
            è§†è§‰ä¸»é¢˜: {visual_results.get('visual_analysis', {}).get('visual_themes', [])}

            è¯·è¿”å›JSONæ ¼å¼çš„å—ä¼—åˆ†æï¼š
            {{
                "primary_audience": "ä¸»è¦ç›®æ ‡å—ä¼—",
                "secondary_audience": "æ¬¡è¦ç›®æ ‡å—ä¼—",
                "age_group": "å¹´é¾„ç¾¤ä½“",
                "interests": ["å…´è¶£1", "å…´è¶£2"],
                "viewing_context": "è§‚çœ‹åœºæ™¯",
                "audience_engagement_potential": "high/medium/low"
            }}
            """
            
            response = await self.llm_service.generate_response(audience_prompt)
            
            try:
                return json.loads(response)
            except json.JSONDecodeError:
                return {
                    "primary_audience": "æœªçŸ¥",
                    "secondary_audience": "æœªçŸ¥",
                    "age_group": "æœªçŸ¥",
                    "interests": [],
                    "viewing_context": "æœªçŸ¥",
                    "audience_engagement_potential": "medium"
                }
                
        except Exception as e:
            logger.error(f"ç›®æ ‡å—ä¼—åˆ†æå¤±è´¥: {e}")
            return {
                "primary_audience": f"åˆ†æå¤±è´¥: {e}",
                "audience_engagement_potential": "unknown"
            }
    
    async def _generate_improvement_suggestions(
        self, 
        semantic_correlation: Dict[str, Any], 
        story_analysis: Dict[str, Any], 
        emotion_tracking: Dict[str, Any]
    ) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        try:
            suggestions_prompt = f"""
            è¯·åŸºäºä»¥ä¸‹åˆ†æç»“æœæä¾›3-5ä¸ªå…·ä½“çš„æ”¹è¿›å»ºè®®ï¼š

            è¯­ä¹‰ä¸€è‡´æ€§: {semantic_correlation.get('overall_semantic_coherence', {})}
            æ•…äº‹ç»“æ„: {story_analysis.get('narrative_structure', {})}
            æƒ…æ„Ÿè¡¨è¾¾: {emotion_tracking.get('emotion_consistency', {})}

            è¯·è¿”å›JSONæ ¼å¼çš„å»ºè®®åˆ—è¡¨ï¼š
            {{
                "suggestions": [
                    "å»ºè®®1ï¼šå…·ä½“å¯è¡Œçš„æ”¹è¿›æ–¹æ¡ˆ",
                    "å»ºè®®2ï¼šé’ˆå¯¹æ€§çš„ä¼˜åŒ–å»ºè®®",
                    "å»ºè®®3ï¼šæå‡æ•ˆæœçš„æ–¹æ³•"
                ]
            }}
            """
            
            response = await self.llm_service.generate_response(suggestions_prompt)
            
            try:
                suggestions_data = json.loads(response)
                return suggestions_data.get("suggestions", [])
            except json.JSONDecodeError:
                return ["æ”¹è¿›å»ºè®®ç”Ÿæˆå¤±è´¥"]
                
        except Exception as e:
            logger.error(f"æ”¹è¿›å»ºè®®ç”Ÿæˆå¤±è´¥: {e}")
            return [f"æ”¹è¿›å»ºè®®ç”Ÿæˆå¤±è´¥: {e}"]
    
    def _calculate_analysis_confidence(
        self, 
        visual_results: Dict[str, Any], 
        audio_results: Dict[str, Any], 
        semantic_correlation: Dict[str, Any]
    ) -> Dict[str, Any]:
        """è®¡ç®—åˆ†æç½®ä¿¡åº¦"""
        try:
            # æå–å„é¡¹ç½®ä¿¡åº¦æŒ‡æ ‡
            visual_confidence = visual_results.get("visual_analysis", {}).get("analysis_confidence", 0.5)
            audio_confidence = audio_results.get("enhanced_speech", {}).get("confidence", 0.5)
            semantic_confidence = semantic_correlation.get("overall_semantic_coherence", {}).get("overall_coherence", 0.5)
            
            # è®¡ç®—ç»¼åˆç½®ä¿¡åº¦
            overall_confidence = (visual_confidence * 0.4 + audio_confidence * 0.3 + semantic_confidence * 0.3)
            
            return {
                "overall_confidence": round(overall_confidence, 3),
                "visual_confidence": round(visual_confidence, 3),
                "audio_confidence": round(audio_confidence, 3),
                "semantic_confidence": round(semantic_confidence, 3),
                "confidence_level": "high" if overall_confidence > 0.7 else "medium" if overall_confidence > 0.4 else "low",
                "reliability": "reliable" if overall_confidence > 0.6 else "moderate" if overall_confidence > 0.4 else "limited"
            }
            
        except Exception as e:
            logger.error(f"åˆ†æç½®ä¿¡åº¦è®¡ç®—å¤±è´¥: {e}")
            return {
                "overall_confidence": 0.0,
                "confidence_level": "unknown",
                "reliability": "error"
            }


# åˆ›å»ºå…¨å±€å®ä¾‹
video_multimodal_service = VideoMultimodalService() 