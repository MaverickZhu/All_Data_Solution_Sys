"""
è§†é¢‘éŸ³é¢‘åˆ†ææœåŠ¡
åŸºäºç°æœ‰çš„éŸ³é¢‘åˆ†æåŸºç¡€è®¾æ–½ï¼Œä¸ºè§†é¢‘åˆ†ææä¾›éŸ³é¢‘è¯­ä¹‰åˆ†æèƒ½åŠ›
é›†æˆWhisperã€éŸ³é¢‘å¢å¼ºã€æ–‡æœ¬ä¼˜åŒ–ç­‰æœåŠ¡
"""
import logging
import asyncio
import tempfile
import subprocess
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import librosa
import numpy as np
from datetime import datetime, timezone
import json
import re

# å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…åˆå§‹åŒ–æ—¶çš„ä¾èµ–é—®é¢˜
# from backend.services.audio_description_service import AudioDescriptionService
# from backend.services.whisper_service import WhisperService
# from backend.services.audio_enhancement import AudioEnhancementService
# from backend.services.text_optimization_service import TextOptimizationService
# from backend.services.llm_service import LLMService

logger = logging.getLogger("service")


class VideoAudioService:
    """
    è§†é¢‘éŸ³é¢‘åˆ†ææœåŠ¡
    ä¸“é—¨å¤„ç†è§†é¢‘ä¸­çš„éŸ³é¢‘å†…å®¹åˆ†æ
    """
    
    def __init__(self):
        # å»¶è¿Ÿåˆå§‹åŒ–æœåŠ¡ï¼Œé¿å…å¯¼å…¥æ—¶çš„ä¾èµ–é—®é¢˜
        self._audio_service = None
        self._whisper_service = None
        self._audio_enhancement = None
        self._text_optimizer = None
        self._llm_service = None
        logger.info("ğŸµ è§†é¢‘éŸ³é¢‘åˆ†ææœåŠ¡åˆå§‹åŒ–å®Œæˆ")
    
    @property
    def audio_service(self):
        """å»¶è¿ŸåŠ è½½éŸ³é¢‘æœåŠ¡"""
        if self._audio_service is None:
            from backend.services.audio_description_service import AudioDescriptionService
            self._audio_service = AudioDescriptionService(enable_preprocessing=True)
        return self._audio_service
    
    @property
    def whisper_service(self):
        """å»¶è¿ŸåŠ è½½WhisperæœåŠ¡"""
        if self._whisper_service is None:
            from backend.services.whisper_service import WhisperService
            self._whisper_service = WhisperService.get_instance()
        return self._whisper_service
    
    @property
    def audio_enhancement(self):
        """å»¶è¿ŸåŠ è½½éŸ³é¢‘å¢å¼ºæœåŠ¡"""
        if self._audio_enhancement is None:
            from backend.services.audio_enhancement import AudioEnhancementService
            self._audio_enhancement = AudioEnhancementService()
        return self._audio_enhancement
    
    @property
    def text_optimizer(self):
        """å»¶è¿ŸåŠ è½½æ–‡æœ¬ä¼˜åŒ–æœåŠ¡"""
        if self._text_optimizer is None:
            from backend.services.text_optimization_service import TextOptimizationService
            self._text_optimizer = TextOptimizationService()
        return self._text_optimizer
    
    @property
    def llm_service(self):
        """å»¶è¿ŸåŠ è½½LLMæœåŠ¡"""
        if self._llm_service is None:
            from backend.services.llm_service import LLMService
            self._llm_service = LLMService()
        return self._llm_service
    
    async def extract_audio_from_video(self, video_path: Path, output_dir: Path) -> Path:
        """
        ä»è§†é¢‘ä¸­æå–éŸ³é¢‘
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            æå–çš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        """
        try:
            logger.info(f"å¼€å§‹ä»è§†é¢‘æå–éŸ³é¢‘: {video_path.name}")
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # ç”ŸæˆéŸ³é¢‘æ–‡ä»¶å
            audio_filename = f"{video_path.stem}_audio.wav"
            audio_path = output_dir / audio_filename
            
            # ä½¿ç”¨ffmpegæå–éŸ³é¢‘
            cmd = [
                "ffmpeg", "-i", str(video_path),
                "-vn",  # ä¸åŒ…å«è§†é¢‘
                "-acodec", "pcm_s16le",  # 16ä½PCMç¼–ç 
                "-ar", "16000",  # 16kHzé‡‡æ ·ç‡ï¼ˆWhisperæ¨èï¼‰
                "-ac", "1",  # å•å£°é“
                "-y",  # è¦†ç›–è¾“å‡ºæ–‡ä»¶
                str(audio_path)
            ]
            
            logger.info(f"æ‰§è¡ŒéŸ³é¢‘æå–å‘½ä»¤: {' '.join(cmd)}")
            
            # æ‰§è¡Œå‘½ä»¤
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
            )
            
            if result.returncode != 0:
                raise RuntimeError(f"éŸ³é¢‘æå–å¤±è´¥: {result.stderr}")
            
            if not audio_path.exists():
                raise FileNotFoundError(f"éŸ³é¢‘æ–‡ä»¶æœªç”Ÿæˆ: {audio_path}")
            
            # è·å–éŸ³é¢‘ä¿¡æ¯
            audio_info = self._get_audio_info(audio_path)
            
            logger.info(f"éŸ³é¢‘æå–æˆåŠŸ: {audio_filename}")
            logger.info(f"éŸ³é¢‘ä¿¡æ¯: {audio_info['duration']:.2f}ç§’, {audio_info['sample_rate']}Hz")
            
            return audio_path
            
        except Exception as e:
            logger.error(f"éŸ³é¢‘æå–å¤±è´¥: {e}")
            raise
    
    def _get_audio_info(self, audio_path: Path) -> Dict[str, Any]:
        """è·å–éŸ³é¢‘åŸºæœ¬ä¿¡æ¯"""
        try:
            duration = librosa.get_duration(path=str(audio_path))
            y, sr = librosa.load(str(audio_path), sr=None, duration=1.0)  # åªåŠ è½½1ç§’ç”¨äºè·å–é‡‡æ ·ç‡
            
            return {
                "duration": duration,
                "sample_rate": sr,
                "file_size": audio_path.stat().st_size,
                "format": "wav"
            }
        except Exception as e:
            logger.error(f"è·å–éŸ³é¢‘ä¿¡æ¯å¤±è´¥: {e}")
            return {"duration": 0, "sample_rate": 0, "file_size": 0, "format": "unknown"}
    
    async def analyze_video_audio(self, video_path: Path, output_dir: Path) -> Dict[str, Any]:
        """
        åˆ†æè§†é¢‘éŸ³é¢‘å†…å®¹
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            éŸ³é¢‘åˆ†æç»“æœ
        """
        try:
            logger.info(f"å¼€å§‹è§†é¢‘éŸ³é¢‘åˆ†æ: {video_path.name}")
            
            # 1. ä»è§†é¢‘æå–éŸ³é¢‘
            logger.info("ç¬¬1æ­¥ï¼šæå–éŸ³é¢‘")
            audio_path = await self.extract_audio_from_video(video_path, output_dir)
            
            # 2. åŸºç¡€éŸ³é¢‘åˆ†æ
            logger.info("ç¬¬2æ­¥ï¼šåŸºç¡€éŸ³é¢‘åˆ†æ")
            basic_analysis = self.audio_service.analyze_audio(audio_path)
            
            # 3. å¢å¼ºè¯­éŸ³è¯†åˆ«
            logger.info("ç¬¬3æ­¥ï¼šå¢å¼ºè¯­éŸ³è¯†åˆ«")
            enhanced_speech = await self._enhanced_speech_recognition(audio_path)
            
            # 4. éŸ³é¢‘å†…å®¹è¯­ä¹‰åˆ†æ
            logger.info("ç¬¬4æ­¥ï¼šéŸ³é¢‘å†…å®¹è¯­ä¹‰åˆ†æ")
            semantic_analysis = await self._analyze_audio_semantics(enhanced_speech)
            
            # 5. æ—¶é—´è½´åˆ†æ
            logger.info("ç¬¬5æ­¥ï¼šæ—¶é—´è½´åˆ†æ")
            timeline_analysis = await self._analyze_audio_timeline(enhanced_speech)
            
            # 6. æ„å»ºå®Œæ•´åˆ†æç»“æœ
            analysis_result = {
                "audio_extraction": {
                    "extracted_audio_path": str(audio_path),
                    "audio_info": self._get_audio_info(audio_path),
                    "extraction_success": True
                },
                "basic_analysis": basic_analysis,
                "enhanced_speech": enhanced_speech,
                "semantic_analysis": semantic_analysis,
                "timeline_analysis": timeline_analysis,
                "analysis_metadata": {
                    "video_file": video_path.name,
                    "processing_timestamp": datetime.now(timezone.utc).isoformat(),
                    "analysis_type": "video_audio_semantic",
                    "services_used": [
                        "audio_extraction",
                        "whisper_recognition", 
                        "semantic_analysis",
                        "timeline_analysis"
                    ]
                }
            }
            
            logger.info(f"è§†é¢‘éŸ³é¢‘åˆ†æå®Œæˆ: {video_path.name}")
            return analysis_result
            
        except Exception as e:
            logger.error(f"è§†é¢‘éŸ³é¢‘åˆ†æå¤±è´¥: {e}")
            return {
                "error": str(e),
                "audio_extraction": {"extraction_success": False},
                "basic_analysis": {},
                "enhanced_speech": {},
                "semantic_analysis": {},
                "timeline_analysis": {},
                "analysis_metadata": {
                    "video_file": video_path.name if video_path else "unknown",
                    "error_timestamp": datetime.now(timezone.utc).isoformat(),
                    "analysis_type": "video_audio_semantic"
                }
            }
    
    async def _enhanced_speech_recognition(self, audio_path: Path) -> Dict[str, Any]:
        """
        å¢å¼ºè¯­éŸ³è¯†åˆ«ï¼ŒåŒ…å«åˆ†æ®µå’Œè¯¦ç»†æ—¶é—´æˆ³
        """
        try:
            logger.info("æ‰§è¡Œå¢å¼ºè¯­éŸ³è¯†åˆ«...")
            
            # ä½¿ç”¨ç°æœ‰çš„éŸ³é¢‘åˆ†ææœåŠ¡
            speech_result = self.audio_service.speech_to_text(audio_path)
            
            if not speech_result.get("success"):
                return {
                    "success": False,
                    "error": speech_result.get("error", "è¯­éŸ³è¯†åˆ«å¤±è´¥"),
                    "segments": [],
                    "full_text": "",
                    "language": "unknown"
                }
            
            # å¢å¼ºåˆ†æ®µä¿¡æ¯
            segments = speech_result.get("segments", [])
            enhanced_segments = []
            
            for segment in segments:
                enhanced_segment = {
                    "id": segment.get("id", 0),
                    "start_time": segment.get("start", 0.0),
                    "end_time": segment.get("end", 0.0),
                    "duration": segment.get("end", 0.0) - segment.get("start", 0.0),
                    "text": segment.get("text", "").strip(),
                    "confidence": segment.get("avg_logprob", 0.0),
                    "no_speech_prob": segment.get("no_speech_prob", 0.0),
                    "words": segment.get("words", [])  # å¦‚æœæœ‰è¯çº§æ—¶é—´æˆ³
                }
                enhanced_segments.append(enhanced_segment)
            
            # ç”Ÿæˆå®Œæ•´æ–‡æœ¬
            full_text = speech_result.get("transcribed_text", "")
            
            # è¯­è¨€æ£€æµ‹
            detected_language = speech_result.get("language_detected", "unknown")
            
            enhanced_result = {
                "success": True,
                "segments": enhanced_segments,
                "segments_count": len(enhanced_segments),
                "full_text": full_text,
                "language": detected_language,
                "total_duration": max([s.get("end_time", 0) for s in enhanced_segments], default=0),
                "confidence": speech_result.get("confidence", 0.0),
                "model_info": speech_result.get("model_info", {}),
                "preprocessing_info": speech_result.get("preprocessing_info", {})
            }
            
            logger.info(f"å¢å¼ºè¯­éŸ³è¯†åˆ«å®Œæˆ: {len(enhanced_segments)}ä¸ªç‰‡æ®µ")
            return enhanced_result
            
        except Exception as e:
            logger.error(f"å¢å¼ºè¯­éŸ³è¯†åˆ«å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "segments": [],
                "full_text": "",
                "language": "unknown"
            }
    
    async def _analyze_audio_semantics(self, speech_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        åˆ†æéŸ³é¢‘å†…å®¹çš„è¯­ä¹‰ä¿¡æ¯
        """
        try:
            logger.info("å¼€å§‹éŸ³é¢‘è¯­ä¹‰åˆ†æ...")
            
            if not speech_result.get("success") or not speech_result.get("full_text"):
                return {
                    "success": False,
                    "error": "æ²¡æœ‰æœ‰æ•ˆçš„è¯­éŸ³è¯†åˆ«ç»“æœ",
                    "content_analysis": {},
                    "emotion_analysis": {},
                    "topic_analysis": {},
                    "speaker_analysis": {}
                }
            
            full_text = speech_result.get("full_text", "")
            segments = speech_result.get("segments", [])
            
            # 1. å†…å®¹åˆ†ç±»åˆ†æ
            content_analysis = await self._analyze_content_type(full_text)
            
            # 2. æƒ…æ„Ÿåˆ†æ
            emotion_analysis = await self._analyze_emotions(full_text, segments)
            
            # 3. è¯é¢˜åˆ†æ
            topic_analysis = await self._analyze_topics(full_text)
            
            # 4. è¯´è¯äººåˆ†æï¼ˆåŸºäºéŸ³é¢‘ç‰¹å¾ï¼‰
            speaker_analysis = await self._analyze_speakers(segments)
            
            semantic_result = {
                "success": True,
                "content_analysis": content_analysis,
                "emotion_analysis": emotion_analysis,
                "topic_analysis": topic_analysis,
                "speaker_analysis": speaker_analysis,
                "text_statistics": {
                    "total_words": len(full_text.split()),
                    "total_characters": len(full_text),
                    "average_segment_length": np.mean([len(s.get("text", "")) for s in segments]) if segments else 0,
                    "speech_rate": len(full_text.split()) / speech_result.get("total_duration", 1) * 60  # è¯/åˆ†é’Ÿ
                }
            }
            
            logger.info("éŸ³é¢‘è¯­ä¹‰åˆ†æå®Œæˆ")
            return semantic_result
            
        except Exception as e:
            logger.error(f"éŸ³é¢‘è¯­ä¹‰åˆ†æå¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "content_analysis": {},
                "emotion_analysis": {},
                "topic_analysis": {},
                "speaker_analysis": {}
            }
    
    async def _analyze_content_type(self, text: str) -> Dict[str, Any]:
        """åˆ†æéŸ³é¢‘å†…å®¹ç±»å‹"""
        try:
            # ä½¿ç”¨LLMåˆ†æå†…å®¹ç±»å‹
            prompt = f"""
            è¯·åˆ†æä»¥ä¸‹éŸ³é¢‘è½¬å½•æ–‡æœ¬çš„å†…å®¹ç±»å‹å’Œç‰¹å¾ï¼š

            æ–‡æœ¬å†…å®¹ï¼š
            {text[:1000]}...

            è¯·æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼š
            {{
                "content_type": "å¯¹è¯/æ¼”è®²/æ•™å­¦/å¨±ä¹/æ–°é—»/å…¶ä»–",
                "content_style": "æ­£å¼/éæ­£å¼/å­¦æœ¯/å£è¯­åŒ–",
                "main_themes": ["ä¸»é¢˜1", "ä¸»é¢˜2", "ä¸»é¢˜3"],
                "content_quality": "é«˜/ä¸­/ä½",
                "estimated_audience": "ç›®æ ‡å—ä¼—æè¿°",
                "confidence": 0.85
            }}
            """
            
            response = await self.llm_service.generate_response(prompt)
            
            # è§£æJSONå“åº”
            try:
                content_analysis = json.loads(response)
            except json.JSONDecodeError:
                # å¦‚æœJSONè§£æå¤±è´¥ï¼Œåˆ›å»ºåŸºç¡€åˆ†æ
                content_analysis = {
                    "content_type": "unknown",
                    "content_style": "unknown",
                    "main_themes": [],
                    "content_quality": "unknown",
                    "estimated_audience": "unknown",
                    "confidence": 0.3
                }
            
            return content_analysis
            
        except Exception as e:
            logger.error(f"å†…å®¹ç±»å‹åˆ†æå¤±è´¥: {e}")
            return {
                "content_type": "unknown",
                "content_style": "unknown",
                "main_themes": [],
                "content_quality": "unknown",
                "estimated_audience": "unknown",
                "confidence": 0.0
            }
    
    async def _analyze_emotions(self, full_text: str, segments: List[Dict]) -> Dict[str, Any]:
        """åˆ†ææƒ…æ„Ÿå˜åŒ–"""
        try:
            # æ•´ä½“æƒ…æ„Ÿåˆ†æ
            overall_emotion = await self._analyze_text_emotion(full_text)
            
            # åˆ†æ®µæƒ…æ„Ÿåˆ†æ
            segment_emotions = []
            for segment in segments[:10]:  # é™åˆ¶åˆ†æå‰10ä¸ªç‰‡æ®µ
                if segment.get("text"):
                    emotion = await self._analyze_text_emotion(segment["text"])
                    segment_emotions.append({
                        "segment_id": segment.get("id", 0),
                        "start_time": segment.get("start_time", 0),
                        "end_time": segment.get("end_time", 0),
                        "emotion": emotion.get("dominant_emotion", "neutral"),
                        "confidence": emotion.get("confidence", 0.0)
                    })
            
            # æƒ…æ„Ÿå˜åŒ–åˆ†æ
            emotion_changes = self._detect_emotion_changes(segment_emotions)
            
            return {
                "overall_emotion": overall_emotion,
                "segment_emotions": segment_emotions,
                "emotion_changes": emotion_changes,
                "emotion_statistics": self._calculate_emotion_statistics(segment_emotions)
            }
            
        except Exception as e:
            logger.error(f"æƒ…æ„Ÿåˆ†æå¤±è´¥: {e}")
            return {
                "overall_emotion": {"dominant_emotion": "unknown", "confidence": 0.0},
                "segment_emotions": [],
                "emotion_changes": [],
                "emotion_statistics": {}
            }
    
    async def _analyze_text_emotion(self, text: str) -> Dict[str, Any]:
        """åˆ†ææ–‡æœ¬æƒ…æ„Ÿ"""
        try:
            prompt = f"""
            è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘ï¼š

            æ–‡æœ¬ï¼š{text[:500]}

            è¯·è¿”å›JSONæ ¼å¼ï¼š
            {{
                "dominant_emotion": "positive/negative/neutral/excited/sad/angry/surprised",
                "confidence": 0.85,
                "emotion_scores": {{
                    "positive": 0.7,
                    "negative": 0.1,
                    "neutral": 0.2
                }}
            }}
            """
            
            response = await self.llm_service.generate_response(prompt)
            
            try:
                emotion_result = json.loads(response)
            except json.JSONDecodeError:
                emotion_result = {
                    "dominant_emotion": "neutral",
                    "confidence": 0.3,
                    "emotion_scores": {"neutral": 1.0}
                }
            
            return emotion_result
            
        except Exception as e:
            logger.error(f"æ–‡æœ¬æƒ…æ„Ÿåˆ†æå¤±è´¥: {e}")
            return {
                "dominant_emotion": "unknown",
                "confidence": 0.0,
                "emotion_scores": {}
            }
    
    def _detect_emotion_changes(self, segment_emotions: List[Dict]) -> List[Dict]:
        """æ£€æµ‹æƒ…æ„Ÿå˜åŒ–ç‚¹"""
        changes = []
        
        for i in range(1, len(segment_emotions)):
            prev_emotion = segment_emotions[i-1].get("emotion", "neutral")
            curr_emotion = segment_emotions[i].get("emotion", "neutral")
            
            if prev_emotion != curr_emotion:
                changes.append({
                    "timestamp": segment_emotions[i].get("start_time", 0),
                    "from_emotion": prev_emotion,
                    "to_emotion": curr_emotion,
                    "change_type": "emotion_transition"
                })
        
        return changes
    
    def _calculate_emotion_statistics(self, segment_emotions: List[Dict]) -> Dict[str, Any]:
        """è®¡ç®—æƒ…æ„Ÿç»Ÿè®¡"""
        if not segment_emotions:
            return {}
        
        emotions = [s.get("emotion", "neutral") for s in segment_emotions]
        from collections import Counter
        emotion_counts = Counter(emotions)
        
        return {
            "dominant_emotion": emotion_counts.most_common(1)[0][0] if emotion_counts else "neutral",
            "emotion_distribution": dict(emotion_counts),
            "emotion_changes_count": len(self._detect_emotion_changes(segment_emotions)),
            "average_confidence": np.mean([s.get("confidence", 0) for s in segment_emotions])
        }
    
    async def _analyze_topics(self, text: str) -> Dict[str, Any]:
        """åˆ†æè¯é¢˜å’Œå…³é”®è¯"""
        try:
            prompt = f"""
            è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬çš„ä¸»è¦è¯é¢˜å’Œå…³é”®è¯ï¼š

            æ–‡æœ¬ï¼š{text[:1000]}

            è¯·è¿”å›JSONæ ¼å¼ï¼š
            {{
                "main_topics": ["è¯é¢˜1", "è¯é¢˜2", "è¯é¢˜3"],
                "keywords": ["å…³é”®è¯1", "å…³é”®è¯2", "å…³é”®è¯3"],
                "topic_categories": ["ç±»åˆ«1", "ç±»åˆ«2"],
                "summary": "ç®€è¦æ€»ç»“",
                "confidence": 0.85
            }}
            """
            
            response = await self.llm_service.generate_response(prompt)
            
            try:
                topic_result = json.loads(response)
            except json.JSONDecodeError:
                topic_result = {
                    "main_topics": [],
                    "keywords": [],
                    "topic_categories": [],
                    "summary": "è¯é¢˜åˆ†æå¤±è´¥",
                    "confidence": 0.3
                }
            
            return topic_result
            
        except Exception as e:
            logger.error(f"è¯é¢˜åˆ†æå¤±è´¥: {e}")
            return {
                "main_topics": [],
                "keywords": [],
                "topic_categories": [],
                "summary": "è¯é¢˜åˆ†æå¤±è´¥",
                "confidence": 0.0
            }
    
    async def _analyze_speakers(self, segments: List[Dict]) -> Dict[str, Any]:
        """åˆ†æè¯´è¯äººç‰¹å¾ï¼ˆåŸºäºéŸ³é¢‘ç‰¹å¾æ¨æ–­ï¼‰"""
        try:
            # ç®€å•çš„è¯´è¯äººåˆ†æï¼ˆåŸºäºç‰‡æ®µç‰¹å¾ï¼‰
            speaker_segments = []
            current_speaker = 1
            
            for segment in segments:
                # åŸºäºéŸ³é¢‘ç‰¹å¾æ¨æ–­è¯´è¯äººï¼ˆç®€åŒ–ç‰ˆï¼‰
                confidence = segment.get("confidence", 0.0)
                duration = segment.get("duration", 0.0)
                
                # ç®€å•çš„è¯´è¯äººåˆ‡æ¢æ£€æµ‹ï¼ˆåŸºäºç½®ä¿¡åº¦å’Œæ—¶é•¿å˜åŒ–ï¼‰
                if len(speaker_segments) > 0:
                    prev_confidence = speaker_segments[-1].get("confidence", 0.0)
                    if abs(confidence - prev_confidence) > 0.3:  # ç½®ä¿¡åº¦å˜åŒ–è¾ƒå¤§
                        current_speaker += 1
                
                speaker_segments.append({
                    "segment_id": segment.get("id", 0),
                    "speaker_id": current_speaker,
                    "start_time": segment.get("start_time", 0),
                    "end_time": segment.get("end_time", 0),
                    "confidence": confidence,
                    "text": segment.get("text", "")
                })
            
            # ç»Ÿè®¡è¯´è¯äººä¿¡æ¯
            speaker_stats = {}
            for seg in speaker_segments:
                speaker_id = seg["speaker_id"]
                if speaker_id not in speaker_stats:
                    speaker_stats[speaker_id] = {
                        "total_duration": 0,
                        "segment_count": 0,
                        "total_words": 0
                    }
                
                speaker_stats[speaker_id]["total_duration"] += seg.get("end_time", 0) - seg.get("start_time", 0)
                speaker_stats[speaker_id]["segment_count"] += 1
                speaker_stats[speaker_id]["total_words"] += len(seg.get("text", "").split())
            
            return {
                "speaker_segments": speaker_segments,
                "estimated_speakers": len(speaker_stats),
                "speaker_statistics": speaker_stats,
                "analysis_method": "audio_feature_based"
            }
            
        except Exception as e:
            logger.error(f"è¯´è¯äººåˆ†æå¤±è´¥: {e}")
            return {
                "speaker_segments": [],
                "estimated_speakers": 1,
                "speaker_statistics": {},
                "analysis_method": "failed"
            }
    
    async def _analyze_audio_timeline(self, speech_result: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æéŸ³é¢‘æ—¶é—´è½´ç‰¹å¾"""
        try:
            logger.info("å¼€å§‹éŸ³é¢‘æ—¶é—´è½´åˆ†æ...")
            
            if not speech_result.get("success"):
                return {
                    "success": False,
                    "error": "æ²¡æœ‰æœ‰æ•ˆçš„è¯­éŸ³è¯†åˆ«ç»“æœ"
                }
            
            segments = speech_result.get("segments", [])
            total_duration = speech_result.get("total_duration", 0)
            
            # 1. è¯­éŸ³æ´»åŠ¨åˆ†æ
            speech_activity = self._analyze_speech_activity(segments, total_duration)
            
            # 2. èŠ‚å¥åˆ†æ
            rhythm_analysis = self._analyze_speech_rhythm(segments)
            
            # 3. åœé¡¿åˆ†æ
            pause_analysis = self._analyze_pauses(segments)
            
            # 4. è¯­é€Ÿåˆ†æ
            speech_rate_analysis = self._analyze_speech_rate(segments)
            
            timeline_result = {
                "success": True,
                "speech_activity": speech_activity,
                "rhythm_analysis": rhythm_analysis,
                "pause_analysis": pause_analysis,
                "speech_rate_analysis": speech_rate_analysis,
                "timeline_statistics": {
                    "total_duration": total_duration,
                    "speech_duration": sum([s.get("duration", 0) for s in segments]),
                    "silence_duration": total_duration - sum([s.get("duration", 0) for s in segments]),
                    "segments_count": len(segments)
                }
            }
            
            logger.info("éŸ³é¢‘æ—¶é—´è½´åˆ†æå®Œæˆ")
            return timeline_result
            
        except Exception as e:
            logger.error(f"éŸ³é¢‘æ—¶é—´è½´åˆ†æå¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def _analyze_speech_activity(self, segments: List[Dict], total_duration: float) -> Dict[str, Any]:
        """åˆ†æè¯­éŸ³æ´»åŠ¨"""
        if not segments:
            return {"activity_ratio": 0.0, "active_periods": []}
        
        # è®¡ç®—è¯­éŸ³æ´»åŠ¨æ¯”ä¾‹
        speech_duration = sum([s.get("duration", 0) for s in segments])
        activity_ratio = speech_duration / total_duration if total_duration > 0 else 0
        
        # è¯†åˆ«æ´»è·ƒæ—¶æ®µ
        active_periods = []
        for segment in segments:
            if segment.get("duration", 0) > 2.0:  # è¶…è¿‡2ç§’çš„ç‰‡æ®µè®¤ä¸ºæ˜¯æ´»è·ƒæ—¶æ®µ
                active_periods.append({
                    "start": segment.get("start_time", 0),
                    "end": segment.get("end_time", 0),
                    "duration": segment.get("duration", 0)
                })
        
        return {
            "activity_ratio": round(activity_ratio, 3),
            "active_periods": active_periods,
            "total_active_periods": len(active_periods)
        }
    
    def _analyze_speech_rhythm(self, segments: List[Dict]) -> Dict[str, Any]:
        """åˆ†æè¯­éŸ³èŠ‚å¥"""
        if not segments:
            return {"rhythm_type": "unknown", "rhythm_stability": 0.0}
        
        # è®¡ç®—ç‰‡æ®µé•¿åº¦å˜åŒ–
        durations = [s.get("duration", 0) for s in segments]
        avg_duration = np.mean(durations)
        duration_std = np.std(durations)
        
        # èŠ‚å¥ç¨³å®šæ€§ï¼ˆæ ‡å‡†å·®è¶Šå°è¶Šç¨³å®šï¼‰
        rhythm_stability = 1.0 - min(duration_std / avg_duration, 1.0) if avg_duration > 0 else 0.0
        
        # èŠ‚å¥ç±»å‹åˆ¤æ–­
        if avg_duration > 10.0:
            rhythm_type = "slow"
        elif avg_duration < 3.0:
            rhythm_type = "fast"
        else:
            rhythm_type = "moderate"
        
        return {
            "rhythm_type": rhythm_type,
            "rhythm_stability": round(rhythm_stability, 3),
            "average_segment_duration": round(avg_duration, 2),
            "duration_variance": round(duration_std, 2)
        }
    
    def _analyze_pauses(self, segments: List[Dict]) -> Dict[str, Any]:
        """åˆ†æåœé¡¿æ¨¡å¼"""
        if len(segments) < 2:
            return {"pause_count": 0, "average_pause_duration": 0.0}
        
        # è®¡ç®—ç‰‡æ®µé—´çš„åœé¡¿
        pauses = []
        for i in range(1, len(segments)):
            prev_end = segments[i-1].get("end_time", 0)
            curr_start = segments[i].get("start_time", 0)
            pause_duration = curr_start - prev_end
            
            if pause_duration > 0.1:  # å¤§äº0.1ç§’è®¤ä¸ºæ˜¯åœé¡¿
                pauses.append({
                    "start": prev_end,
                    "end": curr_start,
                    "duration": pause_duration
                })
        
        if not pauses:
            return {"pause_count": 0, "average_pause_duration": 0.0}
        
        avg_pause = np.mean([p["duration"] for p in pauses])
        
        return {
            "pause_count": len(pauses),
            "average_pause_duration": round(avg_pause, 2),
            "total_pause_duration": round(sum([p["duration"] for p in pauses]), 2),
            "longest_pause": round(max([p["duration"] for p in pauses]), 2)
        }
    
    def _analyze_speech_rate(self, segments: List[Dict]) -> Dict[str, Any]:
        """åˆ†æè¯­é€Ÿ"""
        if not segments:
            return {"words_per_minute": 0.0, "speech_rate_type": "unknown"}
        
        # è®¡ç®—æ¯ä¸ªç‰‡æ®µçš„è¯­é€Ÿ
        segment_rates = []
        for segment in segments:
            text = segment.get("text", "")
            duration = segment.get("duration", 0)
            
            if duration > 0:
                word_count = len(text.split())
                rate = (word_count / duration) * 60  # è¯/åˆ†é’Ÿ
                segment_rates.append(rate)
        
        if not segment_rates:
            return {"words_per_minute": 0.0, "speech_rate_type": "unknown"}
        
        avg_rate = np.mean(segment_rates)
        
        # è¯­é€Ÿç±»å‹åˆ¤æ–­
        if avg_rate > 180:
            rate_type = "fast"
        elif avg_rate < 120:
            rate_type = "slow"
        else:
            rate_type = "normal"
        
        return {
            "words_per_minute": round(avg_rate, 1),
            "speech_rate_type": rate_type,
            "rate_variance": round(np.std(segment_rates), 1),
            "min_rate": round(min(segment_rates), 1),
            "max_rate": round(max(segment_rates), 1)
        }


# åˆ›å»ºå…¨å±€å®ä¾‹
video_audio_service = VideoAudioService() 