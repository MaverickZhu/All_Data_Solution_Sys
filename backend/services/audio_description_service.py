import logging
import re
from pathlib import Path
from typing import Dict, Optional
import librosa
import numpy as np
import tempfile
import os
import whisper
from .audio_enhancement import audio_enhancement_service
from .text_optimization_service import TextOptimizationService

logger = logging.getLogger(__name__)

class AudioDescriptionService:
    """éŸ³é¢‘è¯­éŸ³è¯†åˆ«æœåŠ¡ï¼Œä½¿ç”¨Whisperæ¨¡å‹è¿›è¡Œè¯­éŸ³è½¬æ–‡å­—"""
    
    def __init__(self, whisper_model: str = "base", enable_preprocessing: bool = True):
        self.whisper_model_name = whisper_model
        self._whisper_model = None  # Lazy loading
        self.enable_preprocessing = enable_preprocessing
        self.text_optimizer = TextOptimizationService()
        logger.info(f"ğŸµ éŸ³é¢‘åˆ†ææœåŠ¡åˆå§‹åŒ–ï¼Œæ¨¡å‹: {whisper_model}, é¢„å¤„ç†: {enable_preprocessing}")
    
    def _get_whisper_model(self):
        """æ‡’åŠ è½½Whisperæ¨¡å‹"""
        if self._whisper_model is None:
            try:
                logger.info(f"Loading Whisper model: {self.whisper_model_name}")
                self._whisper_model = whisper.load_model(self.whisper_model_name)
                logger.info(f"Whisper model loaded successfully. Multilingual: {self._whisper_model.is_multilingual}")
            except Exception as e:
                logger.error(f"Failed to load Whisper model: {e}")
                raise
        return self._whisper_model
        
    def extract_audio_features(self, audio_path: Path) -> Dict:
        """æå–éŸ³é¢‘çš„åŸºç¡€ç‰¹å¾"""
        try:
            # åŠ è½½éŸ³é¢‘æ–‡ä»¶
            y, sr = librosa.load(str(audio_path), sr=None, duration=30.0)  # åªåˆ†æå‰30ç§’
            
            # æå–åŸºç¡€ç‰¹å¾
            duration = len(y) / sr
            
            # éŸ³é¢‘å¼ºåº¦ç‰¹å¾
            rms = librosa.feature.rms(y=y)[0]
            avg_amplitude = np.mean(rms)
            max_amplitude = np.max(rms)
            
            # é¢‘è°±ç‰¹å¾
            spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
            avg_spectral_centroid = np.mean(spectral_centroids)
            
            # é›¶äº¤å‰ç‡
            zcr = librosa.feature.zero_crossing_rate(y)[0]
            avg_zcr = np.mean(zcr)
            
            # é¢‘è°±æ»šé™
            spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]
            avg_spectral_rolloff = np.mean(spectral_rolloff)
            
            # MFCCç‰¹å¾
            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
            mfcc_means = np.mean(mfccs, axis=1)
            
            # èŠ‚æ‹æ£€æµ‹
            try:
                tempo, beats = librosa.beat.beat_track(y=y, sr=sr)
            except Exception:
                tempo = 0
                beats = []
            
            return {
                "duration": float(duration),
                "sample_rate": int(sr),
                "avg_amplitude": float(avg_amplitude),
                "max_amplitude": float(max_amplitude),
                "avg_spectral_centroid": float(avg_spectral_centroid),
                "avg_zero_crossing_rate": float(avg_zcr),
                "avg_spectral_rolloff": float(avg_spectral_rolloff),
                "mfcc_features": mfcc_means.tolist(),
                "tempo": float(tempo) if tempo > 0 else None,
                "beat_count": len(beats) if beats is not None else 0
            }
            
        except Exception as e:
            logger.error(f"Failed to extract audio features from {audio_path}: {e}")
            return {}
    
    def speech_to_text(self, audio_path: Path) -> Dict:
        """ä½¿ç”¨Whisperè¿›è¡Œè¯­éŸ³è¯†åˆ«ï¼Œå°†éŸ³é¢‘è½¬æ¢ä¸ºæ–‡å­—ï¼Œå¯é€‰éŸ³é¢‘é¢„å¤„ç†"""
        try:
            logger.info(f"ğŸ¯ å¼€å§‹è¯­éŸ³è¯†åˆ«å¤„ç†: {audio_path}")
            
            # éŸ³é¢‘é¢„å¤„ç†å’Œå»å™ª
            processed_audio_path = audio_path
            preprocessing_info = {}
            
            if self.enable_preprocessing:
                try:
                    logger.info("ğŸ”§ æ‰§è¡ŒéŸ³é¢‘é¢„å¤„ç†...")
                    
                    # åˆ†æéŸ³é¢‘å™ªå£°æ°´å¹³
                    noise_analysis = audio_enhancement_service.analyze_noise_level(audio_path)
                    preprocessing_info["noise_analysis"] = noise_analysis
                    
                    # æ ¹æ®å™ªå£°æ°´å¹³å†³å®šæ˜¯å¦è¿›è¡Œå»å™ª
                    if noise_analysis.get("enhancement_recommended", True):
                        logger.info(f"ğŸ“Š æ£€æµ‹åˆ°å™ªå£°æ°´å¹³: {noise_analysis.get('noise_level', 'unknown')}, å¼€å§‹å»å™ªå¤„ç†...")
                        
                        # æ‰§è¡ŒéŸ³é¢‘å¢å¼ºç®¡é“
                        processed_audio_path = audio_enhancement_service.enhance_audio_pipeline(
                            audio_path,
                            enable_denoise=True,
                            enable_bandpass=True,
                            enable_silence_removal=True,
                            enable_normalization=True
                        )
                        
                        preprocessing_info["enhancement_applied"] = True
                        preprocessing_info["enhanced_file"] = str(processed_audio_path)
                        logger.info(f"âœ… éŸ³é¢‘é¢„å¤„ç†å®Œæˆï¼Œä½¿ç”¨å¢å¼ºéŸ³é¢‘: {processed_audio_path.name}")
                    else:
                        logger.info("ğŸµ éŸ³é¢‘è´¨é‡è‰¯å¥½ï¼Œè·³è¿‡é¢„å¤„ç†")
                        preprocessing_info["enhancement_applied"] = False
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ éŸ³é¢‘é¢„å¤„ç†å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹éŸ³é¢‘: {e}")
                    processed_audio_path = audio_path
                    preprocessing_info["enhancement_applied"] = False
                    preprocessing_info["preprocessing_error"] = str(e)
            
            # è·å–Whisperæ¨¡å‹
            model = self._get_whisper_model()
            
            logger.info(f"ğŸ™ï¸ å¼€å§‹Whisperè¯­éŸ³è¯†åˆ«...")
            
            # ä½¿ç”¨Whisperè¿›è¡Œè½¬å½•
            # Whisperä¼šè‡ªåŠ¨å¤„ç†éŸ³é¢‘æ ¼å¼è½¬æ¢å’Œé¢„å¤„ç†
            result = model.transcribe(
                str(processed_audio_path),
                language="zh",  # é¦–å…ˆå°è¯•ä¸­æ–‡
                task="transcribe",  # è½¬å½•ä»»åŠ¡ï¼ˆè€Œéç¿»è¯‘ï¼‰
                verbose=False
            )
            
            # æå–è½¬å½•ç»“æœ
            transcribed_text = result.get("text", "").strip()
            detected_language = result.get("language", "unknown")
            
            # å¦‚æœä¸­æ–‡è¯†åˆ«ç»“æœå¾ˆçŸ­æˆ–ä¸ºç©ºï¼Œå°è¯•è‹±æ–‡è¯†åˆ«
            if len(transcribed_text) < 3:
                logger.info("Chinese transcription result is too short, trying English...")
                result_en = model.transcribe(
                    str(audio_path),
                    language="en",
                    task="transcribe",
                    verbose=False
                )
                
                transcribed_text_en = result_en.get("text", "").strip()
                
                # é€‰æ‹©æ›´é•¿çš„ç»“æœ
                if len(transcribed_text_en) > len(transcribed_text):
                    transcribed_text = transcribed_text_en
                    detected_language = result_en.get("language", "en")
                    result = result_en
                    logger.info("Selected English transcription result")
            
            # å¦‚æœè¿˜æ˜¯æ²¡æœ‰å¥½çš„ç»“æœï¼Œå°è¯•è‡ªåŠ¨è¯­è¨€æ£€æµ‹
            if len(transcribed_text) < 3:
                logger.info("Trying automatic language detection...")
                result_auto = model.transcribe(
                    str(audio_path),
                    task="transcribe",
                    verbose=False
                )
                
                transcribed_text_auto = result_auto.get("text", "").strip()
                
                if len(transcribed_text_auto) > len(transcribed_text):
                    transcribed_text = transcribed_text_auto
                    detected_language = result_auto.get("language", "auto")
                    result = result_auto
                    logger.info("Selected automatic detection result")
            
            if transcribed_text:
                # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºsegmentsä¸­çš„ç½®ä¿¡åº¦ä¿¡æ¯ï¼‰
                segments = result.get("segments", [])
                total_confidence = 0
                total_duration = 0
                
                for segment in segments:
                    if 'confidence' in segment and 'end' in segment and 'start' in segment:
                        duration = segment['end'] - segment['start']
                        total_confidence += segment.get('confidence', 0) * duration
                        total_duration += duration
                
                avg_confidence = total_confidence / total_duration if total_duration > 0 else 0
                
                # åº”ç”¨æ–‡æœ¬ä¼˜åŒ–
                logger.info(f"ğŸ§  å¼€å§‹æ–‡æœ¬ä¼˜åŒ–ï¼šåŸå§‹æ–‡æœ¬é•¿åº¦ {len(transcribed_text)} å­—ç¬¦")
                text_optimization_result = self.text_optimizer.optimize_speech_text(
                    transcribed_text, 
                    detected_language
                )
                logger.info(f"ğŸ§  æ–‡æœ¬ä¼˜åŒ–å®Œæˆï¼šæˆåŠŸ={text_optimization_result.get('success')}, ä¼˜åŒ–åé•¿åº¦={len(text_optimization_result.get('optimized_text', ''))}")
                logger.info(f"ğŸ§  åº”ç”¨æ”¹è¿›ï¼š{text_optimization_result.get('improvements', [])}")
                
                # æ„å»ºè¿”å›ç»“æœï¼ŒåŒ…å«é¢„å¤„ç†ä¿¡æ¯å’Œæ–‡æœ¬ä¼˜åŒ–ä¿¡æ¯
                optimized_text = text_optimization_result.get("optimized_text", transcribed_text)
                logger.info(f"ğŸ¯ æœ€ç»ˆæ–‡æœ¬å¯¹æ¯”ï¼šåŸå§‹='{transcribed_text[:50]}...' ä¼˜åŒ–='{optimized_text[:50]}...'")
                
                speech_result = {
                    "success": True,
                    "transcribed_text": optimized_text,  # ä½¿ç”¨ä¼˜åŒ–åçš„æ–‡æœ¬
                    "raw_text": transcribed_text,  # ä¿ç•™åŸå§‹è¯†åˆ«æ–‡æœ¬
                    "confidence": round(avg_confidence, 3),
                    "language_detected": detected_language,
                    "segments": segments,
                    "segments_count": len(segments),
                    "model_info": {
                        "model": self.whisper_model_name,
                        "multilingual": model.is_multilingual
                    },
                    "preprocessing_info": preprocessing_info,
                    "text_optimization": text_optimization_result
                }
                
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if (self.enable_preprocessing and 
                    preprocessing_info.get("enhancement_applied") and 
                    processed_audio_path != audio_path):
                    try:
                        os.unlink(processed_audio_path)
                        logger.info(f"ğŸ—‘ï¸ æ¸…ç†ä¸´æ—¶å¢å¼ºéŸ³é¢‘æ–‡ä»¶: {processed_audio_path.name}")
                    except Exception as e:
                        logger.warning(f"âš ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
                
                return speech_result
            else:
                return {
                    "success": False,
                    "error": "No speech content detected in audio",
                    "transcribed_text": "",
                    "confidence": 0,
                    "language_detected": "unknown"
                }
                
        except Exception as e:
            logger.error(f"Whisper speech recognition failed for {audio_path}: {e}")
            return {
                "success": False,
                "error": str(e),
                "transcribed_text": "",
                "confidence": 0,
                "language_detected": "unknown"
            }
    
    def detect_audio_content_type(self, audio_features: Dict, speech_result: Dict = None) -> str:
        """æ£€æµ‹éŸ³é¢‘å†…å®¹ç±»å‹"""
        try:
            # å¦‚æœæœ‰è¯­éŸ³è¯†åˆ«ç»“æœï¼Œè¯´æ˜å¾ˆå¯èƒ½æ˜¯è¯­éŸ³å†…å®¹
            if speech_result and speech_result.get("success") and speech_result.get("transcribed_text"):
                return "speech"
            
            # åŸºäºéŸ³é¢‘ç‰¹å¾çš„å¯å‘å¼åˆ¤æ–­
            zero_crossing_rate = audio_features.get("avg_zero_crossing_rate", 0)
            spectral_centroid = audio_features.get("avg_spectral_centroid", 0)
            tempo = audio_features.get("tempo", 0)
            duration = audio_features.get("duration", 0)
            
            if zero_crossing_rate > 0.1:  # é«˜é›¶äº¤å‰ç‡å¯èƒ½æ˜¯è¯­éŸ³
                return "speech"
            elif tempo and tempo > 60:  # æœ‰æ˜æ˜¾èŠ‚æ‹å¯èƒ½æ˜¯éŸ³ä¹
                return "music"
            elif spectral_centroid > 3000:  # é«˜é¢‘å†…å®¹å¯èƒ½æ˜¯éŸ³ä¹
                return "music"
            elif duration < 10:  # çŸ­éŸ³é¢‘å¯èƒ½æ˜¯éŸ³æ•ˆ
                return "sound_effect"
            else:
                return "unknown"
                
        except Exception as e:
            logger.error(f"Audio content type detection failed: {e}")
            return "unknown"

    def analyze_audio(self, audio_path: Path) -> Dict:
        """åˆ†æéŸ³é¢‘æ–‡ä»¶ï¼ŒåŒ…æ‹¬ç‰¹å¾æå–å’Œè¯­éŸ³è¯†åˆ«"""
        try:
            logger.info(f"Starting comprehensive audio analysis for {audio_path}")
            
            # æå–éŸ³é¢‘ç‰¹å¾
            audio_features = self.extract_audio_features(audio_path)
            
            # å°è¯•è¯­éŸ³è¯†åˆ«
            speech_result = self.speech_to_text(audio_path)
            
            # æ£€æµ‹éŸ³é¢‘å†…å®¹ç±»å‹
            content_type = self.detect_audio_content_type(audio_features, speech_result)
            
            return {
                "success": True,
                "audio_features": audio_features,
                "speech_recognition": speech_result,
                "content_type": content_type,
                "analysis_summary": {
                    "file_analyzed": str(audio_path),
                    "has_speech": speech_result.get("success", False),
                    "detected_type": content_type,
                    "processing_time": "completed"
                }
            }
                
        except Exception as e:
            logger.error(f"Error in comprehensive audio analysis: {e}", exc_info=True)
            return {
                "success": False,
                "error": str(e),
                "audio_features": {},
                "speech_recognition": {
                    "success": False,
                    "error": str(e)
                },
                "content_type": "unknown"
            } 