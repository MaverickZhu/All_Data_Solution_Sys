"""
æ™ºèƒ½è¯­ä¹‰æ ‡ç‚¹å’Œæ®µè½åˆ†å‰²æœåŠ¡
åŸºäºè¯­ä¹‰åˆ†æä¸ºè¯­éŸ³è¯†åˆ«ç»“æœæ·»åŠ åˆç†çš„æ ‡ç‚¹ç¬¦å·å’Œæ®µè½ç»“æ„
"""
import re
import logging
from typing import List, Dict, Tuple, Any
import jieba
import jieba.posseg as pseg

logger = logging.getLogger(__name__)

class SemanticPunctuationService:
    """
    åŸºäºè¯­ä¹‰åˆ†æçš„æ™ºèƒ½æ ‡ç‚¹å’Œæ®µè½åˆ†å‰²æœåŠ¡
    ä¸“é—¨é’ˆå¯¹è¯­éŸ³è¯†åˆ«ç»“æœè¿›è¡Œæ™ºèƒ½åŒ–å¤„ç†
    """
    
    def __init__(self):
        # å¥å­ç»“æŸçš„è¯­ä¹‰æ ‡å¿—è¯
        self.sentence_endings = {
            'å®Œäº†', 'å¥½äº†', 'ç»“æŸäº†', 'è¡Œäº†', 'å¯ä»¥äº†', 'æå®šäº†', 'æ²¡é—®é¢˜',
            'æ˜ç™½äº†', 'çŸ¥é“äº†', 'æ¸…æ¥šäº†', 'æ‡‚äº†', 'å¯¹äº†', 'æ˜¯çš„', 'å¯¹çš„',
            'ä¸æ˜¯', 'ä¸å¯¹', 'é”™äº†', 'ç®—äº†', 'ç½¢äº†', 'å°±è¿™æ ·'
        }
        
        # é—®å¥æ ‡å¿—è¯ï¼ˆæ›´å…¨é¢ï¼‰
        self.question_words = {
            'ä»€ä¹ˆ', 'å“ªä¸ª', 'å“ªé‡Œ', 'å“ªå„¿', 'æ€ä¹ˆ', 'å¦‚ä½•', 'ä¸ºä»€ä¹ˆ',
            'è°', 'ä»€ä¹ˆæ—¶å€™', 'å¤šå°‘', 'å‡ ', 'å—', 'å‘¢', 'æ˜¯å§', 'å¯¹å§',
            'è¡Œä¸è¡Œ', 'å¯ä»¥å—', 'èƒ½ä¸èƒ½', 'ä¼šä¸ä¼š', 'æ˜¯ä¸æ˜¯', 'æœ‰æ²¡æœ‰'
        }
        
        # è½¬æŠ˜è¿æ¥è¯
        self.transition_words = {
            'ä½†æ˜¯', 'å¯æ˜¯', 'ä¸è¿‡', 'ç„¶è€Œ', 'è€Œä¸”', 'å¹¶ä¸”', 'å¦å¤–',
            'è¿˜æœ‰', 'æ¥ç€', 'ç„¶å', 'æ‰€ä»¥', 'å› æ­¤', 'å› ä¸º', 'ç”±äº',
            'å¦‚æœ', 'è¦æ˜¯', 'å‡å¦‚', 'è™½ç„¶', 'å°½ç®¡', 'å³ä½¿'
        }
        
        # åœé¡¿è¯ï¼ˆè¡¨ç¤ºè‡ªç„¶çš„è¯­éŸ³åœé¡¿ï¼‰
        self.pause_indicators = {
            'é‚£ä¸ª', 'è¿™ä¸ª', 'å°±æ˜¯', 'å—¯', 'å•Š', 'å‘ƒ', 'é‚£ä¹ˆ', 'è¿™æ ·',
            'æ¯”å¦‚è¯´', 'æ¢å¥è¯è¯´', 'æ€»ä¹‹', 'æ€»çš„æ¥è¯´', 'ç®€å•æ¥è¯´'
        }
        
        # æ—¶é—´ç›¸å…³è¯æ±‡
        self.time_indicators = {
            'ç°åœ¨', 'ä»Šå¤©', 'æ˜å¤©', 'æ˜¨å¤©', 'ä¸Šåˆ', 'ä¸‹åˆ', 'æ™šä¸Š',
            'æ—©ä¸Š', 'ä¸­åˆ', 'åˆšæ‰', 'é©¬ä¸Š', 'ç«‹å³', 'æ¥ä¸‹æ¥',
            'ä¹‹å‰', 'ä¹‹å', 'åæ¥', 'æœ€å', 'å¼€å§‹', 'ç»“æŸ'
        }
        
        # é€»è¾‘å…³ç³»è¯
        self.logic_words = {
            'é¦–å…ˆ', 'å…¶æ¬¡', 'ç¬¬ä¸€', 'ç¬¬äºŒ', 'æœ€å', 'æ€»ç»“',
            'é‡è¦çš„æ˜¯', 'å…³é”®æ˜¯', 'é—®é¢˜æ˜¯', 'äº‹å®ä¸Š', 'å®é™…ä¸Š'
        }
    
    def add_intelligent_punctuation(self, text: str) -> Dict[str, Any]:
        """
        ä¸ºæ–‡æœ¬æ·»åŠ æ™ºèƒ½æ ‡ç‚¹ç¬¦å·å’Œæ®µè½åˆ†å‰²
        
        Args:
            text: åŸå§‹è¯­éŸ³è¯†åˆ«æ–‡æœ¬
            
        Returns:
            åŒ…å«å¤„ç†ç»“æœçš„å­—å…¸
        """
        if not text or len(text.strip()) < 3:
            return self._create_result(text, "æ–‡æœ¬è¿‡çŸ­ï¼Œæ— éœ€å¤„ç†")
        
        try:
            original_text = text.strip()
            logger.info(f"ğŸ”¤ å¼€å§‹è¯­ä¹‰æ ‡ç‚¹åˆ†æï¼Œæ–‡æœ¬é•¿åº¦: {len(original_text)}")
            
            # ç¬¬ä¸€æ­¥ï¼šæ¸…ç†å’Œé¢„å¤„ç†
            cleaned_text = self._preprocess_text(original_text)
            
            # ç¬¬äºŒæ­¥ï¼šè¯­ä¹‰åˆ†è¯å’Œè¯æ€§åˆ†æ
            segments = self._semantic_segmentation(cleaned_text)
            
            # ç¬¬ä¸‰æ­¥ï¼šæ™ºèƒ½æ·»åŠ æ ‡ç‚¹ç¬¦å·
            punctuated_text = self._add_semantic_punctuation(segments)
            
            # ç¬¬å››æ­¥ï¼šæ®µè½åˆ†å‰²
            paragraphed_text = self._create_paragraphs(punctuated_text)
            
            # ç¬¬äº”æ­¥ï¼šæœ€ç»ˆä¼˜åŒ–
            final_text = self._final_polish(paragraphed_text)
            
            improvements = self._analyze_improvements(original_text, final_text)
            
            logger.info(f"âœ… è¯­ä¹‰æ ‡ç‚¹åˆ†æå®Œæˆ")
            logger.info(f"ğŸ“ åŸå§‹é•¿åº¦: {len(original_text)} -> å¤„ç†åé•¿åº¦: {len(final_text)}")
            logger.info(f"ğŸ”§ åº”ç”¨æ”¹è¿›: {improvements}")
            
            return self._create_result(final_text, "è¯­ä¹‰åˆ†æå®Œæˆ", improvements)
            
        except Exception as e:
            logger.error(f"âŒ è¯­ä¹‰æ ‡ç‚¹åˆ†æå¤±è´¥: {e}", exc_info=True)
            return self._create_result(text, f"å¤„ç†å¤±è´¥ï¼Œä¿ç•™åŸæ–‡: {str(e)}")
    
    def _preprocess_text(self, text: str) -> str:
        """é¢„å¤„ç†æ–‡æœ¬ï¼Œæ¸…ç†æ ¼å¼"""
        # ç»Ÿä¸€ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text)
        # ç§»é™¤å¤šä½™çš„æ ‡ç‚¹
        text = re.sub(r'[ã€‚ï¼ï¼Ÿï¼Œï¼›ï¼š]+', '', text)
        return text.strip()
    
    def _semantic_segmentation(self, text: str) -> List[Dict[str, Any]]:
        """è¯­ä¹‰åˆ†è¯å’Œåˆ†æ"""
        segments = []
        
        # ä½¿ç”¨jiebaè¿›è¡Œè¯æ€§æ ‡æ³¨
        words = list(pseg.cut(text))
        
        current_segment = {
            'words': [],
            'semantic_type': 'statement',  # statement, question, transition
            'confidence': 1.0,
            'should_end': False
        }
        
        for word, pos in words:
            word_info = {
                'text': word,
                'pos': pos,
                'semantic_roles': self._analyze_word_semantics(word)
            }
            
            current_segment['words'].append(word_info)
            
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥ç»“æŸå½“å‰ç‰‡æ®µ
            if self._should_segment_here(word, pos, current_segment):
                segments.append(current_segment)
                current_segment = {
                    'words': [],
                    'semantic_type': self._determine_segment_type(word),
                    'confidence': 1.0,
                    'should_end': False
                }
        
        # æ·»åŠ æœ€åä¸€ä¸ªç‰‡æ®µ
        if current_segment['words']:
            segments.append(current_segment)
        
        return segments
    
    def _analyze_word_semantics(self, word: str) -> List[str]:
        """åˆ†æè¯æ±‡çš„è¯­ä¹‰è§’è‰²"""
        roles = []
        
        if word in self.question_words:
            roles.append('question_indicator')
        if word in self.sentence_endings:
            roles.append('sentence_ending')
        if word in self.transition_words:
            roles.append('transition')
        if word in self.pause_indicators:
            roles.append('pause')
        if word in self.time_indicators:
            roles.append('time')
        if word in self.logic_words:
            roles.append('logic')
        
        return roles
    
    def _should_segment_here(self, word: str, pos: str, current_segment: Dict) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥åœ¨æ­¤å¤„åˆ†æ®µ"""
        # æ£€æŸ¥è¯­ä¹‰æ ‡å¿—
        semantic_roles = self._analyze_word_semantics(word)
        
        # å¥å­ç»“æŸæ ‡å¿—
        if 'sentence_ending' in semantic_roles:
            return True
        
        # è½¬æŠ˜è¯é€šå¸¸å¼€å§‹æ–°å¥å­
        if 'transition' in semantic_roles and len(current_segment['words']) > 3:
            return True
        
        # é€»è¾‘è¯å¼€å§‹æ–°æ®µè½
        if 'logic' in semantic_roles and len(current_segment['words']) > 5:
            return True
        
        # æ—¶é—´è¯å¯èƒ½è¡¨ç¤ºæ–°çš„æ—¶é—´æ®µ
        if 'time' in semantic_roles and len(current_segment['words']) > 8:
            return True
        
        # æ ¹æ®å¥å­é•¿åº¦åˆ¤æ–­
        if len(current_segment['words']) > 15:  # è¶…è¿‡15ä¸ªè¯çš„é•¿å¥éœ€è¦åˆ†å‰²
            return True
        
        return False
    
    def _determine_segment_type(self, trigger_word: str) -> str:
        """æ ¹æ®è§¦å‘è¯ç¡®å®šç‰‡æ®µç±»å‹"""
        roles = self._analyze_word_semantics(trigger_word)
        
        if 'question_indicator' in roles:
            return 'question'
        elif 'transition' in roles:
            return 'transition'
        else:
            return 'statement'
    
    def _add_semantic_punctuation(self, segments: List[Dict]) -> str:
        """ä¸ºè¯­ä¹‰ç‰‡æ®µæ·»åŠ æ ‡ç‚¹ç¬¦å·"""
        result_parts = []
        
        for i, segment in enumerate(segments):
            # æ„å»ºç‰‡æ®µæ–‡æœ¬
            segment_text = ''.join([w['text'] for w in segment['words']])
            
            # æ ¹æ®è¯­ä¹‰ç±»å‹æ·»åŠ æ ‡ç‚¹
            if segment['semantic_type'] == 'question':
                # é—®å¥
                if not segment_text.endswith('ï¼Ÿ'):
                    segment_text += 'ï¼Ÿ'
            elif any('sentence_ending' in w.get('semantic_roles', []) for w in segment['words']):
                # æ˜ç¡®çš„å¥å­ç»“æŸ
                segment_text += 'ã€‚'
            elif i < len(segments) - 1:  # ä¸æ˜¯æœ€åä¸€ä¸ªç‰‡æ®µ
                next_segment = segments[i + 1]
                if next_segment['semantic_type'] == 'transition':
                    segment_text += 'ï¼Œ'
                elif len(segment['words']) > 10:  # é•¿å¥å­ç”¨å¥å·
                    segment_text += 'ã€‚'
                else:
                    segment_text += 'ï¼Œ'
            else:  # æœ€åä¸€ä¸ªç‰‡æ®µ
                segment_text += 'ã€‚'
            
            result_parts.append(segment_text)
        
        return ''.join(result_parts)
    
    def _create_paragraphs(self, text: str) -> str:
        """åˆ›å»ºæ®µè½ç»“æ„"""
        # æŒ‰å¥å·åˆ†å‰²å¥å­
        sentences = re.split(r'([ã€‚ï¼ï¼Ÿ])', text)
        paragraphs = []
        current_paragraph = []
        
        i = 0
        while i < len(sentences) - 1:
            sentence = sentences[i]
            punctuation = sentences[i + 1] if i + 1 < len(sentences) else ''
            
            if sentence.strip():
                full_sentence = sentence + punctuation
                current_paragraph.append(full_sentence)
                
                # åˆ¤æ–­æ˜¯å¦åº”è¯¥å¼€å§‹æ–°æ®µè½
                if self._should_start_new_paragraph(sentence, current_paragraph):
                    paragraphs.append(''.join(current_paragraph))
                    current_paragraph = []
            
            i += 2
        
        # æ·»åŠ æœ€åä¸€ä¸ªæ®µè½
        if current_paragraph:
            paragraphs.append(''.join(current_paragraph))
        
        return '\n\n'.join(paragraphs)
    
    def _should_start_new_paragraph(self, sentence: str, current_paragraph: List[str]) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥å¼€å§‹æ–°æ®µè½"""
        # æ®µè½é•¿åº¦æ§åˆ¶
        if len(current_paragraph) >= 4:  # è¶…è¿‡4å¥è¯å¼€å§‹æ–°æ®µè½
            return True
        
        # åŒ…å«é€»è¾‘è¯çš„å¥å­é€šå¸¸å¼€å§‹æ–°æ®µè½
        if any(word in sentence for word in self.logic_words):
            return True
        
        # åŒ…å«æ—¶é—´è¯çš„å¥å­å¯èƒ½å¼€å§‹æ–°çš„æ—¶é—´æ®µ
        if any(word in sentence for word in self.time_indicators) and len(current_paragraph) >= 2:
            return True
        
        return False
    
    def _final_polish(self, text: str) -> str:
        """æœ€ç»ˆä¼˜åŒ–å¤„ç†"""
        # è§„èŒƒåŒ–ç©ºæ ¼å’Œæ¢è¡Œ
        text = re.sub(r' +', ' ', text)  # å¤šä¸ªç©ºæ ¼åˆå¹¶ä¸ºä¸€ä¸ª
        text = re.sub(r'\n\n+', '\n\n', text)  # å¤šä¸ªæ¢è¡Œåˆå¹¶ä¸ºä¸¤ä¸ª
        
        # æ ‡ç‚¹ç¬¦å·å‰åçš„ç©ºæ ¼å¤„ç†
        text = re.sub(r' +([ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š])', r'\1', text)
        text = re.sub(r'([ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š]) +', r'\1 ', text)
        
        return text.strip()
    
    def _analyze_improvements(self, original: str, processed: str) -> List[str]:
        """åˆ†æå¤„ç†æ”¹è¿›"""
        improvements = []
        
        # ç»Ÿè®¡æ ‡ç‚¹ç¬¦å·æ•°é‡
        original_punct = len(re.findall(r'[ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š]', original))
        processed_punct = len(re.findall(r'[ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š]', processed))
        
        if processed_punct > original_punct:
            improvements.append(f"æ·»åŠ äº†{processed_punct - original_punct}ä¸ªæ ‡ç‚¹ç¬¦å·")
        
        # ç»Ÿè®¡æ®µè½æ•°é‡
        processed_paragraphs = len(processed.split('\n\n'))
        if processed_paragraphs > 1:
            improvements.append(f"åˆ›å»ºäº†{processed_paragraphs}ä¸ªæ®µè½")
        
        # æ£€æŸ¥å¥å­ç»“æ„
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', processed)
        if len(sentences) > 1:
            improvements.append(f"åˆ†å‰²ä¸º{len(sentences)}ä¸ªå¥å­")
        
        return improvements
    
    def _create_result(self, text: str, message: str, improvements: List[str] = None) -> Dict[str, Any]:
        """åˆ›å»ºç»“æœå¯¹è±¡"""
        return {
            'success': True,
            'processed_text': text,
            'message': message,
            'improvements': improvements or [],
            'has_paragraphs': '\n\n' in text,
            'sentence_count': len(re.split(r'[ã€‚ï¼ï¼Ÿ]', text)),
            'paragraph_count': len(text.split('\n\n')) if '\n\n' in text else 1
        }

# å…¨å±€å®ä¾‹
semantic_punctuation_service = SemanticPunctuationService() 