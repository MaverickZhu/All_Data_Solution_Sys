"""
è§†é¢‘è§†è§‰åˆ†ææœåŠ¡
åŸºäºç°æœ‰ImageDescriptionServiceï¼Œæ‰©å±•åˆ°è§†é¢‘å¸§çš„æ™ºèƒ½åˆ†æ
é›†æˆQwen2.5-VLæ¨¡å‹è¿›è¡Œåœºæ™¯ç†è§£ã€ç‰©ä½“è¯†åˆ«ã€æ–‡å­—æå–
"""
import logging
import asyncio
from pathlib import Path
from typing import Dict, List, Any, Optional
import base64
import json
from langchain_community.chat_models import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage

from backend.services.video_frame_extractor import FrameInfo
from backend.services.image_description_service import ImageDescriptionService

logger = logging.getLogger("service")


class VideoVisionService:
    """
    è§†é¢‘è§†è§‰åˆ†ææœåŠ¡
    åŸºäºQwen2.5-VLæ¨¡å‹è¿›è¡Œè§†é¢‘å¸§çš„æ™ºèƒ½åˆ†æ
    """
    
    def __init__(self, 
                 model_name: str = "qwen2.5vl:7b", 
                 ollama_url: str = "http://host.docker.internal:11435"):
        self.model_name = model_name
        self.ollama_url = ollama_url
        self.image_service = ImageDescriptionService(model_name, ollama_url)
        logger.info(f"ğŸ¬ è§†é¢‘è§†è§‰åˆ†ææœåŠ¡åˆå§‹åŒ–: {model_name}")
    
    async def analyze_video_frames(self, frames: List[FrameInfo]) -> Dict[str, Any]:
        """
        åˆ†æè§†é¢‘å¸§åºåˆ—ï¼Œæå–è§†è§‰è¯­ä¹‰ä¿¡æ¯
        
        Args:
            frames: è§†é¢‘å¸§ä¿¡æ¯åˆ—è¡¨
            
        Returns:
            è§†é¢‘è§†è§‰åˆ†æç»“æœ
        """
        try:
            logger.info(f"å¼€å§‹åˆ†æ {len(frames)} ä¸ªè§†é¢‘å¸§")
            
            # åˆ†æç»“æœå®¹å™¨
            frame_analyses = []
            visual_themes = set()
            detected_objects = set()
            scene_types = set()
            text_contents = []
            
            # æ‰¹é‡åˆ†æå¸§ï¼ˆæ§åˆ¶å¹¶å‘æ•°é‡é¿å…è¿‡è½½ï¼‰
            batch_size = 3  # æ§åˆ¶å¹¶å‘æ•°é‡
            for i in range(0, len(frames), batch_size):
                batch = frames[i:i + batch_size]
                logger.info(f"å¤„ç†å¸§æ‰¹æ¬¡ {i//batch_size + 1}/{(len(frames) + batch_size - 1)//batch_size}")
                
                # å¹¶å‘åˆ†æå½“å‰æ‰¹æ¬¡
                batch_tasks = []
                for frame in batch:
                    if frame.frame_path:
                        task = self.analyze_single_frame(frame)
                        batch_tasks.append(task)
                
                # ç­‰å¾…æ‰¹æ¬¡å®Œæˆ
                batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
                
                # å¤„ç†æ‰¹æ¬¡ç»“æœ
                for j, result in enumerate(batch_results):
                    if isinstance(result, Exception):
                        logger.error(f"å¸§åˆ†æå¤±è´¥: {result}")
                        continue
                    
                    frame_analyses.append(result)
                    
                    # æ”¶é›†å…¨å±€ä¿¡æ¯
                    if result.get('visual_themes'):
                        visual_themes.update(result['visual_themes'])
                    if result.get('detected_objects'):
                        detected_objects.update(result['detected_objects'])
                    if result.get('scene_type'):
                        scene_types.add(result['scene_type'])
                    if result.get('text_content'):
                        text_contents.append(result['text_content'])
            
            # ç”Ÿæˆè§†é¢‘çº§åˆ«çš„åˆ†ææ‘˜è¦
            video_summary = await self._generate_video_summary(frame_analyses)
            
            # æ£€æµ‹åœºæ™¯å˜åŒ–
            scene_changes = self._detect_scene_changes(frame_analyses)
            
            analysis_result = {
                "total_frames_analyzed": len(frame_analyses),
                "frame_analyses": frame_analyses,
                "visual_themes": list(visual_themes),
                "detected_objects": list(detected_objects),
                "scene_types": list(scene_types),
                "extracted_text": text_contents,
                "scene_changes": scene_changes,
                "video_summary": video_summary,
                "analysis_metadata": {
                    "model_used": self.model_name,
                    "analysis_type": "video_vision_semantic",
                    "success_rate": len(frame_analyses) / len(frames) if frames else 0
                }
            }
            
            logger.info(f"è§†é¢‘è§†è§‰åˆ†æå®Œæˆ: {len(frame_analyses)}å¸§æˆåŠŸåˆ†æ")
            return analysis_result
            
        except Exception as e:
            logger.error(f"è§†é¢‘è§†è§‰åˆ†æå¤±è´¥: {e}")
            return {
                "error": str(e),
                "total_frames_analyzed": 0,
                "frame_analyses": [],
                "visual_themes": [],
                "detected_objects": [],
                "scene_types": [],
                "extracted_text": [],
                "scene_changes": [],
                "video_summary": "",
                "analysis_metadata": {
                    "model_used": self.model_name,
                    "analysis_type": "video_vision_semantic",
                    "success_rate": 0,
                    "error": str(e)
                }
            }
    
    async def analyze_single_frame(self, frame: FrameInfo) -> Dict[str, Any]:
        """
        åˆ†æå•ä¸ªè§†é¢‘å¸§
        
        Args:
            frame: å¸§ä¿¡æ¯
            
        Returns:
            å•å¸§åˆ†æç»“æœ
        """
        try:
            frame_path = Path(frame.frame_path)
            if not frame_path.exists():
                raise FileNotFoundError(f"å¸§æ–‡ä»¶ä¸å­˜åœ¨: {frame_path}")
            
            logger.debug(f"åˆ†æå¸§: {frame.frame_number} (æ—¶é—´: {frame.timestamp:.2f}s)")
            
            # ä½¿ç”¨ä¸“é—¨çš„è§†é¢‘å¸§åˆ†ææç¤º
            analysis_result = await self._analyze_frame_with_video_context(frame_path, frame)
            
            # è§£æåˆ†æç»“æœ
            parsed_result = self._parse_frame_analysis(analysis_result, frame)
            
            return parsed_result
            
        except Exception as e:
            logger.error(f"å•å¸§åˆ†æå¤±è´¥ (å¸§{frame.frame_number}): {e}")
            return {
                "frame_number": frame.frame_number,
                "timestamp": frame.timestamp,
                "error": str(e),
                "visual_themes": [],
                "detected_objects": [],
                "scene_type": "unknown",
                "text_content": "",
                "description": "",
                "confidence": 0.0
            }
    
    async def _analyze_frame_with_video_context(self, frame_path: Path, frame: FrameInfo) -> str:
        """
        ä½¿ç”¨è§†é¢‘ä¸Šä¸‹æ–‡åˆ†æå¸§
        """
        max_retries = 3
        retry_count = 0
        
        while retry_count < max_retries:
            try:
                # ç¼–ç å›¾åƒ
                image_base64 = self.image_service.encode_image_to_base64(frame_path)
                
                # åˆå§‹åŒ–LLM with timeout
                llm = ChatOllama(
                    base_url=self.ollama_url,
                    model=self.model_name,
                    temperature=0.1,
                    timeout=30  # 30ç§’è¶…æ—¶
                )
                
                # è§†é¢‘å¸§ä¸“ç”¨åˆ†ææç¤º
                prompt_template = f"""
                <|system|>
                ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†é¢‘å†…å®¹åˆ†æä¸“å®¶ã€‚ä½ æ­£åœ¨åˆ†æè§†é¢‘ä¸­çš„ä¸€å¸§ç”»é¢ã€‚
                
                å½“å‰å¸§ä¿¡æ¯ï¼š
                - å¸§åºå·: {frame.frame_number}
                - æ—¶é—´æˆ³: {frame.timestamp:.2f}ç§’
                - å…³é”®å¸§åŸå› : {frame.key_frame_reason}
                - äº®åº¦: {frame.brightness:.1f}
                - å¯¹æ¯”åº¦: {frame.contrast:.1f}
                - æ¸…æ™°åº¦: {frame.sharpness:.3f}
                
                è¯·æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼åˆ†æè¿™ä¸€å¸§ï¼š
                {{
                    "scene_type": "åœºæ™¯ç±»å‹(å®¤å†…/å®¤å¤–/äººç‰©/é£æ™¯/æ–‡æ¡£ç­‰)",
                    "main_objects": ["ä¸»è¦ç‰©ä½“1", "ä¸»è¦ç‰©ä½“2", "ä¸»è¦ç‰©ä½“3"],
                    "visual_themes": ["è§†è§‰ä¸»é¢˜1", "è§†è§‰ä¸»é¢˜2"],
                    "text_content": "æå–çš„æ–‡å­—å†…å®¹(å¦‚æœæœ‰)",
                    "description": "è¯¦ç»†çš„åœºæ™¯æè¿°(50å­—ä»¥å†…)",
                    "activity_level": "æ´»åŠ¨æ°´å¹³(é™æ€/åŠ¨æ€/é«˜åŠ¨æ€)",
                    "lighting_condition": "å…‰ç…§æ¡ä»¶(æ˜äº®/æ˜æš—/èƒŒå…‰ç­‰)",
                    "composition": "æ„å›¾ç‰¹ç‚¹(ç‰¹å†™/è¿œæ™¯/ä¿¯è§†ç­‰)",
                    "color_tone": "è‰²è°ƒç‰¹ç‚¹(æš–è‰²è°ƒ/å†·è‰²è°ƒ/å•è‰²ç­‰)",
                    "confidence": 0.85
                }}
                
                <|user|>
                è¯·åˆ†æè¿™ä¸€å¸§çš„è§†è§‰å†…å®¹ï¼Œè¿”å›æ ‡å‡†JSONæ ¼å¼ï¼š

                å›¾åƒå†…å®¹ï¼š[IMAGE]
                """
                
                # æ„å»ºæ¶ˆæ¯
                messages = [
                    HumanMessage(
                        content=[
                            {"type": "text", "text": prompt_template.replace("[IMAGE]", "")},
                            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}}
                        ]
                    )
                ]
                
                # è°ƒç”¨LLM with timeout
                response = llm.invoke(messages)
                analysis_text = response.content
                
                return analysis_text
                
            except Exception as e:
                retry_count += 1
                logger.warning(f"å¸§åˆ†æå¤±è´¥ (å°è¯• {retry_count}/{max_retries}): {e}")
                
                if retry_count >= max_retries:
                    logger.error(f"å¸§åˆ†ææœ€ç»ˆå¤±è´¥: {e}")
                    # è¿”å›é»˜è®¤åˆ†æç»“æœ
                    return '{"scene_type": "unknown", "main_objects": [], "visual_themes": [], "text_content": "", "description": "åˆ†æå¤±è´¥", "activity_level": "unknown", "lighting_condition": "unknown", "composition": "unknown", "color_tone": "unknown", "confidence": 0.0}'
                
                # ç­‰å¾…åé‡è¯•
                await asyncio.sleep(2)
        
        return '{"scene_type": "unknown", "main_objects": [], "visual_themes": [], "text_content": "", "description": "åˆ†æå¤±è´¥", "activity_level": "unknown", "lighting_condition": "unknown", "composition": "unknown", "color_tone": "unknown", "confidence": 0.0}'
    
    def _parse_frame_analysis(self, analysis_text: str, frame: FrameInfo) -> Dict[str, Any]:
        """
        è§£æå¸§åˆ†æç»“æœ
        """
        try:
            # å°è¯•è§£æJSON
            import re
            
            # æå–JSONéƒ¨åˆ†
            json_match = re.search(r'\{.*\}', analysis_text, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                analysis_data = json.loads(json_str)
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°JSONï¼Œåˆ›å»ºé»˜è®¤ç»“æ„
                analysis_data = self._create_fallback_analysis(analysis_text)
            
            # æ ‡å‡†åŒ–ç»“æœæ ¼å¼
            result = {
                "frame_number": frame.frame_number,
                "timestamp": frame.timestamp,
                "scene_type": analysis_data.get("scene_type", "unknown"),
                "detected_objects": analysis_data.get("main_objects", []),
                "visual_themes": analysis_data.get("visual_themes", []),
                "text_content": analysis_data.get("text_content", ""),
                "description": analysis_data.get("description", ""),
                "activity_level": analysis_data.get("activity_level", "unknown"),
                "lighting_condition": analysis_data.get("lighting_condition", "unknown"),
                "composition": analysis_data.get("composition", "unknown"),
                "color_tone": analysis_data.get("color_tone", "unknown"),
                "confidence": float(analysis_data.get("confidence", 0.5)),
                "quality_metrics": {
                    "brightness": frame.brightness,
                    "contrast": frame.contrast,
                    "sharpness": frame.sharpness
                },
                "key_frame_reason": frame.key_frame_reason
            }
            
            return result
            
        except Exception as e:
            logger.warning(f"è§£æå¸§åˆ†æç»“æœå¤±è´¥: {e}")
            return self._create_fallback_analysis(analysis_text, frame)
    
    def _create_fallback_analysis(self, text: str, frame: FrameInfo = None) -> Dict[str, Any]:
        """
        åˆ›å»ºåå¤‡åˆ†æç»“æœ
        """
        return {
            "frame_number": frame.frame_number if frame else 0,
            "timestamp": frame.timestamp if frame else 0.0,
            "scene_type": "unknown",
            "detected_objects": [],
            "visual_themes": [],
            "text_content": "",
            "description": text[:100] if text else "åˆ†æå¤±è´¥",
            "activity_level": "unknown",
            "lighting_condition": "unknown", 
            "composition": "unknown",
            "color_tone": "unknown",
            "confidence": 0.3,
            "quality_metrics": {
                "brightness": frame.brightness if frame else 0.0,
                "contrast": frame.contrast if frame else 0.0,
                "sharpness": frame.sharpness if frame else 0.0
            },
            "key_frame_reason": frame.key_frame_reason if frame else ""
        }
    
    def _detect_scene_changes(self, frame_analyses: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        æ£€æµ‹åœºæ™¯å˜åŒ–
        """
        scene_changes = []
        
        if len(frame_analyses) < 2:
            return scene_changes
        
        prev_scene = None
        for analysis in frame_analyses:
            current_scene = analysis.get("scene_type", "unknown")
            
            if prev_scene and prev_scene != current_scene:
                scene_changes.append({
                    "timestamp": analysis.get("timestamp", 0.0),
                    "frame_number": analysis.get("frame_number", 0),
                    "from_scene": prev_scene,
                    "to_scene": current_scene,
                    "change_type": "scene_transition"
                })
            
            prev_scene = current_scene
        
        return scene_changes
    
    async def _generate_video_summary(self, frame_analyses: List[Dict[str, Any]]) -> str:
        """
        ç”Ÿæˆè§†é¢‘çº§åˆ«çš„åˆ†ææ‘˜è¦
        """
        try:
            if not frame_analyses:
                return "æ— æ³•ç”Ÿæˆæ‘˜è¦ï¼šæ²¡æœ‰æˆåŠŸåˆ†æçš„å¸§"
            
            # æ”¶é›†å…³é”®ä¿¡æ¯
            scene_types = [f.get("scene_type", "unknown") for f in frame_analyses]
            descriptions = [f.get("description", "") for f in frame_analyses if f.get("description")]
            
            # ç»Ÿè®¡æœ€å¸¸è§çš„åœºæ™¯ç±»å‹
            from collections import Counter
            scene_counter = Counter(scene_types)
            dominant_scene = scene_counter.most_common(1)[0][0] if scene_counter else "unknown"
            
            # ç”Ÿæˆç®€å•æ‘˜è¦
            summary = f"è§†é¢‘ä¸»è¦åœºæ™¯ç±»å‹ï¼š{dominant_scene}ã€‚"
            
            if len(descriptions) > 0:
                # é€‰æ‹©å‡ ä¸ªä»£è¡¨æ€§æè¿°
                sample_descriptions = descriptions[:3]
                summary += f"å…³é”®åœºæ™¯ï¼š{'; '.join(sample_descriptions)}ã€‚"
            
            summary += f"å…±åˆ†æ {len(frame_analyses)} ä¸ªå…³é”®å¸§ã€‚"
            
            return summary
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆè§†é¢‘æ‘˜è¦å¤±è´¥: {e}")
            return f"æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼š{str(e)}"
    
    async def analyze_scene_sequence(self, frames: List[FrameInfo]) -> Dict[str, Any]:
        """
        åˆ†æåœºæ™¯åºåˆ—ï¼Œè¯†åˆ«è§†é¢‘çš„æ•…äº‹ç»“æ„
        """
        try:
            logger.info("å¼€å§‹åœºæ™¯åºåˆ—åˆ†æ...")
            
            # å…ˆè¿›è¡ŒåŸºç¡€å¸§åˆ†æ
            frame_results = await self.analyze_video_frames(frames)
            
            if not frame_results.get("frame_analyses"):
                return {"error": "æ²¡æœ‰æˆåŠŸåˆ†æçš„å¸§"}
            
            # åˆ†æåœºæ™¯åºåˆ—
            sequences = self._identify_scene_sequences(frame_results["frame_analyses"])
            
            # åˆ†ææ•…äº‹ç»“æ„
            story_structure = self._analyze_story_structure(sequences)
            
            return {
                "scene_sequences": sequences,
                "story_structure": story_structure,
                "total_scenes": len(sequences),
                "analysis_metadata": {
                    "model_used": self.model_name,
                    "analysis_type": "scene_sequence"
                }
            }
            
        except Exception as e:
            logger.error(f"åœºæ™¯åºåˆ—åˆ†æå¤±è´¥: {e}")
            return {"error": str(e)}
    
    def _identify_scene_sequences(self, frame_analyses: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        è¯†åˆ«åœºæ™¯åºåˆ—
        """
        sequences = []
        current_sequence = None
        
        for analysis in frame_analyses:
            scene_type = analysis.get("scene_type", "unknown")
            timestamp = analysis.get("timestamp", 0.0)
            
            if current_sequence is None or current_sequence["scene_type"] != scene_type:
                # å¼€å§‹æ–°åºåˆ—
                if current_sequence:
                    current_sequence["end_time"] = prev_timestamp
                    current_sequence["duration"] = current_sequence["end_time"] - current_sequence["start_time"]
                    sequences.append(current_sequence)
                
                current_sequence = {
                    "scene_type": scene_type,
                    "start_time": timestamp,
                    "start_frame": analysis.get("frame_number", 0),
                    "frames": [analysis]
                }
            else:
                # ç»§ç»­å½“å‰åºåˆ—
                current_sequence["frames"].append(analysis)
            
            prev_timestamp = timestamp
        
        # æ·»åŠ æœ€åä¸€ä¸ªåºåˆ—
        if current_sequence:
            current_sequence["end_time"] = prev_timestamp
            current_sequence["duration"] = current_sequence["end_time"] - current_sequence["start_time"]
            sequences.append(current_sequence)
        
        return sequences
    
    def _analyze_story_structure(self, sequences: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        åˆ†ææ•…äº‹ç»“æ„
        """
        if not sequences:
            return {"structure": "unknown", "phases": []}
        
        total_duration = sum(seq.get("duration", 0) for seq in sequences)
        
        # ç®€å•çš„ä¸‰æ®µå¼ç»“æ„åˆ†æ
        structure_phases = []
        cumulative_time = 0
        
        for i, seq in enumerate(sequences):
            duration = seq.get("duration", 0)
            cumulative_time += duration
            progress = cumulative_time / total_duration if total_duration > 0 else 0
            
            if progress <= 0.33:
                phase = "å¼€å§‹"
            elif progress <= 0.66:
                phase = "å‘å±•"
            else:
                phase = "ç»“å°¾"
            
            structure_phases.append({
                "sequence_index": i,
                "scene_type": seq.get("scene_type", "unknown"),
                "phase": phase,
                "start_time": seq.get("start_time", 0),
                "duration": duration,
                "progress": progress
            })
        
        return {
            "structure": "ä¸‰æ®µå¼",
            "phases": structure_phases,
            "total_duration": total_duration,
            "scene_count": len(sequences)
        }


# åˆ›å»ºå…¨å±€å®ä¾‹
video_vision_service = VideoVisionService() 